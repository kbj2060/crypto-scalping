# PPO ëª¨ë¸ ëª…ì„¸ì„œ v2.0 (4-Action Strategy)

## ğŸ“‹ ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#1-ì‹œìŠ¤í…œ-ê°œìš”)
2. [ë°ì´í„° íë¦„ (Data Flow)](#2-ë°ì´í„°-íë¦„-data-flow)
3. [ëª¨ë¸ ì•„í‚¤í…ì²˜](#3-ëª¨ë¸-ì•„í‚¤í…ì²˜)
4. [ì•¡ì…˜ ì²´ê³„ (4-Action)](#4-ì•¡ì…˜-ì²´ê³„-4-action)
5. [ë³´ìƒ í•¨ìˆ˜ (Reward Function)](#5-ë³´ìƒ-í•¨ìˆ˜-reward-function)
6. [í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (Training Process)](#6-í•™ìŠµ-í”„ë¡œì„¸ìŠ¤-training-process)
7. [í•˜ì´í¼íŒŒë¼ë¯¸í„°](#7-í•˜ì´í¼íŒŒë¼ë¯¸í„°)

---

## 1. ì‹œìŠ¤í…œ ê°œìš”

### 1.1 ëª¨ë¸ ê°œìš”
ì´ PPO ëª¨ë¸ì€ ì•”í˜¸í™”í ìŠ¤ìº˜í•‘ ê±°ë˜ë¥¼ ìœ„í•œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

**í•µì‹¬ íŠ¹ì§•:**
- **xLSTM ê¸°ë°˜ Actor-Critic**: Multi-Layer sLSTM + Multi-Head Attention
- **4-Action Strategy**: HOLD, LONG, SHORT, EXIT (ëª…ì‹œì  ì²­ì‚° ë¶„ë¦¬)
- **Dense Reward**: ë§¤ ìŠ¤í… í‰ê°€ì†ìµ ë³€í™”ì— ì‹¤ì‹œê°„ í”¼ë“œë°±
- **State Retention**: LSTM ìƒíƒœë¥¼ ì—í”¼ì†Œë“œ ë‚´ì—ì„œ ìœ ì§€í•˜ì—¬ ì‹œê³„ì—´ ë§¥ë½ ë³´ì¡´
- **Value Clipping**: Critic í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ Clipped Loss ì‚¬ìš©

### 1.2 ê¸°ìˆ  ìŠ¤íƒ
- **ì•Œê³ ë¦¬ì¦˜**: PPO (Proximal Policy Optimization)
- **ë„¤íŠ¸ì›Œí¬**: xLSTM (sLSTMCell) + Multi-Head Attention
- **Action Space**: Discrete (4 actions)
- **State Space**: Tuple (Sequence Features + Info Features)

---

## 2. ë°ì´í„° íë¦„ (Data Flow)

### 2.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
[ì›ë³¸ ë°ì´í„°]
    â†“
[Feature Engineering] â†’ 25ê°œ ê¸°ë³¸ í”¼ì²˜ ìƒì„±
    â”œâ”€ ê°€ê²© & ë³€ë™ì„± (9ê°œ)
    â”œâ”€ ê±°ë˜ëŸ‰ & ì˜¤ë”í”Œë¡œìš° (6ê°œ)
    â”œâ”€ íŒ¨í„´ & ìœ ë™ì„± (5ê°œ)
    â””â”€ ì‹œì¥ ìƒê´€ê´€ê³„ (5ê°œ)
    â†“
[MTF Processing] â†’ 4ê°œ ë©€í‹°íƒ€ì„í”„ë ˆì„ í”¼ì²˜ ì¶”ê°€
    â”œâ”€ RSI_15m, Trend_15m
    â””â”€ RSI_1h, Trend_1h
    â†“
[ì „ëµ ì‹ í˜¸ ê³„ì‚°] â†’ 12ê°œ ì „ëµ ì ìˆ˜ (strategy_0 ~ strategy_11)
    â”œâ”€ í­ë°œì¥ ì „ëµ (6ê°œ)
    â””â”€ íš¡ë³´ì¥ ì „ëµ (6ê°œ)
    â†“
[ë°ì´í„° ì €ì¥] â†’ training_features.csv + cached_strategies.csv
    â†“
[Scaler Training] â†’ Train Set 70%ë§Œ ì‚¬ìš© (Data Leakage ë°©ì§€)
    â””â”€ ì €ì¥: data/ppo_model_best_scaler.pkl
    â†“
[Episode Loop]
    â”œâ”€ Observation ìƒì„±
    â”‚   â”œâ”€ obs_seq: (1, 60, 29) - ì‹œê³„ì—´ í”¼ì²˜
    â”‚   â””â”€ obs_info: (1, 15) - ì „ëµ ì ìˆ˜(12) + í¬ì§€ì…˜ ì •ë³´(3)
    â”œâ”€ Action ì„ íƒ (0, 1, 2, 3)
    â”œâ”€ Trading Logic ì‹¤í–‰
    â”‚   â”œâ”€ Action 1: LONG (ì§„ì…/ìŠ¤ìœ„ì¹­)
    â”‚   â”œâ”€ Action 2: SHORT (ì§„ì…/ìŠ¤ìœ„ì¹­)
    â”‚   â”œâ”€ Action 3: EXIT (ëª…ì‹œì  ì²­ì‚°)
    â”‚   â””â”€ Action 0: HOLD (ê´€ë§/ìœ ì§€)
    â”œâ”€ Reward ê³„ì‚° (4-Action Reward Function)
    â”œâ”€ Transition ì €ì¥
    â””â”€ PPO Update (GAE + Clipped Surrogate + Value Clipping)
    â†“
[Model Save] â†’ data/ppo_model_best.pth / data/ppo_model_last.pth
```

### 2.2 ë°ì´í„° êµ¬ì¡°

#### 2.2.1 ì…ë ¥ ë°ì´í„°
- **ETH/USDT 3ë¶„ë´‰ ë°ì´í„°**: `data/eth_3m_1year.csv`
  - ì»¬ëŸ¼: `open`, `high`, `low`, `close`, `volume`, `taker_buy_volume`, `cvd`
- **BTC/USDT 3ë¶„ë´‰ ë°ì´í„°**: `data/btc_3m_1year.csv` (ì„ íƒì )
  - ì»¬ëŸ¼: `close`, `volume`

#### 2.2.2 í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼
**29ê°œ ì‹œê³„ì—´ í”¼ì²˜** (`training_features.csv`):
1. `log_return` - ë¡œê·¸ ìˆ˜ìµë¥ 
2. `roll_return_6` - 6ë´‰ ë¡¤ë§ ìˆ˜ìµë¥ 
3. `atr_ratio` - ATR ë¹„ìœ¨
4. `bb_width` - ë³¼ë¦°ì € ë°´ë“œ í­
5. `bb_pos` - ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜
6. `rsi` - RSI
7. `macd_hist` - MACD íˆìŠ¤í† ê·¸ë¨
8. `hma_ratio` - HMA ë¹„ìœ¨
9. `cci` - CCI
10. `rvol` - ìƒëŒ€ ê±°ë˜ëŸ‰
11. `taker_ratio` - í…Œì´ì»¤ ë§¤ìˆ˜ ë¹„ìœ¨
12. `cvd_change` - CVD ë³€í™”ëŸ‰
13. `mfi` - MFI
14. `cmf` - CMF
15. `vwap_dist` - VWAP ê±°ë¦¬
16. `wick_upper` - ìƒë‹¨ ì‹¬ì§€ ë¹„ìœ¨
17. `wick_lower` - í•˜ë‹¨ ì‹¬ì§€ ë¹„ìœ¨
18. `range_pos` - ë ˆì¸ì§€ ìœ„ì¹˜
19. `swing_break` - ìŠ¤ìœ™ ë¸Œë ˆì´í¬ í”Œë˜ê·¸
20. `chop` - ì´™ ì¸ë±ìŠ¤
21. `btc_return` - BTC ìˆ˜ìµë¥ 
22. `btc_rsi` - BTC RSI
23. `btc_corr` - BTC-ETH ìƒê´€ê´€ê³„
24. `btc_vol` - BTC ë³€ë™ì„±
25. `eth_btc_ratio` - ETH/BTC ë¹„ìœ¨
26. `rsi_15m` - 15ë¶„ë´‰ RSI
27. `trend_15m` - 15ë¶„ë´‰ ì¶”ì„¸
28. `rsi_1h` - 1ì‹œê°„ë´‰ RSI
29. `trend_1h` - 1ì‹œê°„ë´‰ ì¶”ì„¸

**12ê°œ ì „ëµ ì ìˆ˜** (`cached_strategies.csv`):
- `strategy_0` ~ `strategy_11`: ê° ì „ëµì˜ ì‹ í˜¸ ê°•ë„ (-1.0 ~ 1.0)

#### 2.2.3 ê´€ì¸¡ ê³µê°„ (Observation Space)

**obs_seq (ì‹œê³„ì—´ í”¼ì²˜)**
- Shape: `(1, LOOKBACK, 29)`
- LOOKBACK: 60 (config.LOOKBACK)
- 29ê°œ í”¼ì²˜: ìœ„ì˜ 29ê°œ ì»¬ëŸ¼
- ì •ê·œí™”: Z-Score Normalization (DataPreprocessor)

**obs_info (ì •ë³´ í”¼ì²˜)**
- Shape: `(1, 15)`
- êµ¬ì„±:
  - `[0:12]`: ì „ëµ ì ìˆ˜ (12ê°œ)
  - `[12]`: í¬ì§€ì…˜ ê°’ (1.0=LONG, -1.0=SHORT, 0.0=None)
  - `[13]`: í‰ê°€ì†ìµ (unrealized_pnl * 10)
  - `[14]`: ë³´ìœ  ì‹œê°„ (holding_time / max_steps)

**ìµœì¢… State**
- Type: Tuple `(obs_seq, obs_info)`
- obs_seq: `torch.FloatTensor` shape `(1, 60, 29)`
- obs_info: `torch.FloatTensor` shape `(1, 15)`

### 2.3 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

#### 2.3.1 Feature Engineering (`model/feature_engineering.py`)
```python
FeatureEngineer.generate_features()
  â”œâ”€ _add_price_volatility_features() â†’ 9ê°œ í”¼ì²˜
  â”œâ”€ _add_volume_flow_features() â†’ 6ê°œ í”¼ì²˜
  â”œâ”€ _add_pattern_liquidity_features() â†’ 5ê°œ í”¼ì²˜
  â””â”€ _add_market_correlation_features() â†’ 5ê°œ í”¼ì²˜ (BTC ë°ì´í„° í•„ìš”)
```

#### 2.3.2 MTF Processing (`model/mtf_processor.py`)
```python
MTFProcessor.add_mtf_features()
  â”œâ”€ 15ë¶„ë´‰ ë¦¬ìƒ˜í”Œë§ â†’ RSI_15m, Trend_15m ê³„ì‚° â†’ Shift(1)
  â””â”€ 1ì‹œê°„ë´‰ ë¦¬ìƒ˜í”Œë§ â†’ RSI_1h, Trend_1h ê³„ì‚° â†’ Shift(1)
  â†’ Look-ahead Bias ì™„ë²½ ì°¨ë‹¨
```

#### 2.3.3 ì •ê·œí™” (`model/preprocess.py`)
```python
DataPreprocessor
  â”œâ”€ fit(): ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ mean, std ê³„ì‚°
  â”œâ”€ transform(): Z-Score ì •ê·œí™” (x - mean) / std
  â””â”€ save()/load(): pickleë¡œ ì €ì¥/ë¡œë“œ
```

---

## 3. ëª¨ë¸ ì•„í‚¤í…ì²˜

### 3.1 ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (`model/xlstm_network.py`)

```
Input (obs_seq: [1, 60, 29], obs_info: [1, 15])
    â†“
[Input Projection]
    Linear(29 â†’ 128) + LayerNorm + Dropout(0.1)
    â†“
[xLSTM Stack (2 Layers)]
    For each layer:
        sLSTMCell(128 â†’ 128)
        â”œâ”€ Input Norm (LayerNorm)
        â”œâ”€ sLSTM Forward (h, c, n ìƒíƒœ ìœ ì§€)
        â””â”€ Residual Connection (input + h)
    â†“
[Multi-Head Attention]
    MultiheadAttention(128, heads=4)
    â”œâ”€ Self-Attention
    â”œâ”€ Weighted Pooling (Linear â†’ Softmax â†’ Weighted Sum)
    â””â”€ Output: [1, 128]
    â†“
[Info Encoder]
    Linear(15 â†’ 64) â†’ LayerNorm â†’ GELU â†’ Dropout â†’ Linear(64 â†’ 64)
    Output: [1, 64]
    â†“
[Concatenate]
    Concat([context(128), info_encoded(64)]) â†’ [1, 192]
    â†“
[Shared Trunk]
    Linear(192 â†’ 256) â†’ LayerNorm â†’ GELU
    â†’ Linear(256 â†’ 128) â†’ LayerNorm â†’ GELU
    Output: [1, 128]
    â†“
[Separate Heads]
    â”œâ”€ Actor Head: Linear(128 â†’ 64) â†’ GELU â†’ Dropout â†’ Linear(64 â†’ 4) â†’ Softmax
    â”‚   Output: [1, 4] (Action Probabilities)
    â””â”€ Critic Head: Linear(128 â†’ 32) â†’ LayerNorm â†’ GELU â†’ Linear(32 â†’ 1)
        Output: [1, 1] (State Value)
```

### 3.2 ì£¼ìš” ì»´í¬ë„ŒíŠ¸

#### 3.2.1 sLSTMCell
- **ì…ë ¥**: `(x, h, c, n)`
- **ì¶œë ¥**: `(h_next, c_next, n_next)`
- **íŠ¹ì§•**:
  - Gate Clamping: `[-5, 5]` ë²”ìœ„ë¡œ ì œí•œ
  - State Clamping: `c_next [-1e6, 1e6]`, `n_next [1e-6, 1e6]`
  - NaN/Inf ë°©ì§€: `nan_to_num` ì²˜ë¦¬

#### 3.2.2 MultiHeadAttention
- **ì…ë ¥**: `[batch, seq_len, hidden_dim]`
- **ì¶œë ¥**: `[batch, hidden_dim]` (Weighted Pooling)
- **íŠ¹ì§•**:
  - Self-Attentionìœ¼ë¡œ ì‹œí€€ìŠ¤ ë‚´ ì˜ì¡´ì„± í•™ìŠµ
  - Weighted Poolingìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ë‹¨ì¼ ë²¡í„°ë¡œ ì••ì¶•

#### 3.2.3 Info Encoder
- **ì…ë ¥**: `[batch, 15]` (ì „ëµ ì ìˆ˜ 12 + í¬ì§€ì…˜ ì •ë³´ 3)
- **ì¶œë ¥**: `[batch, 64]`
- **ëª©ì **: ì „ëµ ì‹ í˜¸ì™€ í¬ì§€ì…˜ ì •ë³´ë¥¼ ê³ ì°¨ì›ìœ¼ë¡œ ì¸ì½”ë”©

#### 3.2.4 Shared Trunk
- **ì…ë ¥**: `[batch, 192]` (context 128 + info_encoded 64)
- **ì¶œë ¥**: `[batch, 128]`
- **íŠ¹ì§•**: Actorì™€ Criticì´ ê³µìœ í•˜ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°

#### 3.2.5 Actor Head
- **ì…ë ¥**: `[batch, 128]`
- **ì¶œë ¥**: `[batch, 4]` (Action Probabilities)
- **íŠ¹ì§•**: Dropout ìœ ì§€ (íƒí—˜ ìœ ë„)

#### 3.2.6 Critic Head
- **ì…ë ¥**: `[batch, 128]`
- **ì¶œë ¥**: `[batch, 1]` (State Value)
- **íŠ¹ì§•**: LayerNorm ì¶”ê°€ (Value Function ì•ˆì •í™”), Dropout ì œê±°

### 3.3 ìƒíƒœ ìœ ì§€ (State Retention)

**LSTM ìƒíƒœ êµ¬ì¡°:**
- `h`: Hidden state `[num_layers, batch, hidden_dim]`
- `c`: Cell state `[num_layers, batch, hidden_dim]`
- `n`: Normalization state `[num_layers, batch, hidden_dim]`

**ì—í”¼ì†Œë“œ ë‚´ ìƒíƒœ ìœ ì§€:**
- `select_action()` í˜¸ì¶œ ì‹œ `self.current_states` ìœ ì§€
- `reset_episode_states()`ë¡œ ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì´ˆê¸°í™”

---

## 4. ì•¡ì…˜ ì²´ê³„ (4-Action)

### 4.1 ì•¡ì…˜ ì •ì˜

| Action | ê°’ | ì˜ë¯¸ | ë™ì‘ |
|--------|-----|------|------|
| HOLD | 0 | ê´€ë§/ìœ ì§€ | í˜„ì¬ í¬ì§€ì…˜ ìœ ì§€ (ë¬´í¬ì§€ì…˜ì´ë©´ ê³„ì† ê´€ë§) |
| LONG | 1 | ë¡± ì§„ì…/ìŠ¤ìœ„ì¹­ | SHORT â†’ LONG: ìŠ¤ìœ„ì¹­ (ê¸°ì¡´ ì²­ì‚° + ìƒˆ ì§„ì…)<br>None â†’ LONG: ì§„ì…<br>ì´ë¯¸ LONG: ìœ ì§€ |
| SHORT | 2 | ìˆ ì§„ì…/ìŠ¤ìœ„ì¹­ | LONG â†’ SHORT: ìŠ¤ìœ„ì¹­ (ê¸°ì¡´ ì²­ì‚° + ìƒˆ ì§„ì…)<br>None â†’ SHORT: ì§„ì…<br>ì´ë¯¸ SHORT: ìœ ì§€ |
| EXIT | 3 | ëª…ì‹œì  ì²­ì‚° | í¬ì§€ì…˜ ìˆìœ¼ë©´ ì²­ì‚°<br>í¬ì§€ì…˜ ì—†ìœ¼ë©´ HOLDì™€ ë™ì¼ |

### 4.2 ì•¡ì…˜ ì²˜ë¦¬ ë¡œì§ (`train_ppo.py`)

```python
# Action 1: LONG
if action == 1:
    if current_position == 'SHORT':  # ìŠ¤ìœ„ì¹­
        realized_pnl = unrealized_pnl
        trade_done = True
        current_position = 'LONG'
        entry_price = curr_price
        entry_index = current_idx
    elif current_position is None:  # ì§„ì…
        current_position = 'LONG'
        entry_price = curr_price
        entry_index = current_idx
    # ì´ë¯¸ LONGì´ë©´ ìœ ì§€

# Action 2: SHORT
elif action == 2:
    if current_position == 'LONG':  # ìŠ¤ìœ„ì¹­
        realized_pnl = unrealized_pnl
        trade_done = True
        current_position = 'SHORT'
        entry_price = curr_price
        entry_index = current_idx
    elif current_position is None:  # ì§„ì…
        current_position = 'SHORT'
        entry_price = curr_price
        entry_index = current_idx
    # ì´ë¯¸ SHORTë©´ ìœ ì§€

# Action 3: EXIT
elif action == 3:
    if current_position is not None:
        realized_pnl = unrealized_pnl
        trade_done = True
        current_position = None
        entry_price = 0.0
        entry_index = 0

# Action 0: HOLD
# ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ (Pass)
```

### 4.3 ê°•ì œ ì•ˆì „ì¥ì¹˜

**Stop Loss Threshold:**
- ì„ê³„ê°’: `config.STOP_LOSS_THRESHOLD = -0.05` (-5%)
- ë™ì‘: `unrealized_pnl < -0.05`ì¼ ë•Œ ê°•ì œ ì²­ì‚°
- ëª©ì : ê·¹ë‹¨ì  ì†ì‹¤ ë°©ì§€ (í•™ìŠµ ê°€ì†)

---

## 5. ë³´ìƒ í•¨ìˆ˜ (Reward Function)

### 5.1 ë³´ìƒ í•¨ìˆ˜ êµ¬ì¡° (`model/trading_env.py`)

```python
def calculate_reward(step_pnl, realized_pnl, trade_done, action, prev_position):
    reward = 0.0
    
    # 1. HOLD Small Bonus
    if action == 0:
        reward += 0.0002
    
    # 2. Position Holding Reward (Trend Riding)
    if prev_position is not None:
        reward += step_pnl * 30.0
    
    # 3. Switching Penalty
    if trade_done and (action == 1 or action == 2):
        reward -= 0.5
    
    # 4. EXIT Rewards (Realized PnL)
    if trade_done and action == 3:
        fee = config.TRANSACTION_COST
        net_pnl = realized_pnl - fee
        
        if net_pnl > 0:
            reward += net_pnl * 250.0
            if net_pnl > 0.005:  # 0.5% ì´ìƒ
                reward += 1.0  # ë³´ë„ˆìŠ¤
        else:
            reward += net_pnl * 300.0
            reward -= 0.2  # ê³ ì • í˜ë„í‹°
    
    return np.clip(reward, -10, 10)
```

### 5.2 ë³´ìƒ êµ¬ì„± ìš”ì†Œ

#### 5.2.1 HOLD ë³´ë„ˆìŠ¤
- **ê°’**: `+0.0002`
- **ëª©ì **: ê´€ë§ë„ ì „ëµì„ì„ ì¸ì§€ì‹œí‚´

#### 5.2.2 Position Holding Reward
- **ê³µì‹**: `step_pnl * 30.0`
- **ì˜ë¯¸**: í¬ì§€ì…˜ ë³´ìœ  ì¤‘ í‰ê°€ìµ ë³€í™”ì— ì‹¤ì‹œê°„ í”¼ë“œë°±
- **íš¨ê³¼**: ì¶”ì„¸ë¥¼ ê¸¸ê²Œ íƒ€ë„ë¡ ìœ ë„

#### 5.2.3 Switching Penalty
- **ê°’**: `-0.5`
- **ì¡°ê±´**: `trade_done=True` AND `action in [1, 2]`
- **ëª©ì **: ì¦ì€ í¬ì§€ì…˜ ë³€ê²½ ë°©ì§€

#### 5.2.4 EXIT Rewards
**ìµì ˆ ì‹œ:**
- ê¸°ë³¸: `net_pnl * 250.0`
- ë³´ë„ˆìŠ¤: `net_pnl > 0.005`ì¼ ë•Œ `+1.0`

**ì†ì ˆ ì‹œ:**
- ê¸°ë³¸: `net_pnl * 300.0` (ë” í° í˜ë„í‹°)
- ê³ ì • í˜ë„í‹°: `-0.2`

**ìˆ˜ìˆ˜ë£Œ:**
- `-config.TRANSACTION_COST` (ì•½ -0.0015)

### 5.3 ë³´ìƒ ìŠ¤ì¼€ì¼

**ì˜ˆì‹œ ê³„ì‚°:**

**ì‹œë‚˜ë¦¬ì˜¤ 1: ìµì ˆ (+2%)**
```
EXIT ë³´ìƒ: (0.02 - 0.0015) * 250.0 = +4.625
ë³´ë„ˆìŠ¤: +1.0 (0.02 > 0.005)
ì´ ë³´ìƒ: +5.625
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ì†ì ˆ (-2%)**
```
EXIT ë³´ìƒ: (-0.02 - 0.0015) * 300.0 = -6.45
ê³ ì • í˜ë„í‹°: -0.2
ì´ ë³´ìƒ: -6.65
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ìŠ¤ìœ„ì¹­ (SHORT â†’ LONG)**
```
ìŠ¤ìœ„ì¹­ í˜ë„í‹°: -0.5
Position Holding: step_pnl * 30.0 (í‰ê°€ìµ ë³€í™”ì— ë”°ë¼)
```

---

## 6. í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (Training Process)

### 6.1 PPO ì•Œê³ ë¦¬ì¦˜

#### 6.1.1 GAE (Generalized Advantage Estimation)
```python
# TD Target
td_target = r_batch + gamma * v_next * done_batch
delta = td_target - v_s

# GAE ê³„ì‚° (ì—­ë°©í–¥)
gae = 0
for step in reversed(range(len(r_batch)):
    if done_batch[step] == 0:
        gae = delta[step] + gamma * lambda * gae
    else:
        gae = delta[step]
    advantages.insert(0, gae)

# ì •ê·œí™”
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
returns = advantages + v_s.squeeze()
```

#### 6.1.2 PPO Clipped Surrogate Loss
```python
# Importance Sampling Ratio
ratio = exp(log_prob_new - log_prob_old)

# Clipped Surrogate
surr1 = ratio * advantages
surr2 = clamp(ratio, 1-eps_clip, 1+eps_clip) * advantages
actor_loss = -min(surr1, surr2).mean()
```

#### 6.1.3 Value Clipping (Critic)
```python
# Old Value ì €ì¥
old_values = v_s.clone()

# Unclipped Loss
loss_v1 = smooth_l1_loss(v_pred, v_target, reduction='none')

# Clipped Loss
v_pred_clipped = old_values + clamp(v_pred - old_values, -eps_clip, eps_clip)
loss_v2 = smooth_l1_loss(v_pred_clipped, v_target, reduction='none')

# ë³´ìˆ˜ì  ì—…ë°ì´íŠ¸
critic_loss = max(loss_v1, loss_v2).mean()
```

#### 6.1.4 ì—”íŠ¸ë¡œí”¼ ì •ì±…
```python
entropy_loss = dist.entropy().mean()
current_entropy_coef = max(
    PPO_ENTROPY_MIN,
    PPO_ENTROPY_COEF * (PPO_ENTROPY_DECAY ** episode)
)
```

#### 6.1.5 ìµœì¢… Loss
```python
loss = actor_loss + 0.5 * critic_loss - current_entropy_coef * entropy_loss
```

### 6.2 í•™ìŠµ ë£¨í”„ (`train_ppo.py`)

#### 6.2.1 ì—í”¼ì†Œë“œ êµ¬ì¡°
```python
for episode in range(1, num_episodes + 1):
    # 1. ëœë¤ ì‹œì‘ ì¸ë±ìŠ¤ ì„ íƒ
    start_idx = random(start_min, start_max)
    
    # 2. ì—í”¼ì†Œë“œ ì‹¤í–‰
    episode_reward, trade_count = train_episode(episode)
    
    # 3. PPO ì—…ë°ì´íŠ¸ (ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„)
    loss = agent.train_net(episode=episode)
    
    # 4. ëª¨ë¸ ì €ì¥ (ìµœê³  ì„±ëŠ¥ ë˜ëŠ” ì£¼ê¸°ì )
    if episode_reward > best_reward:
        save_model(best_model)
    elif episode % save_interval == 0:
        save_model(last_model)
```

#### 6.2.2 ìŠ¤í… ë£¨í”„
```python
for step in range(max_steps):
    # 1. í‰ê°€ì†ìµ ê³„ì‚°
    unrealized_pnl = calculate_unrealized_pnl()
    step_pnl = unrealized_pnl - prev_unrealized_pnl
    
    # 2. ê´€ì¸¡ ìƒì„±
    state = env.get_observation(position_info, current_index)
    
    # 3. í–‰ë™ ì„ íƒ
    action, prob = agent.select_action(state)
    
    # 4. ê±°ë˜ ë¡œì§ ì‹¤í–‰
    # - ê°•ì œ ì†ì ˆ ì²´í¬
    # - AI í–‰ë™ ì²˜ë¦¬ (4-Action)
    
    # 5. ë³´ìƒ ê³„ì‚°
    reward = env.calculate_reward(step_pnl, realized_pnl, trade_done, action, prev_position)
    
    # 6. Transition ì €ì¥
    agent.put_data((state, action, reward, next_state, prob, done))
    
    # 7. ë‹¤ìŒ ìŠ¤í… ì¤€ë¹„
    prev_unrealized_pnl = unrealized_pnl if not trade_done else 0.0
```

### 6.3 í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

**LinearLR Scheduler:**
```python
scheduler = LinearLR(
    optimizer,
    start_factor=1.0,
    end_factor=0.01,  # config.PPO_LR_END_FACTOR
    total_iters=2000  # config.TRAIN_NUM_EPISODES
)
```

**ë™ì‘:**
- ì—í”¼ì†Œë“œ 0: `lr = 5e-5 * 1.0 = 5e-5`
- ì—í”¼ì†Œë“œ 1000: `lr = 5e-5 * 0.505 â‰ˆ 2.525e-5`
- ì—í”¼ì†Œë“œ 2000: `lr = 5e-5 * 0.01 = 5e-7`

---

## 7. í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 7.1 PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° (`config.py`)

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `PPO_GAMMA` | 0.99 | í• ì¸ìœ¨ (ë¯¸ë˜ ë³´ìƒ ê°€ì¤‘ì¹˜) |
| `PPO_LAMBDA` | 0.95 | GAE ëŒë‹¤ (bias-variance trade-off) |
| `PPO_EPS_CLIP` | 0.15 | PPO í´ë¦¬í•‘ ë²”ìœ„ |
| `PPO_K_EPOCHS` | 4 | ì—…ë°ì´íŠ¸ ë°˜ë³µ íšŸìˆ˜ |
| `PPO_ENTROPY_COEF` | 0.01 | ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (íƒí—˜ë¥ ) |
| `PPO_ENTROPY_DECAY` | 0.9996 | ì—”íŠ¸ë¡œí”¼ ê°ì†Œìœ¨ |
| `PPO_ENTROPY_MIN` | 0.005 | ì—”íŠ¸ë¡œí”¼ ìµœì†Œê°’ |
| `PPO_LEARNING_RATE` | 5e-5 | í•™ìŠµë¥  |
| `PPO_LR_END_FACTOR` | 0.01 | í•™ìŠµ ì¢…ë£Œ ì‹œ í•™ìŠµë¥  ë¹„ìœ¨ |

### 7.2 ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `NETWORK_HIDDEN_DIM` | 128 | ì€ë‹‰ì¸µ ì°¨ì› |
| `NETWORK_NUM_LAYERS` | 2 | xLSTM ë ˆì´ì–´ ê°œìˆ˜ |
| `NETWORK_DROPOUT` | 0.1 | Dropout ë¹„ìœ¨ |
| `NETWORK_ATTENTION_HEADS` | 4 | Multi-Head Attention í—¤ë“œ ê°œìˆ˜ |
| `NETWORK_INFO_ENCODER_DIM` | 64 | Info Encoder ì¶œë ¥ ì°¨ì› |
| `NETWORK_SHARED_TRUNK_DIM1` | 256 | Shared Trunk ì²« ë²ˆì§¸ ë ˆì´ì–´ |
| `NETWORK_SHARED_TRUNK_DIM2` | 128 | Shared Trunk ë‘ ë²ˆì§¸ ë ˆì´ì–´ |
| `NETWORK_ACTOR_HEAD_DIM` | 64 | Actor Head ì€ë‹‰ì¸µ |
| `NETWORK_CRITIC_HEAD_DIM` | 32 | Critic Head ì€ë‹‰ì¸µ |

### 7.3 í•™ìŠµ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `TRAIN_ACTION_DIM` | 4 | í–‰ë™ ì°¨ì› (HOLD, LONG, SHORT, EXIT) |
| `TRAIN_BATCH_SIZE` | 1024 | ë°°ì¹˜ í¬ê¸° |
| `TRAIN_SAMPLE_SIZE` | 50000 | ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ìƒ˜í”Œ í¬ê¸° |
| `TRAIN_SPLIT` | 0.7 | í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (70%) |
| `TRAIN_NUM_EPISODES` | 2000 | ì´ ì—í”¼ì†Œë“œ ìˆ˜ |
| `TRAIN_MAX_STEPS_PER_EPISODE` | 480 | ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… |
| `TRAIN_SAVE_INTERVAL` | 50 | ëª¨ë¸ ì €ì¥ ê°„ê²© |

### 7.4 ë³´ìƒ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `TRANSACTION_COST` | 0.0015 | ê±°ë˜ ë¹„ìš© (0.15%) |
| `STOP_LOSS_THRESHOLD` | -0.05 | ê°•ì œ ì†ì ˆ ì„ê³„ê°’ (-5%) |

---

## 8. ë°ì´í„° ì €ì¥ ë° ë¡œë“œ

### 8.1 ì €ì¥ íŒŒì¼

**ëª¨ë¸ íŒŒì¼:**
- `data/ppo_model_best.pth`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
- `data/ppo_model_last.pth`: ìµœì‹  ëª¨ë¸

**ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼:**
- `data/ppo_model_best_scaler.pkl`: ìµœê³  ì„±ëŠ¥ ìŠ¤ì¼€ì¼ëŸ¬
- `data/ppo_model_last_scaler.pkl`: ìµœì‹  ìŠ¤ì¼€ì¼ëŸ¬

**ë°ì´í„° íŒŒì¼:**
- `data/training_features.csv`: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼ (29ê°œ í”¼ì²˜)
- `data/cached_strategies.csv`: ì „ëµ ì‹ í˜¸ ìºì‹œ (12ê°œ ì „ëµ)

### 8.2 ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°

```python
{
    'model_state_dict': {...},  # xLSTMActorCritic ê°€ì¤‘ì¹˜
    'optimizer_state_dict': {...}  # Adam optimizer ìƒíƒœ
}
```

---

## 9. ì£¼ìš” ê°œì„  ì‚¬í•­ (v2.0)

### 9.1 4-Action Strategy
- **ê¸°ì¡´**: 3-Action (HOLD, LONG, SHORT)
- **ë³€ê²½**: 4-Action (HOLD, LONG, SHORT, EXIT)
- **íš¨ê³¼**: ëª…ì‹œì  ì²­ì‚°ê³¼ ê´€ë§ì„ ë¶„ë¦¬í•˜ì—¬ í•™ìŠµ ëª…í™•ì„± í–¥ìƒ

### 9.2 ë³´ìƒ í•¨ìˆ˜ ê°œì„ 
- **HOLD ë³´ë„ˆìŠ¤**: ê´€ë§ë„ ì „ëµì„ì„ ì¸ì§€
- **Position Holding Reward**: ì¶”ì„¸ë¥¼ ê¸¸ê²Œ íƒ€ë„ë¡ ìœ ë„
- **Switching Penalty**: ì¦ì€ í¬ì§€ì…˜ ë³€ê²½ ë°©ì§€
- **EXIT Rewards**: Realized PnL ì¤‘ì‹¬ì˜ ê°•ë ¥í•œ í”¼ë“œë°±

### 9.3 Value Clipping
- **Critic Loss Clipping**: í° ë³´ìƒ í™˜ê²½ì—ì„œë„ ì•ˆì •ì  í•™ìŠµ
- **ë³´ìˆ˜ì  ì—…ë°ì´íŠ¸**: `max(loss_v1, loss_v2)` ì‚¬ìš©

### 9.4 ë¬¼ë¦¬ì  ì œì•½ ì œê±°
- **ì¿¨ë‹¤ìš´ ì œê±°**: AIê°€ ììœ ë¡­ê²Œ í–‰ë™
- **ìµœì†Œ ë³´ìœ  ì‹œê°„ ì œê±°**: ì¦‰ê°ì ì¸ ì†ì ˆ ê°€ëŠ¥
- **ë¦¬ì›Œë“œ ê¸°ë°˜ í•™ìŠµ**: ì‹œìŠ¤í…œ ì œì•½ ëŒ€ì‹  ë¦¬ì›Œë“œë¡œ ìì œ í•™ìŠµ

---

## 10. ì„±ëŠ¥ ìµœì í™”

### 10.1 í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìµœì í™”
- ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í•œ ë²ˆë§Œ í”¼ì²˜ ìƒì„±
- ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œ ìƒ˜í”Œë§ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”

### 10.2 ì „ëµ ì‹ í˜¸ ìºì‹±
- `cached_strategies.csv`ë¡œ ì „ëµ ê³„ì‚° ê²°ê³¼ ì¬ì‚¬ìš©
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (joblib)

### 10.3 Gradient Checkpointing
- `NETWORK_USE_CHECKPOINTing`: ë©”ëª¨ë¦¬ ì ˆì•½ (í˜„ì¬ False)

---

## 11. ì°¸ê³ ì‚¬í•­

### 11.1 ì£¼ì˜ì‚¬í•­
- **ëª¨ë¸ í˜¸í™˜ì„±**: 3-Action ëª¨ë¸ì€ 4-Actionê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŒ
- **í•™ìŠµ ì¬ì‹œì‘**: 4-Actionìœ¼ë¡œ ë³€ê²½ ì‹œ ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ í•„ìš”
- **ë°ì´í„° ì¤€ë¹„**: `training_features.csv`ì™€ `cached_strategies.csv` í•„ìš”

### 11.2 ë””ë²„ê¹… íŒ
- ë¡œê·¸ íŒŒì¼: `logs/train_ppo.log`
- ìŠ¤ì¼€ì¼ëŸ¬ ì²´í¬: `scaler_fitted` í”Œë˜ê·¸ í™•ì¸
- ë©”ëª¨ë¦¬ ìƒíƒœ: `len(agent.memory)` í™•ì¸

---

**ë¬¸ì„œ ë²„ì „**: v2.0 (4-Action Strategy)  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-23
