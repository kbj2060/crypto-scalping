# 학습 데이터 흐름도 및 피처 상세 문서

## 📊 학습 데이터 흐름도

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. 데이터 수집 단계 (DataCollector)                              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 저장된 CSV 파일 로드                │
        │ - data/eth_3m_1year.csv            │
        │ - data/btc_3m_1year.csv            │
        │ - 1년치 3분봉 데이터                │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ DataFrame 형태로 저장               │
        │ - timestamp, open, high, low,      │
        │   close, volume, trades,            │
        │   taker_buy_base, ...               │
        └─────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ 2. 전역 스케일러 학습 단계 (_fit_global_scaler)                 │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 전체 데이터에서 샘플링              │
        │ - 최대 50,000개 샘플                │
        │ - 인덱스 20부터 시작 (윈도우 크기) │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 각 샘플에서 7개 피처 생성            │
        │ - 마지막 20봉 데이터 사용            │
        │ - Volume, Trades는 log1p 변환 적용  │
        │ - (20, 7) 형태의 배열               │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ Z-Score 정규화 통계량 계산           │
        │ - Mean: (7,) 배열                    │
        │ - Std: (7,) 배열                     │
        │ - 전역 통계량으로 저장                │
        └─────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ 3. 에피소드 학습 루프 (train_episode)                           │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 에피소드 시작                        │
        │ - 무작위 시작 인덱스 선택            │
        │ - current_index 초기화               │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 각 스텝마다 반복 (최대 1000 스텝)    │
        └─────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ 4. 관측 생성 단계 (get_observation)                             │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 원본 데이터 수집                     │
        │ - get_candles('ETH', count=20)      │
        │ - 마지막 20봉 데이터                 │
        │ - ⚠️ lookback=40이지만 실제 입력은 20봉│
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 7개 시계열 피처 생성                 │
        │ - (20, 7) 형태                      │
        │ - f1~f7 계산                        │
        │ - f5(Volume), f6(Trades): log1p 적용│
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ Z-Score 정규화 적용                 │
        │ - 전역 통계량 사용                   │
        │ - (20, 7) → 정규화                  │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 전략 점수 수집 (10개)                │
        │ - 각 전략의 analyze() 호출           │
        │ - SHORT 신호는 음수로 인코딩         │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 포지션 정보 수집 (3개)               │
        │ - 포지션 타입 (1/0/-1)              │
        │ - 미실현 PnL                        │
        │ - 보유 시간 (인덱스 차이 기반)      │
        │   (current_index - entry_index) / max_steps│
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 관측 생성 완료                       │
        │ - obs_seq: (1, 20, 7) 텐서          │
        │ - obs_info: (1, 13) 텐서            │
        └─────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ 5. 행동 선택 및 실행 (select_action)                            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ xLSTM 모델에 입력                    │
        │ - obs_seq: (1, 20, 7)               │
        │ - obs_info: (1, 13)                │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ Late Fusion 결합                    │
        │ - 시퀀스 정보 (128차원)              │
        │ - 포지션 정보 (13차원)               │
        │ - 결합: (141차원)                   │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 행동 확률 분포 생성                  │
        │ - [Hold, Long, Short]               │
        │ - Categorical 분포로 샘플링         │
        └─────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ 6. 보상 계산 단계 (calculate_reward)                            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 보상 계산                           │
        │ - 미실현 손익 변화량 보상            │
        │ - 실현 수익/손실 비선형 보상/페널티  │
        │ - 수수료 페널티                     │
        │ - 시간 페널티                       │
        └─────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ 7. 트랜지션 저장 및 학습 (update)                               │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 메모리에 트랜지션 저장               │
        │ - (state, action, log_prob,          │
        │   reward, is_terminal)               │
        └─────────────────────────────────────┘
                              │
                              ▼
        ┌─────────────────────────────────────┐
        │ 256개 트랜지션 모이면 업데이트       │
        │ - GAE 계산                          │
        │ - PPO Loss 계산                     │
        │ - 역전파 및 최적화                   │
        │ - 학습률 스케줄러 업데이트           │
        └─────────────────────────────────────┘
```

---

## 🔍 학습 데이터 피처 상세

### ⚠️ 중요: lookback과 시퀀스 길이의 차이

- **lookback (기본값: 40)**: 전략 지표 계산을 위해 필요한 **여유 캔들 수**
  - 예: 볼린저 밴드(20선), RSI(14선) 등 기술적 지표 계산용
  - 전략의 `analyze()` 메서드가 충분한 과거 데이터를 요구할 때 사용
- **실제 시퀀스 길이 (20)**: 신경망(xLSTM)에 입력되는 **실제 시퀀스 길이**
  - `get_observation()`에서 `get_candles(count=20)` 사용
  - 최종 관측 `obs_seq`의 형태: `(1, 20, 7)`

**요약**: lookback=40은 전략 분석용, 실제 신경망 입력은 20봉

### 1. 시계열 피처 (obs_seq) - 7개 차원

시계열 피처는 **마지막 20봉**의 데이터를 사용하여 생성됩니다.

#### f1: Open (Close 대비)
- **계산식**: `(open - close) / (close + 1e-8)`
- **의미**: 시가가 종가 대비 얼마나 높은지/낮은지
- **범위**: 일반적으로 -0.1 ~ 0.1 (10% 이내)
- **특징**: 양수면 시가 > 종가 (하락), 음수면 시가 < 종가 (상승)

#### f2: High (Close 대비)
- **계산식**: `(high - close) / (close + 1e-8)`
- **의미**: 고가가 종가 대비 얼마나 높은지
- **범위**: 일반적으로 0 ~ 0.1 (상승 여력)
- **특징**: 양수면 상승 여력이 있음을 의미

#### f3: Low (Close 대비)
- **계산식**: `(low - close) / (close + 1e-8)`
- **의미**: 저가가 종가 대비 얼마나 낮은지
- **범위**: 일반적으로 -0.1 ~ 0 (하락 여력)
- **특징**: 음수면 하락 여력이 있음을 의미

#### f4: Log_Return
- **계산식**: `np.diff(np.log(close + 1e-8), prepend=np.log(close[0] + 1e-8))`
- **의미**: 로그 수익률 (가격 변화율의 정상성 확보)
- **범위**: 일반적으로 -0.05 ~ 0.05 (5% 이내)
- **특징**: 
  - 정상성(stationarity) 확보
  - 가격 변화의 상대적 크기 표현
  - 첫 번째 값은 0 (prepend로 인해)

#### f5: Volume
- **계산식**: `np.log1p(volume)` → Z-Score 정규화
- **의미**: 거래량 (거래 활동량)
- **범위**: 로그 변환 후 정규화 (일반적으로 -2 ~ 2)
- **특징**: 
  - **로그 변환 적용**: `log1p = log(1+x)` 사용
  - **이유**: 거래량 폭발 구간(수백 배 차이)의 극단적 차이를 완화
  - 거래 활동의 상대적 강도 표현 (절대값보다 패턴 학습에 유리)

#### f6: Trades
- **계산식**: `np.log1p(trades)` → Z-Score 정규화
- **의미**: 거래 건수 (체결 횟수)
- **범위**: 로그 변환 후 정규화 (일반적으로 -2 ~ 2)
- **특징**: 
  - **로그 변환 적용**: `log1p = log(1+x)` 사용
  - **이유**: 거래 건수 폭발 구간의 극단적 차이를 완화
  - 시장 참여도의 상대적 강도 표현

#### f7: Taker_Ratio
- **계산식**: `taker_buy_base / (volume + 1e-8)`
- **의미**: 매수 거래량 비율 (Taker Buy Volume Ratio)
- **범위**: 0.0 ~ 1.0
- **특징**: 
  - 0.5 이상: 매수 압력 > 매도 압력
  - 0.5 미만: 매도 압력 > 매수 압력
  - 시장 심리 지표

### 2. 정보 피처 (obs_info) - 13개 차원

#### 전략 점수 (10개) - 인덱스 0~9

각 전략의 신뢰도 점수(confidence)를 사용하며, SHORT 신호는 음수로 인코딩됩니다.

| 인덱스 | 전략명 | 신호 타입 | 점수 범위 | 설명 |
|--------|--------|-----------|-----------|------|
| 0 | BTC-ETH Correlation | LONG/SHORT | -1.0 ~ 1.0 | BTC와 ETH의 상관관계 기반 신호 |
| 1 | Volatility Squeeze | LONG/SHORT | -1.0 ~ 1.0 | 변동성 스퀴즈 후 폭발 신호 |
| 2 | Orderblock/FVG | LONG/SHORT | -1.0 ~ 1.0 | 오더블록 및 FVG 패턴 신호 |
| 3 | HMA Momentum | LONG/SHORT | -1.0 ~ 1.0 | Hull Moving Average 모멘텀 |
| 4 | MFI Momentum | LONG/SHORT | -1.0 ~ 1.0 | Money Flow Index 모멘텀 |
| 5 | Bollinger Mean Reversion | LONG/SHORT | -1.0 ~ 1.0 | 볼린저 밴드 평균 회귀 |
| 6 | VWAP Deviation | LONG/SHORT | -1.0 ~ 1.0 | VWAP 편차 평균 회귀 |
| 7 | Range Top/Bottom | LONG/SHORT | -1.0 ~ 1.0 | 범위 상단/하단 반전 |
| 8 | Stoch RSI Mean Reversion | LONG/SHORT | -1.0 ~ 1.0 | Stochastic RSI 평균 회귀 |
| 9 | CMF Divergence | LONG/SHORT | -1.0 ~ 1.0 | Chaikin Money Flow 다이버전스 |

**부호 인코딩 규칙**:
- `signal == 'LONG'`: 점수 그대로 사용 (양수)
- `signal == 'SHORT'`: 점수를 음수로 변환 (`score = -score`)
- 신호 없음: `0.0`

#### 포지션 정보 (3개) - 인덱스 10~12

| 인덱스 | 이름 | 범위 | 설명 |
|--------|------|------|------|
| 10 | Position Type | -1.0, 0.0, 1.0 | 현재 포지션 타입 |
| 11 | Unrealized PnL | 실수 | 미실현 손익 (스케일: ×10) |
| 12 | Holding Time | 실수 | 보유 시간 (정규화: 0~1) |

**Position Type**:
- `1.0`: LONG 포지션 보유
- `0.0`: 포지션 없음 (HOLD)
- `-1.0`: SHORT 포지션 보유

**Unrealized PnL**:
- 계산식: `(현재가 - 진입가) / 진입가` (LONG) 또는 `(진입가 - 현재가) / 진입가` (SHORT)
- 스케일 조정: `pnl * 10` (학습 안정화를 위해)
- 범위: 일반적으로 -0.1 ~ 0.1 (10% 이내)

**Holding Time** (⚠️ **중요: 버그 수정됨**):
- **계산식**: `(current_index - entry_index) / max_steps`
- **의미**: 포지션 보유 시간을 **캔들 인덱스 차이**로 계산 (과거 데이터 학습용)
- **범위**: 0.0 ~ 1.0 (에피소드 최대 길이로 정규화)
- **이전 버그**: `datetime.now()` 사용 시 과거 데이터 학습에서 보유 시간이 비정상적으로 큰 값으로 고정되는 문제
- **수정 내용**: 실시간 시간 대신 **캔들 인덱스 차이**를 사용하여 정확한 보유 시간 계산

---

## 📐 데이터 차원 및 형태

### 최종 관측 (Observation)

```python
obs = (obs_seq, obs_info)
```

- **obs_seq**: `torch.Tensor` 형태, `(1, 20, 7)`
  - 배치 크기: 1
  - 시퀀스 길이: 20 (마지막 20봉)
  - 피처 차원: 7 (시계열 피처)
  
- **obs_info**: `torch.Tensor` 형태, `(1, 13)`
  - 배치 크기: 1
  - 피처 차원: 13 (전략 점수 10 + 포지션 정보 3)

### 전처리 파이프라인

1. **원본 데이터 수집**: `get_candles('ETH', count=20)`
   - DataFrame 형태: `['open', 'high', 'low', 'close', 'volume', 'trades', 'taker_buy_base']`
   - ⚠️ **참고**: `lookback=40`이지만 실제로는 20봉만 사용 (신경망 입력용)

2. **7개 피처 계산**: NumPy 배열로 변환
   - 형태: `(20, 7)` - float32
   - **로그 변환 적용**: f5(Volume), f6(Trades)에 `np.log1p()` 적용

3. **Z-Score 정규화**: `preprocessor.transform()`
   - 전역 통계량 사용 (학습 시작 전 `_fit_global_scaler()`에서 계산)
   - 공식: `(x - mean) / std`
   - 형태: `(20, 7)` - float32
   - **로그 변환된 Volume/Trades도 함께 정규화**

4. **텐서 변환**: PyTorch 텐서로 변환
   - `obs_seq`: `(1, 20, 7)` - FloatTensor
   - `obs_info`: `(1, 13)` - FloatTensor

---

## 🔄 학습 루프 상세

### 에피소드 구조

```
에피소드 시작
  ↓
무작위 시작 인덱스 선택 (40 ~ total_candles - max_steps)
  ↓
for step in range(max_steps):  # 최대 1000 스텝
  ↓
  current_index += 1  # 다음 캔들로 이동
  ↓
  관측 생성 (get_observation)
    - 7개 시계열 피처 생성
    - 10개 전략 점수 수집
    - 3개 포지션 정보 수집
  ↓
  행동 선택 (select_action)
    - xLSTM 모델에 입력
    - 행동 확률 분포 생성
    - 샘플링하여 행동 선택
  ↓
  보상 계산 (calculate_reward)
    - 미실현 손익 변화량
    - 실현 수익/손실
    - 수수료 및 시간 페널티
  ↓
  트랜지션 저장
    - (state, action, log_prob, reward, is_terminal)
  ↓
  if len(memory) >= 256:
    - PPO 업데이트 수행
    - 메모리 초기화
  ↓
에피소드 종료
```

### PPO 업데이트 상세

```
256개 트랜지션 수집
  ↓
GAE (Generalized Advantage Estimation) 계산
  - 다음 상태의 가치 예측 (Bootstrap)
  - 어드밴티지 계산
  - 반환값(Returns) 계산
  ↓
for epoch in range(k_epochs):  # 10번 반복
  ↓
  현재 정책으로 행동 확률 재계산
  ↓
  PPO Loss 계산
    - Actor Loss: 클리핑된 목적 함수
    - Critic Loss: MSE (가치 함수)
    - Entropy Bonus: 탐험 장려
  ↓
  역전파 및 최적화
    - 그래디언트 클리핑 (0.5)
    - Adam 최적화
  ↓
학습률 스케줄러 업데이트
  - 200번 업데이트마다 학습률 0.5배 감소
```

---

## 📊 데이터 통계

### 스케일러 학습 데이터

- **샘플 수**: 최대 50,000개
- **윈도우 크기**: 20봉
- **총 피처 수**: 50,000 × 20 = 1,000,000개 샘플
- **피처 차원**: 7개
- **정규화 방법**: Z-Score (전역 통계량)

### 학습 에피소드 데이터

- **에피소드당 최대 스텝**: 1,000
- **트랜지션 버퍼 크기**: 256
- **업데이트 주기**: 256개 트랜지션마다
- **에피소드당 업데이트 횟수**: 약 4회 (1000 / 256)

---

## 🎯 핵심 설계 원칙

1. **차원 일치**: 스케일러 학습과 관측 생성 모두 7개 피처 사용
2. **Late Fusion**: 시계열 정보와 포지션 정보를 분리하여 처리
3. **전역 정규화**: 전체 데이터셋의 통계량으로 정규화 (일관성 확보)
4. **부호 인코딩**: SHORT 신호를 음수로 인코딩하여 방향성 표현
5. **실시간 포지션 정보**: 현재 포지션 상태를 관측에 포함
6. **로그 변환**: Volume과 Trades에 `log1p` 적용하여 극단적 차이 완화
7. **인덱스 기반 시간 계산**: 과거 데이터 학습을 위해 캔들 인덱스 차이로 보유 시간 계산

---

## 📝 참고 사항

- **데이터 소스**: `data/eth_3m_1year.csv`, `data/btc_3m_1year.csv`
- **전처리 모듈**: `model/preprocess.py` - `DataPreprocessor`
- **환경 모듈**: `model/trading_env.py` - `TradingEnvironment`
- **에이전트 모듈**: `model/ppo_agent.py` - `PPOAgent`
- **학습 스크립트**: `model/train_ppo.py` - `PPOTrainer`
