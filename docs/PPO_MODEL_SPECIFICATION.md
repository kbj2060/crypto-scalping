# PPO ëª¨ë¸ ëª…ì„¸ì„œ (PPO Model Specification)

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ë°ì´í„° íë¦„](#ë°ì´í„°-íë¦„)
3. [ëª¨ë¸ ì•„í‚¤í…ì²˜](#ëª¨ë¸-ì•„í‚¤í…ì²˜)
4. [ì•¡ì…˜ ì²´ê³„](#ì•¡ì…˜-ì²´ê³„)
5. [ë³´ìƒ ì²´ê³„](#ë³´ìƒ-ì²´ê³„)
6. [í•™ìŠµ í”„ë¡œì„¸ìŠ¤](#í•™ìŠµ-í”„ë¡œì„¸ìŠ¤)
7. [í•˜ì´í¼íŒŒë¼ë¯¸í„°](#í•˜ì´í¼íŒŒë¼ë¯¸í„°)

---

## 1. ê°œìš”

ì´ PPO ëª¨ë¸ì€ ì•”í˜¸í™”í ìŠ¤ìº˜í•‘ ê±°ë˜ë¥¼ ìœ„í•œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. xLSTM ê¸°ë°˜ Actor-Critic ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , Dense Reward ì²´ê³„ë¥¼ í†µí•´ ë§¤ ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ì‹ í˜¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- **xLSTM ê¸°ë°˜**: Multi-Layer sLSTM + Pre-LN Residual Connection
- **Dense Reward**: ë§¤ ìŠ¤í… í‰ê°€ê¸ˆì•¡ ë³€í™”ì— ë³´ìƒ ë¶€ì—¬
- **AI íŒë‹¨ ì²­ì‚°**: Action 0ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ ì²­ì‚° ì‹œì  ê²°ì •
- **State Retention**: LSTM ìƒíƒœë¥¼ ì—í”¼ì†Œë“œ ë‚´ì—ì„œ ìœ ì§€

---

## 2. ë°ì´í„° íë¦„

### 2.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
[ì›ë³¸ ë°ì´í„°] 
    â†“
[Feature Engineering] â†’ training_features.csv (29ê°œ í”¼ì²˜)
    â†“
[Strategy Pre-calculation] â†’ cached_strategies.csv (12ê°œ ì „ëµ ì ìˆ˜)
    â†“
[Data Loading] â†’ training_features.csv + cached_strategies.csv ë³‘í•©
    â†“
[Scaler Training] â†’ Train Set 80%ë§Œ ì‚¬ìš© (Data Leakage ë°©ì§€)
    â†“
[Episode Loop]
    â”œâ”€ Observation (obs_seq, obs_info) ìƒì„±
    â”œâ”€ Action ì„ íƒ (0, 1, 2)
    â”œâ”€ Trading Logic ì‹¤í–‰
    â”œâ”€ Reward ê³„ì‚° (Dense Reward)
    â””â”€ PPO Update (GAE + Clipped Surrogate)
    â†“
[Model Save] â†’ ppo_model_best.pth / ppo_model_last.pth
```

### 2.2 ë°ì´í„° ë¶„í• 

- **Train Set**: 70% (ì•ë¶€ë¶„)
- **Validation Set**: 15% (ì¤‘ê°„)
- **Test Set**: 15% (ë’·ë¶€ë¶„)

**ì¤‘ìš”**: ìŠ¤ì¼€ì¼ëŸ¬ëŠ” Train Set 80%ë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.

### 2.3 í”¼ì²˜ êµ¬ì„±

#### ì‹œê³„ì—´ í”¼ì²˜ (29ê°œ)
```
1. log_return              # ë¡œê·¸ ìˆ˜ìµë¥ 
2. roll_return_6           # 6ë´‰ ë¡¤ë§ ìˆ˜ìµë¥ 
3. atr_ratio               # ATR ë¹„ìœ¨
4. bb_width                # ë³¼ë¦°ì € ë°´ë“œ í­
5. bb_pos                  # ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜
6. rsi                     # RSI (14)
7. macd_hist               # MACD íˆìŠ¤í† ê·¸ë¨
8. hma_ratio               # HMA ë¹„ìœ¨
9. cci                     # CCI
10. rvol                    # ìƒëŒ€ ë³€ë™ì„±
11. taker_ratio             # í…Œì´ì»¤ ë¹„ìœ¨
12. cvd_change              # CVD ë³€í™”ëŸ‰
13. mfi                     # MFI
14. cmf                     # CMF
15. vwap_dist               # VWAP ê±°ë¦¬
16. wick_upper              # ìƒë‹¨ ì‹¬ì§€
17. wick_lower              # í•˜ë‹¨ ì‹¬ì§€
18. range_pos               # ë ˆì¸ì§€ ìœ„ì¹˜
19. swing_break             # ìŠ¤ìœ™ ë¸Œë ˆì´í¬
20. chop                    # Choppiness Index
21. btc_return              # BTC ìˆ˜ìµë¥ 
22. btc_rsi                 # BTC RSI
23. btc_corr                # BTC ìƒê´€ê³„ìˆ˜
24. btc_vol                 # BTC ë³€ë™ì„±
25. eth_btc_ratio           # ETH/BTC ë¹„ìœ¨
26. rsi_15m                 # 15ë¶„ë´‰ RSI
27. trend_15m               # 15ë¶„ë´‰ ì¶”ì„¸
28. rsi_1h                  # 1ì‹œê°„ë´‰ RSI
29. trend_1h                # 1ì‹œê°„ë´‰ ì¶”ì„¸
```

#### ì „ëµ ì ìˆ˜ (12ê°œ)
```
strategy_0:  BTCEthCorrelationStrategy
strategy_1:  VolatilitySqueezeStrategy
strategy_2:  OrderblockFVGStrategy
strategy_3:  HMAMomentumStrategy
strategy_4:  MFIMomentumStrategy
strategy_5:  BollingerMeanReversionStrategy
strategy_6:  VWAPDeviationStrategy
strategy_7:  RangeTopBottomStrategy
strategy_8:  StochRSIMeanReversionStrategy
strategy_9:  CMFDivergenceStrategy
strategy_10: CCIReversalStrategy
strategy_11: WilliamsRStrategy
```

ê° ì „ëµ ì ìˆ˜ëŠ” `-confidence ~ +confidence` ë²”ìœ„ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤.

#### í¬ì§€ì…˜ ì •ë³´ (3ê°œ)
```
[0]: position_value    # 1.0 (LONG), -1.0 (SHORT), 0.0 (None)
[1]: unrealized_pnl    # í‰ê°€ì†ìµ (Ã—10 ìŠ¤ì¼€ì¼ë§)
[2]: holding_time      # ë³´ìœ  ì‹œê°„ (ì •ê·œí™”: holding_time / max_steps)
```

### 2.4 ê´€ì¸¡ê°’ (Observation) êµ¬ì¡°

```python
obs_seq: torch.Tensor  # Shape: (1, LOOKBACK, 29)
    # ìµœê·¼ 40ë´‰ì˜ 29ê°œ ì‹œê³„ì—´ í”¼ì²˜ (Z-Score ì •ê·œí™”)

obs_info: torch.Tensor  # Shape: (1, 15)
    # [12ê°œ ì „ëµ ì ìˆ˜] + [3ê°œ í¬ì§€ì…˜ ì •ë³´] = 15ì°¨ì›
```

**ë°˜í™˜ í˜•ì‹**: `(obs_seq, obs_info)` íŠœí”Œ

---

## 3. ëª¨ë¸ ì•„í‚¤í…ì²˜

### 3.1 xLSTMActorCritic ë„¤íŠ¸ì›Œí¬

#### ì…ë ¥ ì°¨ì›
- **obs_seq**: `(batch, LOOKBACK, 29)` â†’ `(batch, 40, 29)`
- **obs_info**: `(batch, 15)` â†’ `(batch, 12 + 3)`

#### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°

```
Input (obs_seq: 29ì°¨ì›)
    â†“
[Input Projection] â†’ Linear(29 â†’ 128) + LayerNorm
    â†“
[Multi-Layer xLSTM Stack] (2 layers)
    â”œâ”€ Layer 1: sLSTMCell(128 â†’ 128) + Pre-LN Residual
    â””â”€ Layer 2: sLSTMCell(128 â†’ 128) + Pre-LN Residual
    â†“
[Multi-Head Attention] â†’ Weighted Pooling (4 heads)
    â†“ (128ì°¨ì›)
[Late Fusion] â†’ Concat([attention_output, info_encoded])
    â†“ (128 + 64 = 192ì°¨ì›)
[Info Encoder] â†’ Linear(15 â†’ 64) + LayerNorm + GELU
    â†“ (64ì°¨ì›)
[Shared Trunk]
    â”œâ”€ Linear(192 â†’ 256) + LayerNorm + GELU + Dropout(0.1)
    â””â”€ Linear(256 â†’ 128) + LayerNorm + GELU
    â†“ (128ì°¨ì›)
[Separate Heads]
    â”œâ”€ Actor Head: Linear(128 â†’ 64) â†’ Linear(64 â†’ 3) + Softmax
    â””â”€ Critic Head: Linear(128 â†’ 64) â†’ Linear(64 â†’ 1)
    â†“
Output: (action_probs: [3], value: [1])
```

#### ì£¼ìš” ì»´í¬ë„ŒíŠ¸

1. **sLSTMCell**: Exponential Gatingì„ í†µí•œ ë©”ëª¨ë¦¬ ê°•í™”
   - Input Gate: `i = exp(clamp(i, -5, 5))`
   - Forget Gate: `f = exp(clamp(f, -5, 5))`
   - Cell State: `c_next = f * c + i * tanh(z)`
   - Normalizer: `n_next = f * n + i`
   - Hidden: `h_next = sigmoid(o) * (c_next / n_next)`

2. **Multi-Head Attention**: Weighted Pooling
   - 4ê°œ í—¤ë“œë¡œ ì‹œí€€ìŠ¤ ë‚´ ì¤‘ìš” ì‹œì  í•™ìŠµ
   - í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¡œ í’€ë§

3. **Pre-LN Residual**: `Norm(x) â†’ Layer â†’ x + Output`
   - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì•ˆì •ì ì¸ í•™ìŠµ

4. **State Retention**: LSTM ìƒíƒœë¥¼ ì—í”¼ì†Œë“œ ë‚´ì—ì„œ ìœ ì§€
   - `states = (h, c, n)` í˜•íƒœë¡œ ê´€ë¦¬
   - ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ `reset_episode_states()` í˜¸ì¶œ

### 3.2 ì¶œë ¥

- **Actor Output**: `action_probs` - Shape: `(batch, 3)`
  - `[P(Action 0), P(Action 1), P(Action 2)]`
  - Softmaxë¡œ ì •ê·œí™”ë˜ì–´ í™•ë¥  ë¶„í¬ í˜•ì„±

- **Critic Output**: `value` - Shape: `(batch, 1)`
  - ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ V(s) ì¶”ì •

---

## 4. ì•¡ì…˜ ì²´ê³„

### 4.1 ì•¡ì…˜ ê³µê°„

**Discrete Action Space**: 3ê°œ ì•¡ì…˜

| Action | ì˜ë¯¸ | ë™ì‘ |
|--------|------|------|
| **0** | **AI íŒë‹¨ ì²­ì‚°** | í¬ì§€ì…˜ì´ ìˆì„ ë•Œ â†’ ì¦‰ì‹œ ì²­ì‚° (ìµì ˆ/ì†ì ˆ)<br>í¬ì§€ì…˜ì´ ì—†ì„ ë•Œ â†’ ê´€ë§ (í˜„ê¸ˆ ë³´ìœ ) |
| **1** | **LONG** | í¬ì§€ì…˜ì´ ì—†ì„ ë•Œ â†’ LONG ì§„ì…<br>SHORT í¬ì§€ì…˜ì¼ ë•Œ â†’ ìŠ¤ìœ„ì¹­ (SHORT ì²­ì‚° + LONG ì§„ì…)<br>ì´ë¯¸ LONGì¼ ë•Œ â†’ ìœ ì§€ (Keep Holding) |
| **2** | **SHORT** | í¬ì§€ì…˜ì´ ì—†ì„ ë•Œ â†’ SHORT ì§„ì…<br>LONG í¬ì§€ì…˜ì¼ ë•Œ â†’ ìŠ¤ìœ„ì¹­ (LONG ì²­ì‚° + SHORT ì§„ì…)<br>ì´ë¯¸ SHORTì¼ ë•Œ â†’ ìœ ì§€ (Keep Holding) |

### 4.2 ì•¡ì…˜ ì‹¤í–‰ ë¡œì§

```python
# ìµœì†Œ ë³´ìœ  ì‹œê°„ ì ê¸ˆ (Churning ë°©ì§€)
is_locked = (current_position is not None) and (holding_time < 3)

# A. ê°•ì œ ì•ˆì „ì¥ì¹˜ (Stop Loss -2%)
if unrealized_pnl < -0.02:
    â†’ ì¦‰ì‹œ ì²­ì‚° (ì ê¸ˆ ë¬´ì‹œ)

# B. AI íŒë‹¨ í–‰ë™ (ì ê¸ˆ í•´ì œ ì‹œ)
if action == 0 and current_position is not None:
    â†’ AI íŒë‹¨ ì²­ì‚° (ìµì ˆ/ì†ì ˆ)
    
if action == 1:
    if current_position == 'SHORT': â†’ ìŠ¤ìœ„ì¹­
    elif current_position is None: â†’ LONG ì§„ì…
    else: â†’ ìœ ì§€
    
if action == 2:
    if current_position == 'LONG': â†’ ìŠ¤ìœ„ì¹­
    elif current_position is None: â†’ SHORT ì§„ì…
    else: â†’ ìœ ì§€
```

### 4.3 ì•¡ì…˜ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜

1. **í™•ë¥  ë¶„í¬**: Categorical Distribution ì‚¬ìš©
   - `action_probs`ì—ì„œ ìƒ˜í”Œë§
   - íƒí—˜ì„ ìœ„í•´ ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ì ìš©

2. **ì—”íŠ¸ë¡œí”¼ ìŠ¤ì¼€ì¤„ë§**:
   ```
   entropy_coef = max(0.02, 0.05 * (0.999 ^ episode))
   ```
   - ì´ˆê¸°: 0.05 (ë†’ì€ íƒí—˜)
   - ì ì§„ì  ê°ì†Œ: 0.999^episode
   - ìµœì†Œê°’: 0.02 (ì§€ì†ì  íƒí—˜ ìœ ì§€)

---

## 5. ë³´ìƒ ì²´ê³„

### 5.1 Dense Reward êµ¬ì¡°

**í•µì‹¬ ê°œë…**: ë§¤ ìŠ¤í…ë§ˆë‹¤ í‰ê°€ê¸ˆì•¡ ë³€í™”ì— ë³´ìƒì„ ë¶€ì—¬í•˜ì—¬ í•™ìŠµ ì‹ í˜¸ë¥¼ ë°€ë„ ìˆê²Œ ì œê³µí•©ë‹ˆë‹¤.

#### ë³´ìƒ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜
```python
calculate_reward(step_pnl, realized_pnl, trade_done, holding_time)
```

#### ë³´ìƒ êµ¬ì„± ìš”ì†Œ

**1. ê³¼ì • ë³´ìƒ (Shaping Reward)**
```python
reward += step_pnl * 50.0
```
- `step_pnl = unrealized_pnl - prev_unrealized_pnl`
- í¬ì§€ì…˜ì„ ë“¤ê³  ìˆëŠ” ë™ì•ˆ ê°€ê²©ì´ ìœ ë¦¬í•˜ê²Œ ê°€ë©´ ë³´ìƒ
- ë¶ˆë¦¬í•˜ê²Œ ê°€ë©´ ë²Œì 
- **ëª©ì **: í¬ì§€ì…˜ ìœ ì§€ ì¤‘ì—ë„ í•™ìŠµ ì‹ í˜¸ ì œê³µ

**2. ê²°ê³¼ ë³´ìƒ (Terminal Reward)**
```python
if trade_done:
    fee = 0.0015  # TRANSACTION_COST
    net_pnl = realized_pnl - fee
    
    if net_pnl > 0:
        reward += net_pnl * 100.0  # ìˆ˜ìµì€ í¬ê²Œ ì¹­ì°¬
        reward += 1.0              # ìŠ¹ë¦¬ ë³´ë„ˆìŠ¤
    else:
        reward += net_pnl * 80.0   # ì†ì‹¤ì€ ì•„í”„ê²Œ
```
- ê±°ë˜ ì¢…ë£Œ ì‹œ í™•ì • ì†ìµì— ëŒ€í•œ ë³´ìƒ/í˜ë„í‹°
- ìˆ˜ìˆ˜ë£Œë¥¼ ë°˜ì˜í•œ ìˆœìˆ˜ìµ ê¸°ì¤€

**3. í™€ë”© ë¹„ìš© (Holding Cost)**
```python
if not trade_done:
    reward -= 0.0005 * holding_time
```
- í¬ì§€ì…˜ì„ ë„ˆë¬´ ì˜¤ë˜ ë“¤ê³  ìˆìœ¼ë©´ ë¯¸ë¯¸í•œ í˜ë„í‹°
- ë¹ ë¥¸ ìµì ˆì„ ìœ ë„

#### ë³´ìƒ í´ë¦¬í•‘
```python
reward = clip(reward, -10, 10)
```
- ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ ë³´ìƒ ë²”ìœ„ ì œí•œ

### 5.2 ë³´ìƒ ê³„ì‚° ì˜ˆì‹œ

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ìˆ˜ìµì„± ìˆëŠ” í™€ë”©
```
Step 1: LONG ì§„ì… (entry_price = $3000)
Step 2: ê°€ê²© $3003 (+0.1%) â†’ step_pnl = +0.001
        reward = 0.001 * 50.0 = +0.05
Step 3: ê°€ê²© $3006 (+0.2%) â†’ step_pnl = +0.001
        reward = 0.001 * 50.0 = +0.05
Step 4: AI íŒë‹¨ ì²­ì‚° (Action 0) â†’ realized_pnl = +0.002
        reward = 0.002 * 50.0 + (0.002 - 0.0015) * 100.0 + 1.0
               = 0.1 + 0.05 + 1.0 = +1.15
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì†ì‹¤ì„± í™€ë”©
```
Step 1: LONG ì§„ì… (entry_price = $3000)
Step 2: ê°€ê²© $2997 (-0.1%) â†’ step_pnl = -0.001
        reward = -0.001 * 50.0 = -0.05
Step 3: ê°€ê²© $2994 (-0.2%) â†’ step_pnl = -0.001
        reward = -0.001 * 50.0 = -0.05
Step 4: Stop Loss ë°œë™ (-2%) â†’ realized_pnl = -0.02
        reward = -0.02 * 50.0 + (-0.02 - 0.0015) * 80.0
               = -1.0 + (-1.72) = -2.72
```

### 5.3 ë³´ìƒ ì²´ê³„ì˜ ì¥ì 

1. **ë¹ ë¥¸ í•™ìŠµ**: ë§¤ ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ì‹ í˜¸ ì œê³µ
2. **í¬ì§€ì…˜ ìœ ì§€ ì¸ì„¼í‹°ë¸Œ**: ìˆ˜ìµì„± ìˆëŠ” í¬ì§€ì…˜ì„ ìœ ì§€í•˜ë©´ ì§€ì†ì  ë³´ìƒ
3. **ì•ˆì •ì„±**: í´ë¦¬í•‘ìœ¼ë¡œ ë³´ìƒ í­ë°œ ë°©ì§€
4. **ê· í˜•**: ê³¼ì • ë³´ìƒ(50.0)ê³¼ ê²°ê³¼ ë³´ìƒ(100.0)ì˜ ì ì ˆí•œ ë¹„ìœ¨

---

## 6. í•™ìŠµ í”„ë¡œì„¸ìŠ¤

### 6.1 ì—í”¼ì†Œë“œ êµ¬ì¡°

```
Episode Start
    â†“
[1] LSTM ìƒíƒœ ì´ˆê¸°í™” (reset_episode_states)
    â†“
[2] ëœë¤ ì‹œì‘ì  ì„ íƒ (Train Set ë‚´)
    â†“
[3] For each step (max 480 steps):
    â”œâ”€ Observation ìƒì„± (obs_seq, obs_info)
    â”œâ”€ Action ì„ íƒ (Categorical Sampling)
    â”œâ”€ Trading Logic ì‹¤í–‰
    â”‚   â”œâ”€ Stop Loss ì²´í¬ (-2%)
    â”‚   â”œâ”€ Action 0: AI íŒë‹¨ ì²­ì‚°
    â”‚   â”œâ”€ Action 1: LONG ì§„ì…/ìŠ¤ìœ„ì¹­/ìœ ì§€
    â”‚   â””â”€ Action 2: SHORT ì§„ì…/ìŠ¤ìœ„ì¹­/ìœ ì§€
    â”œâ”€ Reward ê³„ì‚° (Dense Reward)
    â”œâ”€ Transition ì €ì¥ (state, action, reward, next_state, prob, done)
    â””â”€ Next State ìƒì„±
    â†“
[4] PPO Update (GAE + Clipped Surrogate)
    â”œâ”€ GAE ê³„ì‚° (Generalized Advantage Estimation)
    â”œâ”€ PPO Loss ê³„ì‚° (10 epochs)
    â””â”€ Gradient Update
    â†“
Episode End
```

### 6.2 PPO ì—…ë°ì´íŠ¸ ê³¼ì •

#### 1. GAE (Generalized Advantage Estimation)
```python
# TD Target
td_target = r + gamma * V(next_state) * (1 - done)

# TD Error
delta = td_target - V(state)

# GAE (Backward Pass)
gae = delta + gamma * lambda * gae_prev

# Returns
returns = gae + V(state)
```

#### 2. PPO Loss
```python
# Policy Ratio
ratio = exp(log_prob_new - log_prob_old)

# Clipped Surrogate
surr1 = ratio * advantage
surr2 = clip(ratio, 1-eps, 1+eps) * advantage
actor_loss = -min(surr1, surr2).mean()

# Critic Loss
critic_loss = SmoothL1Loss(V(state), returns)

# Entropy Bonus
entropy_loss = dist.entropy().mean()

# Total Loss
loss = actor_loss + 1.0 * critic_loss - entropy_coef * entropy_loss
```

#### 3. í•™ìŠµ íŒŒë¼ë¯¸í„°
- **Learning Rate**: 3e-5 (Adam Optimizer)
- **Gradient Clipping**: 0.5
- **Update Epochs**: 10 (k_epochs)
- **Batch Size**: ë©”ëª¨ë¦¬ì— ìŒ“ì¸ ëª¨ë“  íŠ¸ëœì§€ì…˜

### 6.3 ëª¨ë¸ ì €ì¥

- **Best Model**: `ppo_model_best.pth` + `ppo_model_best_scaler.pkl`
  - ìµœê³  ì ìˆ˜ ê°±ì‹  ì‹œ ì €ì¥
  - ì‹¤ì „ íˆ¬ì…ìš©

- **Last Model**: `ppo_model_last.pth` + `ppo_model_last_scaler.pkl`
  - 10 ì—í”¼ì†Œë“œë§ˆë‹¤ ì €ì¥
  - í•™ìŠµ ì¬ê°œìš©

---

## 7. í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 7.1 PPO ì•Œê³ ë¦¬ì¦˜

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `PPO_GAMMA` | 0.99 | í• ì¸ìœ¨ (Discount Factor) |
| `PPO_LAMBDA` | 0.95 | GAE ëŒë‹¤ íŒŒë¼ë¯¸í„° |
| `PPO_EPS_CLIP` | 0.2 | PPO í´ë¦¬í•‘ ë²”ìœ„ |
| `PPO_K_EPOCHS` | 10 | PPO ì—…ë°ì´íŠ¸ ë°˜ë³µ íšŸìˆ˜ |
| `PPO_LEARNING_RATE` | 3e-5 | í•™ìŠµë¥  |
| `PPO_ENTROPY_COEF` | 0.05 | ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (ì´ˆê¸°ê°’) |
| `PPO_ENTROPY_DECAY` | 0.999 | ì—”íŠ¸ë¡œí”¼ ê°ì†Œìœ¨ |
| `PPO_ENTROPY_MIN` | 0.02 | ì—”íŠ¸ë¡œí”¼ ìµœì†Œê°’ |

### 7.2 ë³´ìƒ í•¨ìˆ˜

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `REWARD_MULTIPLIER` | 300 | (ì‚¬ìš© ì•ˆ í•¨, Dense Reward ì‚¬ìš©) |
| `LOSS_PENALTY_MULTIPLIER` | 500 | (ì‚¬ìš© ì•ˆ í•¨, Dense Reward ì‚¬ìš©) |
| `TRANSACTION_COST` | 0.0015 | ê±°ë˜ ë¹„ìš© (0.15%) |
| `TIME_COST` | 0.0005 | ì‹œê°„ ë¹„ìš© |
| `STOP_LOSS_THRESHOLD` | -0.02 | ê°•ì œ ì†ì ˆ ì„ê³„ê°’ (-2%) |
| **Step PnL Multiplier** | **50.0** | ê³¼ì • ë³´ìƒ ë°°ìœ¨ |
| **Terminal PnL Multiplier** | **100.0** | ê²°ê³¼ ë³´ìƒ ë°°ìœ¨ (ìˆ˜ìµ) |
| **Terminal Loss Multiplier** | **80.0** | ê²°ê³¼ ë³´ìƒ ë°°ìœ¨ (ì†ì‹¤) |
| **Holding Cost** | **0.0005** | í™€ë”© ë¹„ìš© (ìŠ¤í…ë‹¹) |

### 7.3 ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `LOOKBACK` | 40 | ì‹œê³„ì—´ ë´‰ ê°œìˆ˜ |
| `NETWORK_HIDDEN_DIM` | 128 | ì€ë‹‰ì¸µ ì°¨ì› |
| `NETWORK_NUM_LAYERS` | 2 | xLSTM ë ˆì´ì–´ ê°œìˆ˜ |
| `NETWORK_DROPOUT` | 0.1 | Dropout ë¹„ìœ¨ |
| `NETWORK_ATTENTION_HEADS` | 4 | Multi-Head Attention í—¤ë“œ ê°œìˆ˜ |
| `NETWORK_INFO_ENCODER_DIM` | 64 | Info Encoder ì¶œë ¥ ì°¨ì› |
| `NETWORK_SHARED_TRUNK_DIM1` | 256 | Shared Trunk ì²« ë²ˆì§¸ ë ˆì´ì–´ |
| `NETWORK_SHARED_TRUNK_DIM2` | 128 | Shared Trunk ë‘ ë²ˆì§¸ ë ˆì´ì–´ |
| `NETWORK_ACTOR_HEAD_DIM` | 64 | Actor Head ì€ë‹‰ì¸µ |
| `NETWORK_CRITIC_HEAD_DIM` | 64 | Critic Head ì€ë‹‰ì¸µ |

### 7.4 í•™ìŠµ ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `TRAIN_SPLIT` | 0.7 | í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (70%) |
| `VAL_SPLIT` | 0.85 | ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (85%) |
| `TRAIN_MAX_STEPS_PER_EPISODE` | 480 | ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ |
| `TRAIN_NUM_EPISODES` | 2000 | ì´ ì—í”¼ì†Œë“œ ìˆ˜ |
| `MIN_HOLDING_TIME` | 3 | ìµœì†Œ ë³´ìœ  ìº”ë“¤ ìˆ˜ (Churning ë°©ì§€) |
| `TRAIN_SAMPLE_SIZE` | 50000 | ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ìƒ˜í”Œ í¬ê¸° |

---

## 8. ë°ì´í„° ì°¨ì› ìš”ì•½

### 8.1 ì…ë ¥ ì°¨ì›

| êµ¬ì„± ìš”ì†Œ | ì°¨ì› | ì„¤ëª… |
|---------|------|------|
| `obs_seq` | `(1, 40, 29)` | ì‹œê³„ì—´ í”¼ì²˜ (40ë´‰ Ã— 29ê°œ í”¼ì²˜) |
| `obs_info` | `(1, 15)` | ì „ëµ ì ìˆ˜(12) + í¬ì§€ì…˜ ì •ë³´(3) |
| **Total Input** | **1,175ì°¨ì›** | `40 Ã— 29 + 15 = 1,175` |

### 8.2 ì¶œë ¥ ì°¨ì›

| êµ¬ì„± ìš”ì†Œ | ì°¨ì› | ì„¤ëª… |
|---------|------|------|
| `action_probs` | `(1, 3)` | ì•¡ì…˜ í™•ë¥  ë¶„í¬ [P(0), P(1), P(2)] |
| `value` | `(1, 1)` | ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ V(s) |

### 8.3 ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ ì°¨ì›

| ë ˆì´ì–´ | ì…ë ¥ ì°¨ì› | ì¶œë ¥ ì°¨ì› |
|--------|---------|---------|
| Input Projection | 29 | 128 |
| xLSTM Layer 1 | 128 | 128 |
| xLSTM Layer 2 | 128 | 128 |
| Attention Pooling | (40, 128) | 128 |
| Info Encoder | 15 | 64 |
| Late Fusion | - | 192 (128 + 64) |
| Shared Trunk | 192 | 128 |
| Actor Head | 128 | 3 |
| Critic Head | 128 | 1 |

---

## 9. í•™ìŠµ ë£¨í”„ ìƒì„¸

### 9.1 ë‹¨ì¼ ìŠ¤í… ì²˜ë¦¬

```python
# 1. Observation ìƒì„±
state = env.get_observation(
    position_info=[pos_val, unrealized_pnl*10, holding_time/max_steps],
    current_index=current_idx
)
# Returns: (obs_seq: (1,40,29), obs_info: (1,15))

# 2. Action ì„ íƒ
action, log_prob = agent.select_action(state)
# LSTM ìƒíƒœ ìœ ì§€: self.current_states ì—…ë°ì´íŠ¸

# 3. Trading Logic
if action == 0 and current_position is not None:
    â†’ ì²­ì‚° (realized_pnl ê³„ì‚°)
elif action == 1:
    â†’ LONG ì§„ì…/ìŠ¤ìœ„ì¹­/ìœ ì§€
elif action == 2:
    â†’ SHORT ì§„ì…/ìŠ¤ìœ„ì¹­/ìœ ì§€

# 4. Reward ê³„ì‚°
step_pnl = unrealized_pnl - prev_unrealized_pnl
reward = env.calculate_reward(step_pnl, realized_pnl, trade_done, holding_time)

# 5. Transition ì €ì¥
agent.put_data((state, action, reward, next_state, log_prob, done))
```

### 9.2 ë°°ì¹˜ ì—…ë°ì´íŠ¸

```python
# ë©”ëª¨ë¦¬ì— íŠ¸ëœì§€ì…˜ì´ ìŒ“ì´ë©´
if len(agent.memory) >= batch_size or episode_end:
    agent.train_net(episode=episode_num)
    # GAE ê³„ì‚° â†’ PPO Loss â†’ 10 epochs ì—…ë°ì´íŠ¸
```

---

## 10. ì£¼ìš” íŠ¹ì§• ìš”ì•½

### 10.1 Dense Rewardì˜ íš¨ê³¼

- **ì´ì „ (Sparse Reward)**: ê±°ë˜ ì¢…ë£Œ ì‹œì—ë§Œ ë³´ìƒ â†’ í•™ìŠµ ì‹ í˜¸ ë¶€ì¡±
- **í˜„ì¬ (Dense Reward)**: ë§¤ ìŠ¤í… í‰ê°€ê¸ˆì•¡ ë³€í™”ì— ë³´ìƒ â†’ ë¹ ë¥¸ í•™ìŠµ

### 10.2 AI íŒë‹¨ ì²­ì‚°ì˜ íš¨ê³¼

- **ì´ì „ (Passive Hold)**: Action 0 = ìœ ì§€ â†’ ê³¼ì‰ ê±°ë˜ ë°©ì§€
- **í˜„ì¬ (AI Exit)**: Action 0 = ì²­ì‚° â†’ AIê°€ ìŠ¤ìŠ¤ë¡œ ì²­ì‚° ì‹œì  ê²°ì •

### 10.3 State Retentionì˜ íš¨ê³¼

- **ì´ì „**: ë§¤ ìŠ¤í…ë§ˆë‹¤ LSTM ìƒíƒœ ì´ˆê¸°í™” â†’ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ ë¶ˆê°€
- **í˜„ì¬**: ì—í”¼ì†Œë“œ ë‚´ LSTM ìƒíƒœ ìœ ì§€ â†’ ì¥ê¸° íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥

---

## 11. íŒŒì¼ êµ¬ì¡°

```
model/
â”œâ”€â”€ train_ppo.py          # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ evaluate_ppo.py       # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ppo_agent.py          # PPO ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
â”œâ”€â”€ trading_env.py        # íŠ¸ë ˆì´ë”© í™˜ê²½ (ë³´ìƒ í•¨ìˆ˜ í¬í•¨)
â”œâ”€â”€ xlstm_network.py      # xLSTM ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜
â”œâ”€â”€ preprocess.py         # ë°ì´í„° ì „ì²˜ë¦¬ (Z-Score)
â”œâ”€â”€ feature_engineering.py  # í”¼ì²˜ ìƒì„±
â””â”€â”€ mtf_processor.py     # ë©€í‹° íƒ€ì„í”„ë ˆì„ ì²˜ë¦¬

data/
â”œâ”€â”€ training_features.csv      # 29ê°œ í”¼ì²˜ + ì „ëµ ì ìˆ˜
â”œâ”€â”€ cached_strategies.csv      # ì „ëµ ì ìˆ˜ ìºì‹œ
â”œâ”€â”€ ppo_model_best.pth         # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”œâ”€â”€ ppo_model_best_scaler.pkl  # Best ëª¨ë¸ìš© ìŠ¤ì¼€ì¼ëŸ¬
â”œâ”€â”€ ppo_model_last.pth         # ìµœì‹  ëª¨ë¸
â””â”€â”€ ppo_model_last_scaler.pkl  # Last ëª¨ë¸ìš© ìŠ¤ì¼€ì¼ëŸ¬

config.py                 # ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ì•™ ê´€ë¦¬
```

---

## 12. ì„±ëŠ¥ ìµœì í™”

### 12.1 ìºì‹± ì‹œìŠ¤í…œ

1. **í”¼ì²˜ ìºì‹±**: `training_features.csv`
   - 29ê°œ í”¼ì²˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ì €ì¥
   - í•™ìŠµ ì‹œì‘ ì‹œ ì¦‰ì‹œ ë¡œë“œ

2. **ì „ëµ ìºì‹±**: `cached_strategies.csv`
   - 12ê°œ ì „ëµ ì ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ì €ì¥
   - ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê³„ì‚° ì†ë„ í–¥ìƒ

### 12.2 ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€

- ìŠ¤ì¼€ì¼ëŸ¬ëŠ” Train Set 80%ë§Œ ì‚¬ìš©
- ì—í”¼ì†Œë“œëŠ” Train Set ë‚´ì—ì„œë§Œ ì‹¤í–‰
- Test Setì€ í‰ê°€ ì‹œì—ë§Œ ì‚¬ìš©

---

## 13. ì°¸ê³ ì‚¬í•­

### 13.1 ì•¡ì…˜ ì˜ë¯¸ ë³€ê²½ ì´ë ¥

1. **ì´ˆê¸°**: Action 0 = Hold (ìœ ì§€)
2. **ì¤‘ê°„**: Action 0 = Exit (ì¦‰ì‹œ ì²­ì‚°) â†’ ê³¼ì‰ ê±°ë˜ ë°œìƒ
3. **í˜„ì¬**: Action 0 = AI íŒë‹¨ ì²­ì‚° (ìµœì†Œ ë³´ìœ  ì‹œê°„ í›„ ê°€ëŠ¥)

### 13.2 ë³´ìƒ ì²´ê³„ ë³€ê²½ ì´ë ¥

1. **ì´ˆê¸°**: Realized PnLë§Œ ë³´ìƒ (Sparse)
2. **ì¤‘ê°„**: Unrealized PnL ë³€í™”ë„ ë³´ìƒ (Dense)
3. **í˜„ì¬**: Step PnL + Terminal PnL (Dense Reward)

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-23  
**ì‘ì„±ì**: PPO Model Documentation
