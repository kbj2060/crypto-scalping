"""
ê¸´ê¸‰ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸ (debug_training.py)
AIì˜ ë‡Œ ì†ì„ ì§ì ‘ ë“¤ì—¬ë‹¤ë³´ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
"""
import torch
import numpy as np
import pandas as pd
from model.train_ppo import PPOTrainer
from model import config

def debug_agent():
    print("ğŸš¨ [ê¸´ê¸‰ ì§„ë‹¨] PPO ì—ì´ì „íŠ¸ ìƒíƒœ ì ê²€ ì‹œì‘...\n")
    
    # 1. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” (ë°ì´í„° ë¡œë“œ í¬í•¨)
    trainer = PPOTrainer(enable_visualization=False)
    env = trainer.env
    agent = trainer.agent
    
    # 2. ë°ì´í„° ìƒíƒœ ì ê²€ (ëˆˆ ê²€ì‚¬)
    print("-" * 50)
    print("1ï¸âƒ£ ì…ë ¥ ë°ì´í„°(State) ì ê²€")
    
    # ì„ì˜ì˜ ì‹œì ì—ì„œ ê´€ì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°
    idx = 1000
    env.collector.current_index = idx
    # ê°€ìƒì˜ í¬ì§€ì…˜ ì •ë³´
    pos_info = [0.0, 0.0, 0.0] 
    
    state = env.get_observation(position_info=pos_info, current_index=idx)
    
    if state is None:
        print("âŒ ì˜¤ë¥˜: Stateê°€ Noneì…ë‹ˆë‹¤. ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨.")
        return

    obs_seq, obs_info = state
    
    print(f"   - Sequence Shape: {obs_seq.shape}")
    print(f"   - Info Shape: {obs_info.shape}")
    print(f"   - Seq Mean: {obs_seq.mean().item():.4f} | Max: {obs_seq.max().item():.4f} | Min: {obs_seq.min().item():.4f}")
    
    if torch.isnan(obs_seq).any():
        print("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì…ë ¥ ë°ì´í„°ì— NaN(ê²°ì¸¡ì¹˜)ì´ ìˆìŠµë‹ˆë‹¤!")
    elif obs_seq.abs().sum() == 0:
        print("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì…ë ¥ ë°ì´í„°ê°€ ì „ë¶€ 0ì…ë‹ˆë‹¤! (Scaler ê³ ì¥)")
    else:
        print("âœ… ì…ë ¥ ë°ì´í„° ì •ìƒ (ê°’ ë¶„í¬ í™•ì¸ë¨)")

    # 3. ì‹ ê²½ë§ ì¶œë ¥ ì ê²€ (ë‡Œ ê²€ì‚¬)
    print("\n" + "-" * 50)
    print("2ï¸âƒ£ ì‹ ê²½ë§ ì¶œë ¥(Action Probability) ì ê²€")
    
    agent.model.eval()
    with torch.no_grad():
        # ë°°ì¹˜ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ì„œ ë„£ì–´ë´„
        obs_seq = obs_seq.to(agent.device)
        obs_info = obs_info.to(agent.device)
        
        # LSTM ìƒíƒœ ì´ˆê¸°í™”
        agent.reset_episode_states()
        
        # return_states=Trueë¡œ í˜¸ì¶œ (3ê°œ ë°˜í™˜ê°’)
        probs, value, states = agent.model(obs_seq, obs_info, states=None, return_states=True)
        
    probs_np = probs.cpu().numpy()[0]
    value_np = value.cpu().numpy()[0]
    entropy = -np.sum(probs_np * np.log(probs_np + 1e-8))
    
    print(f"   - Action Probabilities: {probs_np}")
    # value_npê°€ ìŠ¤ì¹¼ë¼ ë°°ì—´ì¸ ê²½ìš° ì²˜ë¦¬
    if isinstance(value_np, np.ndarray):
        value_scalar = value_np.item() if value_np.size == 1 else float(value_np[0])
    else:
        value_scalar = float(value_np)
    print(f"   - State Value (V): {value_scalar:.4f}")
    print(f"   - Entropy: {entropy:.4f}")
    
    if np.any(np.isnan(probs_np)):
        print("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹ ê²½ë§ ì¶œë ¥ì´ NaNì…ë‹ˆë‹¤. (ê°€ì¤‘ì¹˜ í­ë°œ)")
    elif np.max(probs_np) > 0.99:
        print("âš ï¸ ê²½ê³ : ì´ˆê¸° ìƒíƒœì¸ë° í™•ì‹ ì´ ë„ˆë¬´ ê°•í•©ë‹ˆë‹¤. (Policy Collapse ì˜ì‹¬)")
        print("   -> Entropy Coefficientë¥¼ ë†’ì´ê±°ë‚˜ ì´ˆê¸°í™”ë¥¼ ë‹¤ì‹œ í•´ì•¼ í•©ë‹ˆë‹¤.")
    elif np.allclose(probs_np, 1.0/len(probs_np), atol=0.01):
        print("âœ… ì‹ ê²½ë§ ì¶œë ¥ ì •ìƒ (ì´ˆê¸° íƒìƒ‰ ê°€ëŠ¥ ìƒíƒœ)")
    else:
        print("âœ… ì‹ ê²½ë§ ì¶œë ¥ ì–‘í˜¸ (ì ì ˆí•œ í™•ë¥  ë¶„í¬)")

    # 4. ë¦¬ì›Œë“œ ìŠ¤ì¼€ì¼ ì ê²€
    print("\n" + "-" * 50)
    print("3ï¸âƒ£ ë¦¬ì›Œë“œ í•¨ìˆ˜ ì ê²€ (ê°€ìƒ ì‹œë®¬ë ˆì´ì…˜)")
    
    # 3-Action êµ¬ì¡°: 0=Neutral, 1=Long, 2=Short
    # 1% ìˆ˜ìµ ìƒí™© ê°€ì • (Long í¬ì§€ì…˜ ì²­ì‚°)
    r_profit = env.calculate_reward(
        step_pnl=0.01, 
        realized_pnl=0.01, 
        trade_done=True, 
        action=0,  # Neutral (ì²­ì‚°)
        prev_position='LONG', 
        current_position=None
    )
    # -1% ì†ì‹¤ ìƒí™© ê°€ì • (Long í¬ì§€ì…˜ ì²­ì‚°)
    r_loss = env.calculate_reward(
        step_pnl=-0.01, 
        realized_pnl=-0.01, 
        trade_done=True, 
        action=0,  # Neutral (ì²­ì‚°)
        prev_position='LONG', 
        current_position=None
    )
    
    print(f"   - 1% ìµì ˆ ì‹œ ë¦¬ì›Œë“œ: {r_profit:.4f}")
    print(f"   - 1% ì†ì ˆ ì‹œ ë¦¬ì›Œë“œ: {r_loss:.4f}")
    print(f"   - ë¦¬ì›Œë“œ ë¹„ìœ¨ (ìµì ˆ/ì†ì ˆ): {abs(r_profit/r_loss) if r_loss != 0 else 'N/A':.2f}")
    
    if abs(r_profit) < 0.1:
        print("âš ï¸ ê²½ê³ : ë¦¬ì›Œë“œê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. í•™ìŠµì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif abs(r_profit) > 100:
        print("âš ï¸ ê²½ê³ : ë¦¬ì›Œë“œê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. í•™ìŠµì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âœ… ë¦¬ì›Œë“œ ìŠ¤ì¼€ì¼ ì–‘í˜¸")
    
    # 5. ì¶”ê°€ ì§„ë‹¨: í™€ë”© ë³´ìƒ í™•ì¸
    print("\n" + "-" * 50)
    print("4ï¸âƒ£ í™€ë”© ë³´ìƒ ì ê²€")
    
    # í¬ì§€ì…˜ ìœ ì§€ ì¤‘ ì‘ì€ ìˆ˜ìµ
    r_hold_profit = env.calculate_reward(
        step_pnl=0.001,  # 0.1% ìˆ˜ìµ
        realized_pnl=0.0,
        trade_done=False,
        action=1,  # Long ìœ ì§€
        prev_position='LONG',
        current_position='LONG'
    )
    
    # í¬ì§€ì…˜ ìœ ì§€ ì¤‘ ì‘ì€ ì†ì‹¤
    r_hold_loss = env.calculate_reward(
        step_pnl=-0.001,  # 0.1% ì†ì‹¤
        realized_pnl=0.0,
        trade_done=False,
        action=1,  # Long ìœ ì§€
        prev_position='LONG',
        current_position='LONG'
    )
    
    print(f"   - í™€ë”© ì¤‘ 0.1% ìˆ˜ìµ ì‹œ ë¦¬ì›Œë“œ: {r_hold_profit:.4f}")
    print(f"   - í™€ë”© ì¤‘ 0.1% ì†ì‹¤ ì‹œ ë¦¬ì›Œë“œ: {r_hold_loss:.4f}")
    
    if r_hold_profit > 0 and r_hold_loss < 0:
        print("âœ… í™€ë”© ë³´ìƒ ì •ìƒ (ìˆ˜ìµ ì‹œ ì–‘ìˆ˜, ì†ì‹¤ ì‹œ ìŒìˆ˜)")
    else:
        print("âš ï¸ ê²½ê³ : í™€ë”© ë³´ìƒ ë¡œì§ í™•ì¸ í•„ìš”")

    # 6. ë°ì´í„° ë¶„í•  í™•ì¸
    print("\n" + "-" * 50)
    print("5ï¸âƒ£ ë°ì´í„° ë¶„í•  í™•ì¸")
    
    total_len = len(env.collector.eth_data)
    train_end = int(total_len * config.TRAIN_SPLIT)
    val_end = int(total_len * (config.TRAIN_SPLIT + config.VAL_SPLIT))
    test_start = val_end
    
    print(f"   - ì „ì²´ ë°ì´í„°: {total_len}ê°œ")
    print(f"   - Train Set: 0 ~ {train_end} ({train_end/total_len*100:.1f}%)")
    print(f"   - Val Set: {train_end} ~ {val_end} ({(val_end-train_end)/total_len*100:.1f}%)")
    print(f"   - Test Set: {val_end} ~ {total_len} ({(total_len-val_end)/total_len*100:.1f}%)")
    
    # ì „ëµ ì‹ í˜¸ê°€ Train Set ì´í›„ì— ê³„ì‚°ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if 'strategy_0' in env.collector.eth_data.columns:
        test_strategy_sum = env.collector.eth_data['strategy_0'].iloc[test_start:].abs().sum()
        if test_strategy_sum == 0:
            print("âœ… ë°ì´í„° ëˆ„ìˆ˜ ì°¨ë‹¨ í™•ì¸: Test Setì˜ ì „ëµ ì‹ í˜¸ëŠ” 0ì…ë‹ˆë‹¤.")
        else:
            print(f"âš ï¸ ê²½ê³ : Test Setì— ì „ëµ ì‹ í˜¸ê°€ ìˆìŠµë‹ˆë‹¤! (í•©ê³„: {test_strategy_sum:.2f})")
            print("   -> ì „ëµ ê³„ì‚°ì´ Train Setë§Œ ìˆ˜í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸ í•„ìš”")
    
    print("\n" + "=" * 50)
    print("âœ… ì§„ë‹¨ ì™„ë£Œ!")
    print("=" * 50)

if __name__ == "__main__":
    debug_agent()
