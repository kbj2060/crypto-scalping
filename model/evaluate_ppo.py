"""
PPO Î™®Îç∏ ÌèâÍ∞Ä Ïä§ÌÅ¨Î¶ΩÌä∏ (3-Action, Data Leakage Ï∞®Îã®)
ÌïôÏäµÎêú Î™®Îç∏ÏùÑ Validation/Test Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÌèâÍ∞ÄÌïòÍ≥† ÏÑ±Îä• ÏßÄÌëúÎ•º Ï∂úÎ†•Ìï©ÎãàÎã§.
"""
import logging
import os
import sys
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from core import DataCollector
from strategies import (
    BTCEthCorrelationStrategy, VolatilitySqueezeStrategy, OrderblockFVGStrategy,
    HMAMomentumStrategy, MFIMomentumStrategy, BollingerMeanReversionStrategy,
    VWAPDeviationStrategy, RangeTopBottomStrategy, StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy, CCIReversalStrategy, WilliamsRStrategy
)
from model.trading_env import TradingEnvironment
from model.ppo_agent import PPOAgent

# Î°úÍπÖ ÏÑ§Ï†ï
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(message)s',
    handlers=[
        logging.FileHandler('logs/evaluate_ppo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Î∂àÌïÑÏöîÌïú Î°úÍ∑∏ ÎÅÑÍ∏∞
logging.getLogger('model.feature_engineering').setLevel(logging.WARNING)
logging.getLogger('model.mtf_processor').setLevel(logging.WARNING)


class PPOEvaluator:
    def __init__(self, mode='test'):
        """
        mode: 'val' (Í≤ÄÏ¶ùÏÖã 70~85%) or 'test' (ÌÖåÏä§Ìä∏ÏÖã 85~100%)
        """
        self.data_collector = DataCollector(use_saved_data=True)
        self.strategies = [
            BTCEthCorrelationStrategy(), VolatilitySqueezeStrategy(), OrderblockFVGStrategy(),
            HMAMomentumStrategy(), MFIMomentumStrategy(), BollingerMeanReversionStrategy(),
            VWAPDeviationStrategy(), RangeTopBottomStrategy(), StochRSIMeanReversionStrategy(),
            CMFDivergenceStrategy(), CCIReversalStrategy(), WilliamsRStrategy()
        ]
        
        # 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        self._load_features()
        
        # 2. Îç∞Ïù¥ÌÑ∞ Íµ¨Í∞Ñ ÏÑ§Ï†ï (Critical Fix - Data Leakage Ï∞®Îã®)
        total_len = len(self.data_collector.eth_data)
        train_end = int(total_len * config.TRAIN_SPLIT)
        val_end = int(total_len * (config.TRAIN_SPLIT + config.VAL_SPLIT))
        
        if mode == 'val':
            self.start_idx = train_end
            self.end_idx = val_end
            logger.info(f"üîç Evaluation Mode: VALIDATION Set ({self.start_idx} ~ {self.end_idx}, {self.end_idx - self.start_idx} steps)")
        else:  # test
            self.start_idx = val_end
            self.end_idx = total_len
            logger.info(f"üîç Evaluation Mode: TEST Set ({self.start_idx} ~ {self.end_idx}, {self.end_idx - self.start_idx} steps)")

        # 3. ÌôòÍ≤Ω Î∞è ÏóêÏù¥Ï†ÑÌä∏ ÏÑ§Ï†ï
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        
        # [Critical] ÌïôÏäµÎêú Scaler Î°úÎìú (ÏÉàÎ°ú fitÌïòÏßÄ ÏïäÏùå!)
        scaler_path = config.AI_MODEL_PATH.replace('.pth', '_best_scaler.pkl')
        if os.path.exists(scaler_path):
            self.env.preprocessor.load(scaler_path)
            self.env.scaler_fitted = True
            logger.info(f"‚úÖ Trained Scaler loaded: {scaler_path}")
        else:
            # Fallback: last_scaler ÏãúÎèÑ
            scaler_path = config.AI_MODEL_PATH.replace('.pth', '_scaler.pkl')
            if os.path.exists(scaler_path):
                self.env.preprocessor.load(scaler_path)
                self.env.scaler_fitted = True
                logger.info(f"‚úÖ Trained Scaler loaded (fallback): {scaler_path}")
            else:
                logger.error("‚ùå Scaler file not found. Train first!")
                sys.exit(1)

        # 4. Î™®Îç∏ Î°úÎìú (3-Action)
        state_dim = self.env.get_state_dim()
        action_dim = 3  # 3-Action: 0:Neutral, 1:Long, 2:Short
        info_dim = len(self.strategies) + 3
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"üîß Device: {device} | Action Dim: {action_dim} (3-Action)")
        
        self.agent = PPOAgent(state_dim, action_dim, info_dim=info_dim, device=device)
        
        model_path = config.AI_MODEL_PATH.replace('.pth', '_best.pth')  # Best Î™®Îç∏ ÌèâÍ∞Ä
        if os.path.exists(model_path):
            try:
                self.agent.load_model(model_path)
                self.agent.model.eval()  # ÌèâÍ∞Ä Î™®Îìú
                logger.info(f"‚úÖ Best Model loaded: {model_path}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Best model load failed (structure mismatch?): {e}")
                # Fallback: last model ÏãúÎèÑ
                model_path = config.AI_MODEL_PATH.replace('.pth', '_last.pth')
                if os.path.exists(model_path):
                    try:
                        self.agent.load_model(model_path)
                        self.agent.model.eval()
                        logger.info(f"‚úÖ Last Model loaded (fallback): {model_path}")
                    except Exception as e2:
                        logger.error(f"‚ùå Model load failed: {e2}")
                        sys.exit(1)
                else:
                    logger.error("‚ùå Model file not found.")
                    sys.exit(1)
        else:
            logger.error("‚ùå Model file not found.")
            sys.exit(1)

    def _load_features(self):
        """ÌîºÏ≤ò ÌååÏùº Î°úÎìú"""
        path = 'data/training_features.csv'
        cached_strategies_path = 'data/cached_strategies.csv'
        
        if os.path.exists(path):
            df = pd.read_csv(path, index_col=0, parse_dates=True)
            if os.path.exists(cached_strategies_path):
                try:
                    cached_df = pd.read_csv(cached_strategies_path, index_col=0, parse_dates=True)
                    strategy_cols = [col for col in cached_df.columns if col.startswith('strategy_')]
                    for col in strategy_cols:
                        if col in cached_df.columns:
                            df[col] = cached_df[col]
                except Exception:
                    pass
            self.data_collector.eth_data = df
        else:
            logger.error("‚ùå ÌîºÏ≤ò ÌååÏùº ÏóÜÏùå")
            sys.exit(1)

    def evaluate(self):
        """ÌèâÍ∞Ä Î£®ÌîÑ (Ïã§Ï†Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò)"""
        logger.info("üöÄ Starting Evaluation...")
        
        current_position = None  # None, 'LONG', 'SHORT'
        entry_price = 0.0
        entry_index = 0
        total_reward = 0.0
        trades = []
        balance_history = [config.EVAL_INITIAL_CAPITAL]  # Ï¥àÍ∏∞ ÏûêÎ≥∏
        prev_unrealized_pnl = 0.0  # Ïù¥Ï†Ñ Ïä§ÌÖùÏùò ÌèâÍ∞ÄÏÜêÏùµ Ï∂îÏ†Å
        prev_entry_index = 0  # Ïä§ÏúÑÏπ≠ Ïãú Ïù¥Ï†Ñ Ìè¨ÏßÄÏÖòÏùò entry_index Ï†ÄÏû•Ïö©
        
        # LSTM ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
        self.agent.reset_episode_states()
        
        # [Ï§ëÏöî] Ï†ÑÎûµ Ïã†Ìò∏Îäî Ïù¥ÎØ∏ training_features.csvÏóê Í≥ÑÏÇ∞ÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï
        # ÌïòÏßÄÎßå ÏóÑÍ≤©Ìïú ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ÏÑ† Ïó¨Í∏∞ÏÑú Ïã§ÏãúÍ∞ÑÏúºÎ°ú Í≥ÑÏÇ∞ÌïòÎäî Í≤å ÎßûÏùå.
        # (ÏÑ±Îä•ÏÉÅ Ïó¨Í∏∞ÏÑúÎäî Ï†ÄÏû•Îêú Í∞í ÏÇ¨Ïö©ÌïòÎêò, Ï†ÄÏû• Ïãú look-ahead ÏóÜÏóàÎäîÏßÄ ÌôïÏù∏ ÌïÑÏàò)
        
        pbar = tqdm(range(self.start_idx, self.end_idx - 1), desc="Evaluating")
        
        for idx in pbar:
            self.data_collector.current_index = idx
            curr_price = float(self.data_collector.eth_data.iloc[idx]['close'])
            
            # PnL Í≥ÑÏÇ∞
            unrealized_pnl = 0.0
            if current_position == 'LONG':
                unrealized_pnl = (curr_price - entry_price) / entry_price
            elif current_position == 'SHORT':
                unrealized_pnl = (entry_price - curr_price) / entry_price
            
            # Í¥ÄÏ∏°
            pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            holding_time = (idx - entry_index) if current_position else 0
            pos_info = [pos_val, unrealized_pnl * 10, holding_time / 1000]  # Ï†ïÍ∑úÌôî ÎåÄÎûµ
            
            state = self.env.get_observation(position_info=pos_info, current_index=idx)
            if state is None:
                continue
            
            # Ïù¥Ï†Ñ Ìè¨ÏßÄÏÖò Ï†ÄÏû• (Î¶¨ÏõåÎìú Í≥ÑÏÇ∞Ïö©)
            prev_pos_str = current_position
            
            # ÌñâÎèô ÏÑ†ÌÉù (Deterministic=True for Eval)
            # ÌèâÍ∞Ä ÎïåÎäî ÌôïÎ•†Ï†Å ÏÉòÌîåÎßÅ ÎåÄÏã† ÌôïÎ•† Í∞ÄÏû• ÎÜíÏùÄ ÌñâÎèô ÏÑ†ÌÉù Í∂åÏû•
            with torch.no_grad():
                obs_seq, obs_info = state
                obs_seq = obs_seq.to(self.agent.device)
                obs_info = obs_info.to(self.agent.device)
                
                probs, _ = self.agent.model(obs_seq, obs_info, states=None, return_states=False)
                action = torch.argmax(probs, dim=-1).item()  # Deterministic
            
            # 3-Action Logic (Ïä§ÏúÑÏπ≠ ÏßÄÏõê)
            trade_done = False
            realized_pnl = 0.0
            
            # Action 0: Neutral (HOLD/Ï≤≠ÏÇ∞)
            if action == 0:
                if current_position is not None:
                    # Ï≤≠ÏÇ∞
                    realized_pnl = unrealized_pnl
                    trade_done = True
                    prev_entry_index = entry_index  # Í±∞Îûò Í∏∞Î°ùÏö©
                    current_position = None
                    entry_price = 0.0
                    entry_index = 0
                # Ìè¨ÏßÄÏÖò ÏóÜÏúºÎ©¥ HOLD (Pass)
            
            # Action 1: Long (ÏßÑÏûÖ/Ïú†ÏßÄ/Ïä§ÏúÑÏπ≠)
            elif action == 1:
                if current_position is None:
                    # ÏßÑÏûÖ
                    current_position = 'LONG'
                    entry_price = curr_price
                    entry_index = idx
                elif current_position == 'SHORT':
                    # Ïä§ÏúÑÏπ≠: SHORT Ï≤≠ÏÇ∞ ÌõÑ LONG ÏßÑÏûÖ
                    realized_pnl = unrealized_pnl
                    trade_done = True
                    prev_entry_index = entry_index  # Í±∞Îûò Í∏∞Î°ùÏö©
                    current_position = 'LONG'
                    entry_price = curr_price
                    entry_index = idx
                # Ïù¥ÎØ∏ LONGÏù¥Î©¥ Ïú†ÏßÄ (Pass)
            
            # Action 2: Short (ÏßÑÏûÖ/Ïú†ÏßÄ/Ïä§ÏúÑÏπ≠)
            elif action == 2:
                if current_position is None:
                    # ÏßÑÏûÖ
                    current_position = 'SHORT'
                    entry_price = curr_price
                    entry_index = idx
                elif current_position == 'LONG':
                    # Ïä§ÏúÑÏπ≠: LONG Ï≤≠ÏÇ∞ ÌõÑ SHORT ÏßÑÏûÖ
                    realized_pnl = unrealized_pnl
                    trade_done = True
                    prev_entry_index = entry_index  # Í±∞Îûò Í∏∞Î°ùÏö©
                    current_position = 'SHORT'
                    entry_price = curr_price
                    entry_index = idx
                # Ïù¥ÎØ∏ SHORTÎ©¥ Ïú†ÏßÄ (Pass)
            
            # Î¶¨ÏõåÎìú Í≥ÑÏÇ∞ (3-Action ÎåÄÏùë)
            # step_pnl: Ïù¥Ï†Ñ Ïä§ÌÖù ÎåÄÎπÑ ÌèâÍ∞ÄÏÜêÏùµ Î≥ÄÌôî
            step_pnl = unrealized_pnl - prev_unrealized_pnl if current_position else 0.0
            
            reward = self.env.calculate_reward(
                step_pnl=step_pnl,
                realized_pnl=realized_pnl,
                trade_done=trade_done,
                action=action,
                prev_position=prev_pos_str,
                current_position=current_position
            )
            total_reward += reward
            
            # Í±∞Îûò Í∏∞Î°ù Î∞è ÏûêÎ≥∏Í∏à ÏóÖÎç∞Ïù¥Ìä∏
            if trade_done:
                fee = getattr(config, 'TRANSACTION_COST', 0.001)
                # Ïä§ÏúÑÏπ≠Ïùò Í≤ΩÏö∞ ÏàòÏàòÎ£åÍ∞Ä Îëê Î≤à Î∞úÏÉùÌï† ÏàòÎèÑ ÏûàÏßÄÎßå(Ï≤≠ÏÇ∞+ÏßÑÏûÖ),
                # Ïó¨Í∏∞ÏÑúÎäî 1ÌöåÎ∂ÑÎßå Î∞òÏòÅÌïòÍ±∞ÎÇò, ÏóÑÍ≤©ÌïòÍ≤å 2Î∞∞ Ìï† Ïàò ÏûàÏùå.
                # ÏùºÎã® 1.5Î∞∞ Ï†ïÎèÑÎ°ú ÌèâÍ∑† ÎÇ¥ÏÑú Ï†ÅÏö©
                actual_fee = fee * 1.5 if (prev_pos_str is not None and current_position is not None) else fee
                
                net_pnl = realized_pnl - actual_fee
                new_balance = balance_history[-1] * (1 + net_pnl)
                balance_history.append(new_balance)
                
                trades.append({
                    'entry_idx': prev_entry_index,
                    'exit_idx': idx,
                    'type': prev_pos_str,
                    'pnl': realized_pnl,
                    'net_pnl': net_pnl
                })
                # Í±∞Îûò ÏôÑÎ£å ÌõÑ prev_unrealized_pnl Ï¥àÍ∏∞Ìôî
                prev_unrealized_pnl = 0.0
            else:
                # Ìè¨ÏßÄÏÖò Ïú†ÏßÄ Ï§ë: Îã§Ïùå Ïä§ÌÖùÏùÑ ÏúÑÌï¥ ÌòÑÏû¨ ÌèâÍ∞ÄÏÜêÏùµ Ï†ÄÏû•
                prev_unrealized_pnl = unrealized_pnl
            
            pbar.set_postfix({'Bal': f"${balance_history[-1]:.0f}", 'Tr': len(trades)})
        
        # ÎßàÏßÄÎßâ Ìè¨ÏßÄÏÖò Ï≤≠ÏÇ∞
        if current_position is not None:
            final_price = float(self.data_collector.eth_data.iloc[self.end_idx - 1]['close'])
            if current_position == 'LONG':
                realized_pnl = (final_price - entry_price) / entry_price
            else:
                realized_pnl = (entry_price - final_price) / entry_price
            
            fee = getattr(config, 'TRANSACTION_COST', 0.001)
            net_pnl = realized_pnl - fee
            new_balance = balance_history[-1] * (1 + net_pnl)
            balance_history.append(new_balance)
            
            trades.append({
                'entry_idx': entry_index,
                'exit_idx': self.end_idx - 1,
                'type': current_position,
                'pnl': realized_pnl,
                'net_pnl': net_pnl
            })
        
        # Í≤∞Í≥º Î¶¨Ìè¨Ìä∏
        self._print_report(trades, balance_history, total_reward)

    def _print_report(self, trades, balance_history, total_reward):
        """ÌèâÍ∞Ä Í≤∞Í≥º Î¶¨Ìè¨Ìä∏ Ï∂úÎ†•"""
        if len(trades) == 0:
            logger.warning("‚ö†Ô∏è No trades executed")
            return
        
        df_trades = pd.DataFrame(trades)
        final_balance = balance_history[-1]
        initial_balance = balance_history[0]
        roi = (final_balance - initial_balance) / initial_balance * 100
        
        # ÏäπÎ•† Í≥ÑÏÇ∞
        win_trades = df_trades[df_trades['net_pnl'] > 0]
        win_rate = len(win_trades) / len(trades) * 100
        avg_pnl = df_trades['net_pnl'].mean() * 100
        
        # ÏµúÎåÄ ÎÇôÌè≠ Í≥ÑÏÇ∞
        equity_array = np.array(balance_history)
        peak = np.maximum.accumulate(equity_array)
        drawdown = (equity_array - peak) / peak * 100
        max_drawdown = np.min(drawdown)
        
        print("\n" + "="*50)
        print(f"üìä Evaluation Report")
        print("="*50)
        print(f"Initial Balance: ${initial_balance:,.2f}")
        print(f"Final Balance:   ${final_balance:,.2f}")
        print(f"Net ROI:         {roi:.2f}%")
        print(f"Total Reward:    {total_reward:.2f}")
        print(f"Total Trades:    {len(trades)}")
        print(f"Win Rate:        {win_rate:.2f}%")
        print(f"Avg PnL:         {avg_pnl:.4f}%")
        print(f"Max Drawdown:    {max_drawdown:.2f}%")
        print("="*50)
        
        # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞
        try:
            plt.figure(figsize=(12, 6))
            plt.plot(balance_history, label='Balance', linewidth=2)
            plt.title('Evaluation Balance History')
            plt.xlabel('Trades')
            plt.ylabel('Balance ($)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.show()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Plotting failed: {e}")


if __name__ == "__main__":
    # Test ModeÎ°ú Ïã§Ìñâ
    evaluator = PPOEvaluator(mode='test')
    evaluator.evaluate()
