"""
PPO ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ë³„ë„ë¡œ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
"""
import logging
import os
import sys
import time
from datetime import datetime

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€ (config, core, strategies ëª¨ë“ˆ ì ‘ê·¼ìš©)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core import DataCollector, BinanceClient
from core.indicators import Indicators
from strategies import (
    BTCEthCorrelationStrategy,
    CVDDeltaStrategy,
    VolatilitySqueezeStrategy,
    OrderblockFVGStrategy,
    LiquidationSpikeStrategy,
    BollingerMeanReversionStrategy,
    VWAPDeviationStrategy,
    RangeTopBottomStrategy,
    StochRSIMeanReversionStrategy,
    CVDFakePressureStrategy
)

# AI ê°•í™”í•™ìŠµ ëª¨ë“ˆ
try:
    import torch
    from model.trading_env import TradingEnvironment
    from model.ppo_agent import PPOAgent
    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"âŒ AI ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("torchê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: pip install torch")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_ppo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class PPOTrainer:
    """PPO ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    def __init__(self):
        # ì €ì¥ëœ ë°ì´í„° ì‚¬ìš© (í•™ìŠµìš©)
        self.data_collector = DataCollector(use_saved_data=True)
        self.client = BinanceClient()
        
        # ì „ëµ ì´ˆê¸°í™”
        self.breakout_strategies = []
        self.range_strategies = []
        
        # í­ë°œì¥ ì „ëµ
        if config.STRATEGIES.get('btc_eth_correlation', False):
            self.breakout_strategies.append(BTCEthCorrelationStrategy())
        if config.STRATEGIES.get('cvd_delta', False):
            self.breakout_strategies.append(CVDDeltaStrategy())
        if config.STRATEGIES.get('volatility_squeeze', False):
            self.breakout_strategies.append(VolatilitySqueezeStrategy())
        if config.STRATEGIES.get('orderblock_fvg', False):
            self.breakout_strategies.append(OrderblockFVGStrategy())
        if config.STRATEGIES.get('liquidation_spike', False) and self.client.use_futures:
            self.breakout_strategies.append(LiquidationSpikeStrategy())
        
        # íš¡ë³´ì¥ ì „ëµ
        if config.STRATEGIES.get('bollinger_mean_reversion', False):
            self.range_strategies.append(BollingerMeanReversionStrategy())
        if config.STRATEGIES.get('vwap_deviation', False):
            self.range_strategies.append(VWAPDeviationStrategy())
        if config.STRATEGIES.get('range_top_bottom', False):
            self.range_strategies.append(RangeTopBottomStrategy())
        if config.STRATEGIES.get('stoch_rsi_mean_reversion', False):
            self.range_strategies.append(StochRSIMeanReversionStrategy())
        if config.STRATEGIES.get('cvd_fake_pressure', False):
            self.range_strategies.append(CVDFakePressureStrategy())
        
        self.strategies = self.breakout_strategies + self.range_strategies
        
        if len(self.strategies) == 0:
            raise ValueError("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. config.pyì—ì„œ ì „ëµì„ í™œì„±í™”í•˜ì„¸ìš”.")
        
        logger.info(f"ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.strategies)}ê°œ ì „ëµ")
        
        # íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„±
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        state_dim = self.env.get_state_dim()
        action_dim = 3  # 0: Hold, 1: Long, 2: Short
        
        # PPO ì—ì´ì „íŠ¸ ìƒì„±
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        self.agent = PPOAgent(state_dim, action_dim, hidden_dim=128, device=device)
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if os.path.exists(config.AI_MODEL_PATH):
            try:
                self.agent.load_model(config.AI_MODEL_PATH)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {config.AI_MODEL_PATH}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘): {e}")
        else:
            logger.info("ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµ ì‹œì‘")
        
        # í•™ìŠµ ìƒíƒœ
        self.current_position = None
        self.entry_price = None
        self.entry_time = None
        self.episode_rewards = []
        self.total_steps = 0
        
    def train_episode(self, max_steps=100):
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ"""
        episode_reward = 0.0
        steps = 0
        
        # ì €ì¥ëœ ë°ì´í„°ì—ì„œ ì¸ë±ìŠ¤ ë¦¬ì…‹ (ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘)
        self.data_collector.reset_index()
        
        # ì´ˆê¸° ë°ì´í„° í™•ì¸
        if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
            logger.error("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. model/collect_training_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ ìŠ¤í… ìˆ˜ ê³„ì‚°
        available_steps = len(self.data_collector.eth_data) - self.data_collector.current_index
        actual_steps = min(max_steps, available_steps)
        
        if actual_steps <= 0:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None
        
        logger.info(f"ì—í”¼ì†Œë“œ ì‹œì‘: ì´ {len(self.data_collector.eth_data)}ê°œ ìº”ë“¤ ì¤‘ {actual_steps}ê°œ ì‚¬ìš© (ì¸ë±ìŠ¤: {self.data_collector.current_index}ë¶€í„°)")
        
        for step in range(actual_steps):
            try:
                # 1. ì €ì¥ëœ ë°ì´í„°ì—ì„œ ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰ (ì¸ë±ìŠ¤ë§Œ ì¦ê°€)
                # get_candlesê°€ í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ
                # ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ì¦ê°€ì‹œì¼œì•¼ í•¨
                if self.data_collector.current_index >= len(self.data_collector.eth_data):
                    logger.info("ë°ì´í„° ëì— ë„ë‹¬, ì—í”¼ì†Œë“œ ì¢…ë£Œ")
                    break
                
                # ì¸ë±ìŠ¤ ì¦ê°€ (ë‹¤ìŒ ìº”ë“¤ë¡œ ì´ë™)
                self.data_collector.current_index += 1
                
                # 2. ìƒíƒœ ê´€ì¸¡ (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í¬í•¨)
                # get_candlesê°€ í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ lookbackê°œë¥¼ ë°˜í™˜
                state = self.env.get_observation()
                if state is None:
                    logger.warning("ìƒíƒœ ê´€ì¸¡ ì‹¤íŒ¨, ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰")
                    continue
                
                # 2. í–‰ë™ ì„ íƒ
                action, log_prob = self.agent.select_action(state)
                action_names = {0: 'HOLD', 1: 'LONG', 2: 'SHORT'}
                action_name = action_names[action]
                
                # 3. í˜„ì¬ ê°€ê²© í™•ì¸ (í˜„ì¬ ì¸ë±ìŠ¤ì˜ ìº”ë“¤)
                if self.data_collector.current_index > 0:
                    current_candle = self.data_collector.eth_data.iloc[self.data_collector.current_index - 1]
                    current_price = float(current_candle['close'])
                else:
                    continue
                
                # 4. ë³´ìƒ ê³„ì‚° ë° í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
                reward = 0.0
                trade_done = False
                
                if action == 1:  # LONG
                    if self.current_position != 'LONG':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'SHORT' and self.entry_price:
                            pnl = (self.entry_price - current_price) / self.entry_price
                            reward = self.env.calculate_reward(pnl, True)
                            trade_done = True
                            logger.info(f"ğŸ’° ìˆ ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}")
                        
                        # ë¡± ì§„ì…
                        self.current_position = 'LONG'
                        self.entry_price = current_price
                        self.entry_time = datetime.now()
                        logger.debug(f"ğŸ“ˆ ë¡± ì§„ì…: ${current_price:.2f}")
                
                elif action == 2:  # SHORT
                    if self.current_position != 'SHORT':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'LONG' and self.entry_price:
                            pnl = (current_price - self.entry_price) / self.entry_price
                            reward = self.env.calculate_reward(pnl, True)
                            trade_done = True
                            logger.info(f"ğŸ’° ë¡± ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}")
                        
                        # ìˆ ì§„ì…
                        self.current_position = 'SHORT'
                        self.entry_price = current_price
                        self.entry_time = datetime.now()
                        logger.debug(f"ğŸ“‰ ìˆ ì§„ì…: ${current_price:.2f}")
                
                else:  # HOLD
                    # ë³´ìœ  ì¤‘ì¸ í¬ì§€ì…˜ì˜ ìˆ˜ìµë¥  ê³„ì‚°
                    if self.current_position and self.entry_price:
                        if self.current_position == 'LONG':
                            pnl = (current_price - self.entry_price) / self.entry_price
                        else:  # SHORT
                            pnl = (self.entry_price - current_price) / self.entry_price
                        
                        holding_time = (datetime.now() - self.entry_time).total_seconds() / 60 if self.entry_time else 0
                        reward = self.env.calculate_reward(pnl, False, holding_time)
                
                # 5. íŠ¸ëœì§€ì…˜ ì €ì¥
                is_terminal = False
                self.agent.store_transition(state, action, log_prob, reward, is_terminal)
                
                episode_reward += reward
                steps += 1
                self.total_steps += 1
                
                # 6. ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ (10ê°œ íŠ¸ëœì§€ì…˜ë§ˆë‹¤)
                if len(self.agent.memory) >= 10:
                    logger.info(f"ğŸ”„ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì¤‘... (ë©”ëª¨ë¦¬: {len(self.agent.memory)}ê°œ)")
                    self.agent.update()
                    logger.info("âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
                # ì €ì¥ëœ ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰ë¨ (ëŒ€ê¸° ë¶ˆí•„ìš”)
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
                raise
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                time.sleep(5)
                continue
        
        return episode_reward, steps
    
    def train(self, num_episodes=100, max_steps_per_episode=100, save_interval=10):
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info("=" * 60)
        logger.info("ğŸš€ PPO ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        logger.info(f"ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…: {max_steps_per_episode}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ê°„ê²©: {save_interval} ì—í”¼ì†Œë“œ")
        logger.info("=" * 60)
        
        best_reward = float('-inf')
        
        for episode in range(1, num_episodes + 1):
            try:
                logger.info(f"\n{'=' * 60}")
                logger.info(f"ğŸ“š ì—í”¼ì†Œë“œ {episode}/{num_episodes}")
                logger.info(f"{'=' * 60}")
                
                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                result = self.train_episode(max_steps=max_steps_per_episode)
                if result is None:
                    logger.warning("ì—í”¼ì†Œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰")
                    continue
                
                episode_reward, steps = result
                self.episode_rewards.append(episode_reward)
                
                # í†µê³„ ì¶œë ¥
                avg_reward = sum(self.episode_rewards[-10:]) / len(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else episode_reward
                logger.info(f"âœ… ì—í”¼ì†Œë“œ {episode} ì™„ë£Œ")
                logger.info(f"   ì´ ë³´ìƒ: {episode_reward:.4f}")
                logger.info(f"   ìŠ¤í… ìˆ˜: {steps}")
                logger.info(f"   ìµœê·¼ 10ê°œ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if episode_reward > best_reward:
                    best_reward = episode_reward
                    os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
                    self.agent.save_model(config.AI_MODEL_PATH)
                    logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: ë³´ìƒ {best_reward:.4f}")
                
                # ì£¼ê¸°ì  ì €ì¥
                elif episode % save_interval == 0:
                    os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
                    self.agent.save_model(config.AI_MODEL_PATH)
                    logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ (ì—í”¼ì†Œë“œ {episode})")
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨")
                break
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ {episode} ì‹¤íŒ¨: {e}", exc_info=True)
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
        self.agent.save_model(config.AI_MODEL_PATH)
        logger.info("=" * 60)
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ")
        logger.info(f"ì´ ìŠ¤í…: {self.total_steps}")
        logger.info(f"í‰ê·  ë³´ìƒ: {sum(self.episode_rewards) / len(self.episode_rewards) if self.episode_rewards else 0:.4f}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {config.AI_MODEL_PATH}")
        logger.info("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='PPO ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--episodes', type=int, default=100, help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--steps', type=int, default=100, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--save-interval', type=int, default=10, help='ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ)')
    
    args = parser.parse_args()
    
    try:
        trainer = PPOTrainer()
        trainer.train(
            num_episodes=args.episodes,
            max_steps_per_episode=args.steps,
            save_interval=args.save_interval
        )
    except KeyboardInterrupt:
        logger.info("í•™ìŠµ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
