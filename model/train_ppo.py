"""
PPO ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DQN ìŠ¤íƒ€ì¼ ë°ì´í„° ìºì‹± ì ìš©)
- í”¼ì²˜ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš© (ì†ë„ ìµœì í™”)
- í•™ìŠµ ì‹œì‘ ì‹œ ë§¤ë²ˆ ê³„ì‚°í•˜ì§€ ì•Šê³  ë¡œë“œë§Œ ìˆ˜í–‰
"""
import logging
import os
import sys
import time
import numpy as np
import pandas as pd

# ì‹œê°í™” ëª¨ë“ˆ (ì„ íƒì )
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    plt = None

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core import DataCollector
from strategies import (
    BTCEthCorrelationStrategy, VolatilitySqueezeStrategy, OrderblockFVGStrategy,
    HMAMomentumStrategy, MFIMomentumStrategy, BollingerMeanReversionStrategy,
    VWAPDeviationStrategy, RangeTopBottomStrategy, StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy,
    CCIReversalStrategy, WilliamsRStrategy  # [ì¶”ê°€] ì´ 2ê°œê°€ ë¹ ì ¸ìˆì—ˆìŠµë‹ˆë‹¤!
)

# AI ê°•í™”í•™ìŠµ ëª¨ë“ˆ
try:
    import torch
    from model.trading_env import TradingEnvironment
    from model.ppo_agent import PPOAgent
    from model.feature_engineering import FeatureEngineer
    from model.mtf_processor import MTFProcessor
    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"âŒ AI ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_ppo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œê·¸ ë„ê¸° (WARNING ì´ìƒë§Œ ì¶œë ¥)
logging.getLogger('model.feature_engineering').setLevel(logging.WARNING)
logging.getLogger('model.mtf_processor').setLevel(logging.WARNING)


class LiveVisualizer:
    """í•™ìŠµ ë¦¬ì›Œë“œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„í™”í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, window_size=10):
        if not MATPLOTLIB_AVAILABLE:
            self.enabled = False
            return
        
        self.enabled = True
        plt.ion()
        self.fig, self.ax = plt.subplots(figsize=(10, 5))
        self.rewards = []
        self.moving_avg = []
        self.window_size = window_size
        
        self.ax.set_title("Real-time Training Performance")
        self.ax.set_xlabel("Episode")
        self.ax.set_ylabel("Total Reward")
        self.line1, = self.ax.plot([], [], label='Episode Reward', alpha=0.3, color='blue')
        self.line2, = self.ax.plot([], [], label=f'Moving Avg ({window_size})', color='red', linewidth=2)
        self.ax.legend()
        self.ax.grid(True)

    def update(self, reward):
        if not self.enabled: return
        self.rewards.append(reward)
        if len(self.rewards) >= self.window_size:
            avg = np.mean(self.rewards[-self.window_size:])
        else:
            avg = np.mean(self.rewards)
        self.moving_avg.append(avg)
        
        x = np.arange(len(self.rewards))
        self.line1.set_data(x, self.rewards)
        self.line2.set_data(x, self.moving_avg)
        self.ax.relim()
        self.ax.autoscale_view()
        plt.draw()
        plt.pause(0.01)


class PPOTrainer:
    """PPO ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    def __init__(self, enable_visualization=False):
        # 1. ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.data_collector = DataCollector(use_saved_data=True)
        
        # 2. ì „ëµ ì´ˆê¸°í™” (12ê°œ ì „ëµ ì™„ì „ì²´)
        self.breakout_strategies = []
        self.range_strategies = []
        
        # í­ë°œì¥ ì „ëµ
        if config.STRATEGIES.get('btc_eth_correlation', False):
            self.breakout_strategies.append(BTCEthCorrelationStrategy())
        if config.STRATEGIES.get('volatility_squeeze', False):
            self.breakout_strategies.append(VolatilitySqueezeStrategy())
        if config.STRATEGIES.get('orderblock_fvg', False):
            self.breakout_strategies.append(OrderblockFVGStrategy())
        if config.STRATEGIES.get('hma_momentum', False):
            self.breakout_strategies.append(HMAMomentumStrategy())
        if config.STRATEGIES.get('mfi_momentum', False):
            self.breakout_strategies.append(MFIMomentumStrategy())
        
        # [ì¶”ê°€] CCI ë°˜ì „ ì „ëµ (í­ë°œ/ì¶”ì„¸ìš©)
        # configì— í‚¤ê°€ ì—†ë‹¤ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì¶”ê°€í•˜ê±°ë‚˜ config.py í™•ì¸ í•„ìš”
        self.breakout_strategies.append(CCIReversalStrategy())
        
        # íš¡ë³´ì¥ ì „ëµ
        if config.STRATEGIES.get('bollinger_mean_reversion', False):
            self.range_strategies.append(BollingerMeanReversionStrategy())
        if config.STRATEGIES.get('vwap_deviation', False):
            self.range_strategies.append(VWAPDeviationStrategy())
        if config.STRATEGIES.get('range_top_bottom', False):
            self.range_strategies.append(RangeTopBottomStrategy())
        if config.STRATEGIES.get('stoch_rsi_mean_reversion', False):
            self.range_strategies.append(StochRSIMeanReversionStrategy())
        if config.STRATEGIES.get('cmf_divergence', False):
            self.range_strategies.append(CMFDivergenceStrategy())
        
        # [ì¶”ê°€] Williams %R ì „ëµ (íš¡ë³´/ë°˜ì „ìš©)
        self.range_strategies.append(WilliamsRStrategy())
        
        # ì „ì²´ í•©ì¹˜ê¸° (ì´ 12ê°œ)
        self.strategies = self.breakout_strategies + self.range_strategies
        
        if len(self.strategies) == 0:
            raise ValueError("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. config.pyì—ì„œ ì „ëµì„ í™œì„±í™”í•˜ì„¸ìš”.")
        
        logger.info(f"ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.strategies)}ê°œ ì „ëµ (ëª©í‘œ: 12ê°œ)")
        
        # 3. [í•µì‹¬] í”¼ì²˜ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„± (DQN ìŠ¤íƒ€ì¼)
        # íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ìƒì„± í›„ ì €ì¥í•©ë‹ˆë‹¤.
        self._load_or_create_features()
        
        # 4. í™˜ê²½ ìƒì„±
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        
        # 5. ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (ë¡œë“œëœ ë°ì´í„° ì‚¬ìš©)
        self._fit_global_scaler()
        
        # 6. ì—ì´ì „íŠ¸ ìƒì„±
        state_dim = self.env.get_state_dim() # 29
        # info_dim = ì „ëµ ì ìˆ˜ ê°œìˆ˜ + í¬ì§€ì…˜ ì •ë³´ ê°œìˆ˜ (3ê°œ)
        info_dim = len(self.strategies) + 3  # 12ê°œ ì „ëµ + 3ê°œ í¬ì§€ì…˜ ì •ë³´ = 15
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"ì •ë³´ ì°¨ì›: {info_dim} (ì „ëµ {len(self.strategies)}ê°œ + í¬ì§€ì…˜ ì •ë³´ 3ê°œ)")
        self.agent = PPOAgent(state_dim, action_dim=3, hidden_dim=128, device=device, info_dim=info_dim)
        
        # ëª¨ë¸ ë¡œë“œ
        if os.path.exists(config.AI_MODEL_PATH):
            try:
                self.agent.load_model(config.AI_MODEL_PATH)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                logger.info("ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘")
        
        # ìƒíƒœ ë³€ìˆ˜
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        self.prev_pnl = 0.0
        self.episode_rewards = []
        self.total_steps = 0
        
        if enable_visualization:
            self.visualizer = LiveVisualizer()
        else:
            self.visualizer = None

    def _load_or_create_features(self):
        """
        [DQN ìŠ¤íƒ€ì¼] í”¼ì²˜ ìºì‹± ì‹œìŠ¤í…œ
        1. data/training_features.csv í™•ì¸
        2. ìˆìœ¼ë©´ -> ë¡œë“œ (ì´ˆê³ ì†)
        3. ì—†ìœ¼ë©´ -> ê³„ì‚° í›„ ì €ì¥ (ìµœì´ˆ 1íšŒ)
        """
        feature_file_path = 'data/training_features.csv'
        
        # 1. ìºì‹œ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
        if os.path.exists(feature_file_path):
            logger.info(f"ğŸ“‚ ìºì‹œëœ í”¼ì²˜ íŒŒì¼ ë°œê²¬: {feature_file_path}")
            logger.info("âš¡ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ê±´ë„ˆë›°ê³  ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            
            try:
                # CSV ë¡œë“œ (ì¸ë±ìŠ¤ëŠ” timestampë¡œ ì§€ì •)
                # parse_dates=Trueë¡œ ë‚ ì§œ í˜•ì‹ ìë™ ë³€í™˜
                df = pd.read_csv(feature_file_path, index_col=0, parse_dates=True)
                
                # ë°ì´í„° êµì²´
                self.data_collector.eth_data = df
                logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
                return
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì¬ìƒì„±í•©ë‹ˆë‹¤): {e}")
        
        # 2. íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        logger.info("ğŸš€ í”¼ì²˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœì´ˆ 1íšŒ ìˆ˜í–‰)...")
        
        eth_data = self.data_collector.eth_data
        btc_data = self.data_collector.btc_data
        
        if eth_data is None or len(eth_data) == 0:
            logger.error("ì›ë³¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì¸ë±ìŠ¤ ì •ë¦¬
        if not isinstance(eth_data.index, pd.DatetimeIndex):
            if 'timestamp' in eth_data.columns:
                eth_data.index = pd.to_datetime(eth_data['timestamp'])
            else:
                eth_data.index = pd.date_range(end=pd.Timestamp.now(), periods=len(eth_data), freq='3min')
        
        if btc_data is not None and not isinstance(btc_data.index, pd.DatetimeIndex):
             if 'timestamp' in btc_data.columns:
                btc_data.index = pd.to_datetime(btc_data['timestamp'])

        # Feature Engineering
        fe = FeatureEngineer(eth_data, btc_data)
        df = fe.generate_features()
        
        if df is None: return
            
        # MTF Processing
        mtf = MTFProcessor(df)
        df = mtf.add_mtf_features()
        
        # [ì¤‘ìš”] CSVë¡œ ì €ì¥ (ë‚˜ì¤‘ì„ ìœ„í•´)
        os.makedirs('data', exist_ok=True)
        df.to_csv(feature_file_path, index=True)
        
        # ë©”ëª¨ë¦¬ì— ì ìš©
        self.data_collector.eth_data = df
        logger.info(f"ğŸ’¾ í”¼ì²˜ ê³„ì‚° ë° ì €ì¥ ì™„ë£Œ: {feature_file_path}")

    def _fit_global_scaler(self):
        """ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì ìš©)"""
        try:
            logger.info("ğŸš€ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘ (Data Leakage ë°©ì§€ ì ìš©)...")
            df = self.data_collector.eth_data
            
            if df is None or len(df) == 0:
                logger.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # 1. ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  (Time Series Split)
            total_len = len(df)
            train_end = int(total_len * config.TRAIN_SPLIT)
            val_end = int(total_len * config.VAL_SPLIT)
            
            # ë‚˜ì¤‘ì— ì“°ê¸° ìœ„í•´ ì €ì¥
            self.train_end_idx = train_end
            self.val_end_idx = val_end
            
            logger.info(f"ë°ì´í„° ë¶„í• : Train(~{train_end}), Val(~{val_end}), Test(~{total_len})")

            # 2. í•™ìŠµ ë°ì´í„°ë§Œ ì¶”ì¶œ
            train_df = df.iloc[:train_end].copy()
            
            # ì‚¬ìš©í•  29ê°œ ì»¬ëŸ¼ ì •ì˜ (DQNê³¼ ë™ì¼)
            target_cols = [
                'log_return', 'roll_return_6', 'atr_ratio', 'bb_width', 'bb_pos', 
                'rsi', 'macd_hist', 'hma_ratio', 'cci', 
                'rvol', 'taker_ratio', 'cvd_change', 'mfi', 'cmf', 'vwap_dist',
                'wick_upper', 'wick_lower', 'range_pos', 'swing_break', 'chop',
                'btc_return', 'btc_rsi', 'btc_corr', 'btc_vol', 'eth_btc_ratio',
                'rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'
            ]
            
            # ì—†ëŠ” ì»¬ëŸ¼ ì±„ìš°ê¸° & ìˆœì„œ ë³´ì¥
            missing_cols = [c for c in target_cols if c not in train_df.columns]
            if missing_cols:
                for c in missing_cols:
                    train_df[c] = 0.0
            
            # ìƒ˜í”Œë§ (Train ë°ì´í„° ë‚´ì—ì„œë§Œ, ìµœëŒ€ 5ë§Œê°œ)
            sample_size = min(50000, len(train_df))
            sampled_df = train_df.sample(n=sample_size)[target_cols]
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            data_array = sampled_df.values.astype(np.float32)
            self.env.preprocessor.fit(data_array)
            self.env.scaler_fitted = True
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            scaler_path = config.AI_MODEL_PATH.replace('.pth', '_scaler.pkl')
            if not scaler_path.endswith('.pkl'):
                scaler_path = config.AI_MODEL_PATH + '_scaler.pkl'
            self.env.preprocessor.save_scaler(scaler_path, feature_names=target_cols)
            
            logger.info("âœ… í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)

    def train_episode(self, episode_num, max_steps=480):
        """
        [ìˆ˜ì •] ì•ˆì •ì ì¸ ì—í”¼ì†Œë“œ í•™ìŠµ ë£¨í”„
        - í•™ìŠµ ë°ì´í„° êµ¬ê°„(0 ~ train_end_idx) ë‚´ì—ì„œë§Œ ëœë¤ ì‹œì‘
        - ì¸ë±ìŠ¤ ê²½ê³„ ì²´í¬ ê°•í™”
        """
        # í•™ìŠµ êµ¬ê°„ ì„¤ì •
        if not hasattr(self, 'train_end_idx'):
            self.train_end_idx = int(len(self.data_collector.eth_data) * 0.7)
            
        # ì‹œì‘ ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ ë²”ìœ„ (Lookback í™•ë³´ ~ í•™ìŠµêµ¬ê°„ ë - ì—í”¼ì†Œë“œ ê¸¸ì´)
        start_min = self.env.lookback + 50
        start_max = self.train_end_idx - max_steps - 50
        
        if start_max <= start_min:
            logger.error("í•™ìŠµ ë°ì´í„° êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            return None

        # ëœë¤ ì‹œì‘ì  ì„ íƒ
        import random
        start_idx = random.randint(start_min, start_max)
        self.data_collector.current_index = start_idx
        
        # ìƒíƒœ ì´ˆê¸°í™”
        self.prev_pnl = 0.0
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        
        # [ê°œì„  1] ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ LSTM ìƒíƒœ ì´ˆê¸°í™”
        self.agent.reset_episode_states()
        
        episode_reward = 0.0
        steps = 0
        
        for step in range(max_steps):
            current_idx = self.data_collector.current_index
            
            # 1. ì¸ë±ìŠ¤ ì´ˆê³¼ ì•ˆì „ì¥ì¹˜
            if current_idx >= self.train_end_idx:  # í•™ìŠµ êµ¬ê°„ ë„˜ì–´ê°€ë©´ ì¢…ë£Œ
                break
                
            # 2. ê´€ì¸¡ ì •ë³´ ìƒì„±
            pos_val = 1.0 if self.current_position == 'LONG' else (-1.0 if self.current_position == 'SHORT' else 0.0)
            holding_time_idx = (current_idx - self.entry_index) if self.entry_index is not None else 0
            hold_val = holding_time_idx / max_steps
            pnl_val = self.prev_pnl * 10
            pos_info = [pos_val, pnl_val, hold_val]
            
            # get_observation í˜¸ì¶œ (mask í¬í•¨)
            state = self.env.get_observation(
                position_info=pos_info,
                current_index=current_idx,
                entry_index=self.entry_index,
                current_position=self.current_position
            )
            
            if state is None:
                break
            
            # 3. í–‰ë™ ì„ íƒ
            action, log_prob = self.agent.select_action(state)
            
            # 4. ê°€ê²© ë°ì´í„° ë° ë³´ìƒ ê³„ì‚°
            try:
                current_price = float(self.data_collector.eth_data.iloc[current_idx]['close'])
                
                # --- ë³´ìƒ ê³„ì‚° ë° í¬ì§€ì…˜ ë¡œì§ ---
                reward = 0.0
                trade_done = False
                current_pnl = 0.0
                pnl_change = 0.0
                
                # ğŸ›‘ [ì¶”ê°€] ê°•ì œ ì†ì ˆ (Hard Stop Loss) - 2% ì†ì‹¤ ì‹œ ë¬´ì¡°ê±´ ì²­ì‚°
                # í¬ì§€ì…˜ì´ ìˆì„ ë•Œë§Œ ì²´í¬
                if self.current_position is not None:
                    if self.current_position == 'LONG':
                        current_pnl = (current_price - self.entry_price) / self.entry_price
                    elif self.current_position == 'SHORT':
                        current_pnl = (self.entry_price - current_price) / self.entry_price
                    
                    stop_loss_threshold = config.STOP_LOSS_THRESHOLD
                    
                    if current_pnl < stop_loss_threshold:
                        # ê°•ì œ ì²­ì‚° ë¡œì§ ì‹¤í–‰
                        reward = self.env.calculate_reward(current_pnl, True, 0, 0)
                        # ì†ì ˆì€ ë¼ˆì•„í”„ê²Œ í˜ë„í‹° ì¶”ê°€
                        reward -= 1.0
                        
                        trade_done = True
                        logger.info(f"ğŸ›‘ ì†ì ˆ ë°œë™: ìˆ˜ìµë¥  {current_pnl:.2%}, ê°€ê²©: ${current_price:.2f}")
                        
                        # í¬ì§€ì…˜ ì´ˆê¸°í™”
                        self.current_position = None
                        self.entry_price = None
                        self.entry_index = None
                        self.prev_pnl = 0.0
                        
                        # íŠ¸ëœì§€ì…˜ ì €ì¥ ë° ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ
                        # stateëŠ” (obs_seq, obs_info, mask) íŠœí”Œì´ë¯€ë¡œ ì• 2ê°œë§Œ ì €ì¥
                        state_to_store = (state[0], state[1])
                        is_terminal = (step == max_steps - 1)
                        self.agent.store_transition(state_to_store, action, log_prob, reward, is_terminal)
                        episode_reward += reward
                        steps += 1
                        self.data_collector.current_index += 1
                        continue  # ì´ë²ˆ ìŠ¤í… ì¢…ë£Œ
                
                if action == 1: # LONG
                    if self.current_position != 'LONG':
                        if self.current_position == 'SHORT': # ì²­ì‚°
                            pnl = (self.entry_price - current_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl
                            reward = self.env.calculate_reward(pnl, True, 0, pnl_change)
                            trade_done = True
                            logger.info(f"ğŸ’° ìˆ ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}, ê°€ê²©: ${current_price:.2f}")
                            self.prev_pnl = 0.0
                        # ë¡± ì§„ì…
                        self.current_position = 'LONG'
                        self.entry_price = current_price
                        self.entry_index = current_idx
                        self.prev_pnl = 0.0
                        logger.debug(f"ğŸ“ˆ ë¡± ì§„ì…: ${current_price:.2f} (ì¸ë±ìŠ¤: {self.entry_index})")
                elif action == 2: # SHORT
                    if self.current_position != 'SHORT':
                        if self.current_position == 'LONG': # ì²­ì‚°
                            pnl = (current_price - self.entry_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl
                            reward = self.env.calculate_reward(pnl, True, 0, pnl_change)
                            trade_done = True
                            logger.info(f"ğŸ’° ë¡± ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}, ê°€ê²©: ${current_price:.2f}")
                            self.prev_pnl = 0.0
                        # ìˆ ì§„ì…
                        self.current_position = 'SHORT'
                        self.entry_price = current_price
                        self.entry_index = current_idx
                        self.prev_pnl = 0.0
                        logger.debug(f"ğŸ“‰ ìˆ ì§„ì…: ${current_price:.2f} (ì¸ë±ìŠ¤: {self.entry_index})")
                else: # HOLD
                    if self.current_position:
                        if self.current_position == 'LONG':
                            current_pnl = (current_price - self.entry_price) / self.entry_price
                        else:
                            current_pnl = (self.entry_price - current_price) / self.entry_price
                        pnl_change = current_pnl - self.prev_pnl
                        holding_time = current_idx - self.entry_index
                        reward = self.env.calculate_reward(current_pnl, False, holding_time, pnl_change)
                        self.prev_pnl = current_pnl
                    else:
                        # [ìˆ˜ì •] ë¬´í¬ì§€ì…˜ì¼ ë•Œ ê´€ë§ ë³´ìƒ
                        # ê¸°ì¡´: reward = -0.0001 (ê´€ë§í•˜ë©´ ë²Œì )
                        # ë³€ê²½: reward = 0.0 (ê´€ë§ì€ ë³¸ì „)
                        reward = 0.0
                
                # 5. íŠ¸ëœì§€ì…˜ ì €ì¥ (Mask ì œì™¸í•˜ê³  ì €ì¥)
                # stateëŠ” (obs_seq, obs_info, mask) íŠœí”Œì´ë¯€ë¡œ ì• 2ê°œë§Œ ì €ì¥
                state_to_store = (state[0], state[1])
                is_terminal = (step == max_steps - 1)
                
                self.agent.store_transition(state_to_store, action, log_prob, reward, is_terminal)
                episode_reward += reward
                steps += 1
                
                # 6. ë°°ì¹˜ ì—…ë°ì´íŠ¸
                if len(self.agent.memory) >= config.TRAIN_BATCH_SIZE:
                    # Next State (Bootstrapìš©)
                    next_idx = current_idx + 1
                    if not is_terminal and next_idx < self.train_end_idx:
                        # ë‹¤ìŒ ìƒíƒœ ê·¼ì‚¬
                        next_pos_info = pos_info  # ê·¼ì‚¬ê°’
                        next_state_full = self.env.get_observation(
                            position_info=next_pos_info,
                            current_index=next_idx,
                            entry_index=self.entry_index,
                            current_position=self.current_position
                        )
                        if next_state_full:
                            next_state = (next_state_full[0], next_state_full[1])  # Mask ì œì™¸
                            self.agent.update(next_state=next_state, episode=episode_num)
                    else:
                        self.agent.update(next_state=None, episode=episode_num)
                
                # ì¸ë±ìŠ¤ ì¦ê°€
                self.data_collector.current_index += 1
                
            except Exception as e:
                logger.error(f"Step Error: {e}")
                break
                
        return episode_reward, steps

    def train(self, num_episodes=1000, max_steps_per_episode=None, save_interval=None):
        # ê¸°ë³¸ê°’ ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        if max_steps_per_episode is None:
            max_steps_per_episode = config.TRAIN_MAX_STEPS_PER_EPISODE
        if save_interval is None:
            save_interval = config.TRAIN_SAVE_INTERVAL
        logger.info("ğŸš€ í•™ìŠµ ì‹œì‘")
        best_reward = float('-inf')
        scaler_path = config.AI_MODEL_PATH.replace('.pth', '_scaler.pkl')
        if not scaler_path.endswith('.pkl'):
            scaler_path = config.AI_MODEL_PATH + '_scaler.pkl'
        
        for episode in range(1, num_episodes + 1):
            try:
                result = self.train_episode(episode, max_steps_per_episode)
                if result is None: continue
                
                reward, steps = result
                self.episode_rewards.append(reward)
                if self.visualizer: self.visualizer.update(reward)
                
                avg_reward = np.mean(self.episode_rewards[-10:])
                logger.info(f"Ep {episode}: Reward {reward:.4f} | Avg {avg_reward:.4f} | Steps {steps}")
                
                # [ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸]
                self.agent.step_scheduler(avg_reward)
                
                # ëª¨ë¸ ì €ì¥
                if reward > best_reward:
                    best_reward = reward
                    self.agent.save_model(config.AI_MODEL_PATH)
                    # ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥ (ì¤‘ìš”)
                    self.env.preprocessor.save_scaler(scaler_path)
                elif episode % save_interval == 0:
                    self.agent.save_model(config.AI_MODEL_PATH)
                    self.env.preprocessor.save_scaler(scaler_path)
                    
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨")
                break
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ ì˜¤ë¥˜: {e}")
                continue

if __name__ == '__main__':
    trainer = PPOTrainer(enable_visualization=True)
    trainer.train()
