"""
PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ë²„ì „: 3-Action Target Position)
- Action 0: NEUTRAL (ì²­ì‚° ë˜ëŠ” ê´€ë§)
- Action 1: LONG (ì§„ì… ë˜ëŠ” ìŠ¤ìœ„ì¹­ ë˜ëŠ” í™€ë”©)
- Action 2: SHORT (ì§„ì… ë˜ëŠ” ìŠ¤ìœ„ì¹­ ë˜ëŠ” í™€ë”©)
- ê°œì„ ì‚¬í•­: 1-Step ìŠ¤ìœ„ì¹­ ì§€ì›, LSTM State Reset ì ìš©, ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€(Scaler)
"""
import logging
import os
import sys
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from core import DataCollector
from strategies import *
from model.trading_env import TradingEnvironment
from model.ppo_agent import PPOAgent

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(message)s',
    handlers=[logging.FileHandler('logs/train_ppo.log', encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

logging.getLogger('model.feature_engineering').setLevel(logging.WARNING)
logging.getLogger('model.mtf_processor').setLevel(logging.WARNING)

try:
    from joblib import Parallel, delayed
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False

# ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
def calculate_chunk(start_idx, end_idx, strategies, collector_data):
    from core import DataCollector
    temp_collector = DataCollector(use_saved_data=True)
    temp_collector.eth_data = collector_data
    results = {}
    for s_idx in range(len(strategies)):
        results[f'strategy_{s_idx}'] = np.zeros(end_idx - start_idx)
    for i in range(start_idx, end_idx):
        temp_collector.current_index = i
        rel_i = i - start_idx
        for s_idx, strategy in enumerate(strategies):
            try:
                result = strategy.analyze(temp_collector)
                score = 0.0
                if result:
                    conf = float(result.get('confidence', 0.0))
                    sig = result.get('signal', 'NEUTRAL')
                    if sig == 'LONG': score = conf
                    elif sig == 'SHORT': score = -conf
                results[f'strategy_{s_idx}'][rel_i] = score
            except: continue
    return results

class PPOTrainer:
    def __init__(self, enable_visualization=True):
        self.data_collector = DataCollector(use_saved_data=True)
        self.strategies = [
            BTCEthCorrelationStrategy(), VolatilitySqueezeStrategy(), OrderblockFVGStrategy(),
            HMAMomentumStrategy(), MFIMomentumStrategy(), BollingerMeanReversionStrategy(),
            VWAPDeviationStrategy(), RangeTopBottomStrategy(), StochRSIMeanReversionStrategy(),
            CMFDivergenceStrategy(), CCIReversalStrategy(), WilliamsRStrategy()
        ]
        
        # 1. ë°ì´í„° ë¡œë“œ (Forward Fill ì ìš©)
        self._load_features()
        self.precalculate_strategies_parallel()
        
        # 2. í™˜ê²½ ì„¤ì •
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        self._fit_global_scaler()

        # 3. ì—ì´ì „íŠ¸ ì„¤ì • (3-Action Target)
        state_dim = self.env.get_state_dim()
        action_dim = 3  # 0:Neutral, 1:Long, 2:Short
        info_dim = len(self.strategies) + 3
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        logger.info(f"Setting: Device={device} | Action Dim={action_dim} (3-Action Target)")
        
        self.agent = PPOAgent(state_dim, action_dim, info_dim=info_dim, device=device)
        
        # ëª¨ë¸ ë¡œë“œ
        base_path = config.AI_MODEL_PATH.replace('.pth', '')
        last_model_path = f"{base_path}_last.pth"
        if os.path.exists(last_model_path):
            try:
                self.agent.load_model(last_model_path)
                logger.info("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                logger.warning("âš ï¸ êµ¬ì¡° ë³€ê²½ ê°ì§€ -> ìƒˆ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        self.episode_rewards = []
        self.avg_rewards = []
        
        try:
            plt.ion()
            self.fig, self.ax = plt.subplots(figsize=(10, 5))
            self.plotting_enabled = True
        except: self.plotting_enabled = False

    def _load_features(self):
        """
        í”¼ì²˜ íŒŒì¼ ë¡œë“œ ë˜ëŠ” ìƒì„±
        """
        path = 'data/training_features.csv'
        cached_strategies_path = 'data/cached_strategies.csv'
        
        # 1. íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        if os.path.exists(path):
            logger.info(f"ğŸ“‚ ê¸°ì¡´ í”¼ì²˜ íŒŒì¼ ë¡œë“œ: {path}")
            df = pd.read_csv(path, index_col=0, parse_dates=True)
            
            # Forward Fillë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Data Quality)
            df = df.ffill().bfill()
            
            if os.path.exists(cached_strategies_path):
                try:
                    cached_df = pd.read_csv(cached_strategies_path, index_col=0, parse_dates=True)
                    strategy_cols = [c for c in cached_df.columns if c.startswith('strategy_')]
                    for col in strategy_cols:
                        if col in cached_df.columns: df[col] = cached_df[col]
                    logger.info("ğŸ“‚ ìºì‹œëœ ì „ëµ ì‹ í˜¸ ë³‘í•© ì™„ë£Œ")
                except: pass
                
            self.data_collector.eth_data = df
            
        # 2. íŒŒì¼ì´ ì—†ìœ¼ë©´ -> ì›ë³¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™” (ìƒˆë¡œ ê³„ì‚° ì¤€ë¹„)
        else:
            logger.warning("âš ï¸ í”¼ì²˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™”í•˜ê³  ìƒˆë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.")
            # ì›ë³¸ ë°ì´í„°ê°€ data_collectorì— ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆë‹¤ê³  ê°€ì •
            if self.data_collector.eth_data is None:
                logger.error("âŒ ì›ë³¸ ë°ì´í„°(ETH)ë„ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. collect_training_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                sys.exit(1)
            
            # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ (ìƒˆë¡œ ê³„ì‚°)
            # (ì´ ë¶€ë¶„ì€ precalculate_strategies_parallel ì—ì„œ ìˆ˜í–‰ë˜ê±°ë‚˜, 
            #  ì—¬ê¸°ì„œ ëª…ì‹œì ìœ¼ë¡œ feature engineeringì„ í˜¸ì¶œí•´ì•¼ í•  ìˆ˜ ìˆìŒ)
            #  ì¼ë‹¨ì€ ë¹ˆ ìƒíƒœë¡œ ë‘ê³  ë’¤ì—ì„œ ê³„ì‚°í•˜ë„ë¡ íŒ¨ìŠ¤
            pass

    def precalculate_strategies_parallel(self):
        """
        [Critical Fix] ì „ëµ ì‹ í˜¸ ê³„ì‚° ì‹œ Look-ahead Bias ì°¨ë‹¨
        - ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ, ì˜¤ì§ Train Set êµ¬ê°„ë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.
        - Test Set êµ¬ê°„ì€ 0.0ìœ¼ë¡œ ë‚¨ê²¨ë‘ì–´ ë¬¼ë¦¬ì ìœ¼ë¡œ ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
        """
        df = self.data_collector.eth_data
        if 'strategy_0' in df.columns: return

        logger.info("ğŸ§  ì „ëµ ì‹ í˜¸ ê³„ì‚° ì¤‘... (Only Train Set)")
        
        # [ìˆ˜ì •] ì „ì²´ ê¸¸ì´ê°€ ì•„ë‹ˆë¼, í•™ìŠµ ë°ì´í„° êµ¬ê°„ê¹Œì§€ë§Œ ê³„ì‚°
        total_len = len(df)
        train_end_idx = int(total_len * config.TRAIN_SPLIT)
        
        start_idx = config.LOOKBACK + 50
        
        # Joblib ë³‘ë ¬ ì²˜ë¦¬
        if JOBLIB_AVAILABLE and train_end_idx > 10000:
            from multiprocessing import cpu_count
            n_jobs = max(1, cpu_count() - 1)
            # ì²­í¬ë¥¼ ë‚˜ëˆŒ ë•Œ train_end_idxë¥¼ ëìœ¼ë¡œ ì„¤ì •
            chunk_size = (train_end_idx - start_idx) // n_jobs
            chunks = [(start_idx + i*chunk_size, start_idx + (i+1)*chunk_size if i < n_jobs-1 else train_end_idx) for i in range(n_jobs)]
            
            results_list = Parallel(n_jobs=n_jobs)(delayed(calculate_chunk)(s, e, self.strategies, df) for s, e in chunks)
            
            for s_idx in range(len(self.strategies)):
                col = f'strategy_{s_idx}'
                df[col] = 0.0  # ì´ˆê¸°í™” (Test Setì€ 0ìœ¼ë¡œ ìœ ì§€ë¨)
                
                # ê³„ì‚°ëœ Train Set êµ¬ê°„ë§Œ ì±„ì›Œë„£ê¸°
                full_s = np.zeros(total_len)
                for i, (s, e) in enumerate(chunks):
                    # ì²­í¬ ë²”ìœ„ë§Œí¼ë§Œ ì—…ë°ì´íŠ¸
                    chunk_len = e - s
                    # results_list[i][col]ì˜ ê¸¸ì´ê°€ chunk_lenê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(results_list[i][col]) == chunk_len:
                        full_s[s:e] = results_list[i][col]
                
                df[col] = full_s
        else:
            # ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€ ì‹œ ìˆœì°¨ ì²˜ë¦¬ (ë²”ìœ„ ì œí•œ ì ìš©)
            self._precalculate_strategies_sequential(df, start_idx, train_end_idx)
            
        df.to_csv('data/training_features.csv', index=True)
        # ìºì‹±ë„ ìˆ˜í–‰
        strategy_cols = [c for c in df.columns if c.startswith('strategy_')]
        if strategy_cols:
            df[strategy_cols].to_csv('data/cached_strategies.csv', index=True)
            
        self.data_collector.eth_data = df

    def _precalculate_strategies_sequential(self, df, start_idx, end_idx):
        """ìˆœì°¨ ê³„ì‚° (Train Set êµ¬ê°„ ì œí•œ ì ìš©ë¨)"""
        # ì»¬ëŸ¼ ì´ˆê¸°í™”
        for i in range(len(self.strategies)): 
            if f'strategy_{i}' not in df.columns:
                df[f'strategy_{i}'] = 0.0
            
        # [í•µì‹¬] end_idxê¹Œì§€ë§Œ ë£¨í”„ë¥¼ ë” (Train Set ì´í›„ëŠ” ê³„ì‚° ì•ˆ í•¨)
        for i in tqdm(range(start_idx, end_idx), desc="Calc Strategies (Train Only)"):
            self.data_collector.current_index = i
            for s_idx, strategy in enumerate(self.strategies):
                try:
                    res = strategy.analyze(self.data_collector)
                    score = 0.0
                    if res:
                        conf = float(res.get('confidence', 0.0))
                        sig = res.get('signal', 'NEUTRAL')
                        score = conf if sig == 'LONG' else (-conf if sig == 'SHORT' else 0.0)
                    df.iat[i, df.columns.get_loc(f'strategy_{s_idx}')] = score
                except: continue

    def _fit_global_scaler(self):
        """[Critical] Look-ahead Bias ë°©ì§€: ì˜¤ì§ Train Setìœ¼ë¡œë§Œ Scaler í•™ìŠµ"""
        if not self.env.scaler_fitted:
            df = self.data_collector.eth_data
            
            # config.TRAIN_SPLIT ì—„ê²© ì ìš©
            train_size = int(len(df) * config.TRAIN_SPLIT)
            self.train_end_idx = train_size
            
            train_df = df.iloc[:train_size].copy()
            target_cols = [
                'log_return', 'roll_return_6', 'atr_ratio', 'bb_width', 'bb_pos', 
                'rsi', 'macd_hist', 'hma_ratio', 'cci', 
                'rvol', 'taker_ratio', 'cvd_change', 'mfi', 'cmf', 'vwap_dist',
                'wick_upper', 'wick_lower', 'range_pos', 'swing_break', 'chop',
                'btc_return', 'btc_rsi', 'btc_corr', 'btc_vol', 'eth_btc_ratio',
                'rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'
            ]
            
            for col in target_cols:
                if col not in train_df.columns: train_df[col] = 0.0
            
            # ëœë¤ ìƒ˜í”Œë§ë„ Train ë°ì´í„° ë‚´ì—ì„œë§Œ
            sample = train_df[target_cols].sample(n=min(20000, len(train_df))).values.astype(np.float32)
            self.env.preprocessor.fit(sample)
            self.env.scaler_fitted = True
            
            path = config.AI_MODEL_PATH.replace('.pth', '_scaler.pkl')
            self.env.preprocessor.save(path)

    def train_episode(self, episode_num, max_steps=None):
        """ì—í”¼ì†Œë“œ í•™ìŠµ (3-Action Target Position + State Reset)"""
        if max_steps is None: max_steps = config.TRAIN_MAX_STEPS_PER_EPISODE
        
        start_min = config.LOOKBACK + 100
        start_max = self.train_end_idx - max_steps - 50
        if start_max <= start_min: return None
        
        start_idx = np.random.randint(start_min, start_max)
        self.data_collector.current_index = start_idx
        
        current_position = None
        entry_price = 0.0
        entry_index = 0
        episode_reward = 0.0
        trade_count = 0
        prev_unrealized_pnl = 0.0
        
        # [ì¤‘ìš”] ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ State Reset
        self.agent.reset_episode_states()
        pbar = tqdm(range(max_steps), desc=f"Ep {episode_num}", leave=False)
        
        for step in pbar:
            current_idx = self.data_collector.current_index
            if current_idx >= self.train_end_idx: break
            
            curr_price = float(self.data_collector.eth_data.iloc[current_idx]['close'])
            
            # PnL ê³„ì‚°
            unrealized_pnl = 0.0
            if current_position == 'LONG':
                unrealized_pnl = (curr_price - entry_price) / entry_price
            elif current_position == 'SHORT':
                unrealized_pnl = (entry_price - curr_price) / entry_price
            
            step_pnl = unrealized_pnl - prev_unrealized_pnl if current_position else 0.0
            
            # ìƒíƒœ ê´€ì¸¡
            pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            holding_time = (current_idx - entry_index) if current_position else 0
            pos_info = [pos_val, unrealized_pnl * 10, holding_time / max_steps]
            
            state = self.env.get_observation(position_info=pos_info, current_index=current_idx)
            if state is None: break

            prev_pos_str = current_position 

            # í–‰ë™ ì„ íƒ (3-Action)
            action, prob = self.agent.select_action(state)
            
            reward = 0.0
            trade_done = False
            realized_pnl = 0.0
            
            # -----------------------------------------------------------
            # 3-Action Logic (Target Position)
            # -----------------------------------------------------------
            
            # Action 0: Neutral (ì²­ì‚° ë˜ëŠ” ê´€ë§)
            if action == 0:
                if current_position is not None:
                    realized_pnl = unrealized_pnl
                    trade_done = True
                    current_position = None
                    entry_price = 0.0; entry_index = 0
                    trade_count += 1
            
            # Action 1: Long (ì§„ì… ë˜ëŠ” ìŠ¤ìœ„ì¹­ ë˜ëŠ” í™€ë”©)
            elif action == 1:
                if current_position == 'SHORT': # [ì¦‰ì‹œ ìŠ¤ìœ„ì¹­]
                    realized_pnl = unrealized_pnl
                    trade_done = True
                    # ìŠ¤ìœ„ì¹­ ì‹œ ì¦‰ì‹œ ì¬ì§„ì…
                    current_position = 'LONG'
                    entry_price = curr_price; entry_index = current_idx
                    trade_count += 1
                elif current_position is None: # ì§„ì…
                    current_position = 'LONG'
                    entry_price = curr_price; entry_index = current_idx
                    trade_count += 1
                # ì´ë¯¸ LONGì´ë©´ Pass (í™€ë”©)

            # Action 2: Short (ì§„ì… ë˜ëŠ” ìŠ¤ìœ„ì¹­ ë˜ëŠ” í™€ë”©)
            elif action == 2:
                if current_position == 'LONG': # [ì¦‰ì‹œ ìŠ¤ìœ„ì¹­]
                    realized_pnl = unrealized_pnl
                    trade_done = True
                    # ìŠ¤ìœ„ì¹­ ì‹œ ì¦‰ì‹œ ì¬ì§„ì…
                    current_position = 'SHORT'
                    entry_price = curr_price; entry_index = current_idx
                    trade_count += 1
                elif current_position is None: # ì§„ì…
                    current_position = 'SHORT'
                    entry_price = curr_price; entry_index = current_idx
                    trade_count += 1
                # ì´ë¯¸ SHORTë©´ Pass (í™€ë”©)

            # ë¦¬ì›Œë“œ ê³„ì‚°
            reward = self.env.calculate_reward(
                step_pnl=step_pnl, 
                realized_pnl=realized_pnl, 
                trade_done=trade_done, 
                action=action,              
                prev_position=prev_pos_str,
                current_position=current_position
            )
            
            # [ìˆ˜ì •] ê±°ë˜ ì¢…ë£Œ ì‹œ LSTM ìƒíƒœ ì´ˆê¸°í™” (ë…ë¦½ì„± ë³´ì¥)
            if trade_done:
                self.agent.reset_episode_states()

            # ë‹¤ìŒ ìŠ¤í…
            prev_unrealized_pnl = unrealized_pnl if not trade_done else 0.0
            self.data_collector.current_index += 1
            next_idx = self.data_collector.current_index
            
            next_pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            next_hold_time = (next_idx - entry_index) if current_position is not None else 0
            next_pos_info = [next_pos_val, 0.0, next_hold_time / max_steps]
            next_state = self.env.get_observation(position_info=next_pos_info, current_index=next_idx)
            
            done = False if step < max_steps - 1 else True
            if next_state is None: done = True; next_state = state
            
            self.agent.put_data((state, action, reward, next_state, prob, done))
            episode_reward += reward
            pbar.set_postfix({'R': f'{episode_reward:.1f}', 'Tr': trade_count})
            
            if done: break
        
        pbar.close()
        loss = self.agent.train_net(episode=episode_num)
        return episode_reward, trade_count

    def live_plot(self):
        if not self.plotting_enabled: return
        try:
            x = range(len(self.episode_rewards))
            self.line1.set_data(x, self.episode_rewards)
            self.line2.set_data(x, self.avg_rewards)
            self.ax.relim(); self.ax.autoscale_view()
            self.fig.canvas.draw(); self.fig.canvas.flush_events()
            plt.pause(0.01)
        except: pass

    def train(self, num_episodes=1000):
        logger.info("ğŸš€ PPO í•™ìŠµ ì‹œì‘ (3-Action Target + State Reset)")
        best_reward = -float('inf')
        base_path = config.AI_MODEL_PATH.replace('.pth', '')
        
        self.env.preprocessor.save(f"{base_path}_last_scaler.pkl")
        
        for ep in range(1, num_episodes + 1):
            try:
                res = self.train_episode(ep)
                if res is None: continue
                r, c = res
                self.episode_rewards.append(r)
                avg_r = np.mean(self.episode_rewards[-10:])
                self.avg_rewards.append(avg_r)
                logger.info(f"âœ… Ep {ep}: R {r:.2f} | Avg {avg_r:.2f} | Tr {c}")
                self.live_plot()
                if r > best_reward:
                    best_reward = r
                    self.agent.save_model(f"{base_path}_best.pth")
                    self.env.preprocessor.save(f"{base_path}_best_scaler.pkl")
                if ep % 10 == 0:
                    self.agent.save_model(f"{base_path}_last.pth")
            except KeyboardInterrupt: break
            except Exception as e: logger.error(f"Err: {e}"); continue
        if self.plotting_enabled: plt.ioff(); plt.show()

if __name__ == "__main__":
    trainer = PPOTrainer()
    trainer.train(num_episodes=config.TRAIN_NUM_EPISODES)