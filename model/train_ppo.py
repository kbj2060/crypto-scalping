"""
PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ê³¼ì‰ ê±°ë˜ ë°©ì§€ & ë¡œê·¸ ìµœì í™”)
- ìµœì†Œ ë³´ìœ  ì‹œê°„(Min Holding Time) 3ìº”ë“¤ ì ìš© -> ì¦ì€ ë§¤ë§¤ ë°©ì§€
- ìŠ¤í…ë³„ ë¡œê·¸ ì œê±° -> ì§„í–‰ë°”(tqdm)ë¡œ ê¹”ë”í•˜ê²Œ í™•ì¸
- Action 0ì˜ ì˜ë¯¸ ë³€ê²½: ìœ ì§€ -> ì²­ì‚°(Exit)
- AIê°€ í¬ì§€ì…˜ì„ ìœ ì§€í•˜ë ¤ë©´ ê³„ì†í•´ì„œ 1(Long)ì´ë‚˜ 2(Short)ë¥¼ ë‚´ë±‰ì–´ì•¼ í•¨
"""
import logging
import os
import sys
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt  # ì‹œê°í™”
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from core import DataCollector
from strategies import (
    BTCEthCorrelationStrategy, VolatilitySqueezeStrategy, OrderblockFVGStrategy,
    HMAMomentumStrategy, MFIMomentumStrategy, BollingerMeanReversionStrategy,
    VWAPDeviationStrategy, RangeTopBottomStrategy, StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy, CCIReversalStrategy, WilliamsRStrategy
)
from model.trading_env import TradingEnvironment
from model.ppo_agent import PPOAgent
from model.feature_engineering import FeatureEngineer
from model.mtf_processor import MTFProcessor

# ë¡œê¹… ì„¤ì • (ë¡œê·¸ ë ˆë²¨ ì¡°ì • - ë¶ˆí•„ìš”í•œ ë¡œê·¸ ìƒëµ)
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(message)s',
    handlers=[
        logging.FileHandler('logs/train_ppo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œê·¸ ë„ê¸°
logging.getLogger('model.feature_engineering').setLevel(logging.WARNING)
logging.getLogger('model.mtf_processor').setLevel(logging.WARNING)

# ë³‘ë ¬ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ (ì„ íƒì )
try:
    from joblib import Parallel, delayed, cpu_count
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False
    logger.info("joblibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „ëµì„ ê³„ì‚°í•©ë‹ˆë‹¤.")

def calculate_chunk(start_idx, end_idx, strategies, collector_data):
    """ì „ëµ ê³„ì‚° ì²­í¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    from core import DataCollector
    temp_collector = DataCollector(use_saved_data=True)
    temp_collector.eth_data = collector_data
    results = {}
    for s_idx in range(len(strategies)):
        results[f'strategy_{s_idx}'] = np.zeros(end_idx - start_idx)

    for i in range(start_idx, end_idx):
        temp_collector.current_index = i
        rel_i = i - start_idx
        for s_idx, strategy in enumerate(strategies):
            try:
                result = strategy.analyze(temp_collector)
                score = 0.0
                if result:
                    conf = float(result.get('confidence', 0.0))
                    signal = result.get('signal', 'NEUTRAL')
                    if signal == 'LONG': score = conf
                    elif signal == 'SHORT': score = -conf
                results[f'strategy_{s_idx}'][rel_i] = score
            except:
                continue
    return results

class PPOTrainer:
    def __init__(self, enable_visualization=True):
        self.data_collector = DataCollector(use_saved_data=True)
        self.strategies = [
            BTCEthCorrelationStrategy(), VolatilitySqueezeStrategy(), OrderblockFVGStrategy(),
            HMAMomentumStrategy(), MFIMomentumStrategy(), BollingerMeanReversionStrategy(),
            VWAPDeviationStrategy(), RangeTopBottomStrategy(), StochRSIMeanReversionStrategy(),
            CMFDivergenceStrategy(), CCIReversalStrategy(), WilliamsRStrategy()
        ]
        
        logger.info(f"ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.strategies)}ê°œ ì „ëµ")
        
        # 1. í”¼ì²˜ ë°ì´í„° ë¡œë“œ
        self._load_features()
        
        # 2. ì „ëµ ì‚¬ì „ ê³„ì‚° (ë³‘ë ¬ ì²˜ë¦¬)
        self.precalculate_strategies_parallel()
        
        # 3. í™˜ê²½ ì„¤ì •
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        self._fit_global_scaler()

        # 4. ì—ì´ì „íŠ¸ ì„¤ì •
        state_dim = self.env.get_state_dim()
        action_dim = 3  # HOLD, LONG, SHORT
        info_dim = len(self.strategies) + 3
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"ì •ë³´ ì°¨ì›: {info_dim} (ì „ëµ {len(self.strategies)}ê°œ + í¬ì§€ì…˜ ì •ë³´ 3ê°œ)")
        
        self.agent = PPOAgent(state_dim, action_dim, info_dim=info_dim, device=device)
        
        # ëª¨ë¸ ë¡œë“œ
        if os.path.exists(config.AI_MODEL_PATH):
            try:
                self.agent.load_model(config.AI_MODEL_PATH)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        self.episode_rewards = []
        self.avg_rewards = []  # í‰ê·  ë¦¬ì›Œë“œ ì¶”ì ìš©
        
        # [NEW] ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì„¤ì •
        try:
            plt.ion()  # Interactive Mode On
            self.fig, self.ax = plt.subplots(figsize=(10, 5))
            self.ax.set_title('PPO Real-time Training')
            self.ax.set_xlabel('Episode')
            self.ax.set_ylabel('Reward')
            self.ax.grid(True, alpha=0.3)
            self.line1, = self.ax.plot([], [], label='Reward', alpha=0.3, color='gray')
            self.line2, = self.ax.plot([], [], label='Avg (10)', color='red', linewidth=2)
            self.ax.legend()
            self.plotting_enabled = True
        except Exception as e:
            logger.warning(f"ê·¸ë˜í”„ ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            self.plotting_enabled = False

    def _load_features(self):
        """í”¼ì²˜ íŒŒì¼ ë¡œë“œ (ì „ëµ ìºì‹œ í¬í•¨)"""
        path = 'data/training_features.csv'
        cached_strategies_path = 'data/cached_strategies.csv'
        
        if os.path.exists(path):
            logger.info("ğŸ“‚ í”¼ì²˜ íŒŒì¼ ë¡œë“œ")
            df = pd.read_csv(path, index_col=0, parse_dates=True)
            
            # ì „ëµ ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë³‘í•©
            if os.path.exists(cached_strategies_path):
                logger.info("ğŸ“‚ ì „ëµ ìºì‹œ íŒŒì¼ ë°œê²¬, ë³‘í•© ì¤‘...")
                try:
                    cached_df = pd.read_csv(cached_strategies_path, index_col=0, parse_dates=True)
                    # ì „ëµ ì»¬ëŸ¼ë§Œ ë³‘í•© (strategy_0, strategy_1, ...)
                    strategy_cols = [col for col in cached_df.columns if col.startswith('strategy_')]
                    if strategy_cols:
                        for col in strategy_cols:
                            if col in cached_df.columns:
                                df[col] = cached_df[col]
                        logger.info(f"âœ… ì „ëµ ìºì‹œ ë³‘í•© ì™„ë£Œ: {len(strategy_cols)}ê°œ ì „ëµ")
                except Exception as e:
                    logger.warning(f"ì „ëµ ìºì‹œ ë³‘í•© ì‹¤íŒ¨: {e}")
            
            self.data_collector.eth_data = df
        else:
            logger.error("âŒ í”¼ì²˜ íŒŒì¼ ì—†ìŒ. ë¨¼ì € í”¼ì²˜ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            sys.exit(1)

    def precalculate_strategies_parallel(self):
        """ì „ëµ ì‹ í˜¸ ë³‘ë ¬ ê³„ì‚° (ìºì‹œ í™•ì¸ í¬í•¨)"""
        df = self.data_collector.eth_data
        
        # ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if 'strategy_0' in df.columns:
            logger.info("âœ… ì „ëµ ì‹ í˜¸ ì´ë¯¸ ì¡´ì¬ (ê³„ì‚° ìƒëµ)")
            return

        # cached_strategies.csv íŒŒì¼ í™•ì¸
        cached_strategies_path = 'data/cached_strategies.csv'
        if os.path.exists(cached_strategies_path):
            logger.info("ğŸ“‚ ì „ëµ ìºì‹œ íŒŒì¼ ë°œê²¬, ë¡œë“œ ì¤‘...")
            try:
                cached_df = pd.read_csv(cached_strategies_path, index_col=0, parse_dates=True)
                strategy_cols = [col for col in cached_df.columns if col.startswith('strategy_')]
                if strategy_cols and len(strategy_cols) == len(self.strategies):
                    # ì¸ë±ìŠ¤ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(cached_df) == len(df):
                        for col in strategy_cols:
                            df[col] = cached_df[col]
                        logger.info(f"âœ… ì „ëµ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(strategy_cols)}ê°œ ì „ëµ")
                        # training_features.csvì—ë„ ì €ì¥
                        df.to_csv('data/training_features.csv', index=True)
                        self.data_collector.eth_data = df
                        return
                    else:
                        logger.warning(f"ìºì‹œ íŒŒì¼ í¬ê¸° ë¶ˆì¼ì¹˜ (ìºì‹œ: {len(cached_df)}, í”¼ì²˜: {len(df)}), ì¬ê³„ì‚°í•©ë‹ˆë‹¤.")
                else:
                    logger.warning(f"ìºì‹œ íŒŒì¼ ì „ëµ ê°œìˆ˜ ë¶ˆì¼ì¹˜ (ìºì‹œ: {len(strategy_cols)}, í•„ìš”: {len(self.strategies)}), ì¬ê³„ì‚°í•©ë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"ì „ëµ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}, ì¬ê³„ì‚°í•©ë‹ˆë‹¤.")

        logger.info("ğŸ§  ì „ëµ ì‹ í˜¸ ê³„ì‚° ì‹œì‘...")
        total_len = len(df)
        start_idx = config.LOOKBACK + 50
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹œë„
        if JOBLIB_AVAILABLE and total_len > 10000:
            try:
                n_jobs = max(1, cpu_count() - 1)
                chunk_size = (total_len - start_idx) // n_jobs
                chunks = [(start_idx + i*chunk_size, 
                          start_idx + (i+1)*chunk_size if i < n_jobs-1 else total_len) 
                         for i in range(n_jobs)]
                
                logger.info(f"ë³‘ë ¬ ì²˜ë¦¬: {n_jobs}ê°œ ì‘ì—…ìœ¼ë¡œ ë¶„í• ")
                results_list = Parallel(n_jobs=n_jobs)(
                    delayed(calculate_chunk)(s, e, self.strategies, df) 
                    for s, e in chunks
                )
                
                # ê²°ê³¼ ë³‘í•©
                for s_idx in range(len(self.strategies)):
                    col_name = f'strategy_{s_idx}'
                    df[col_name] = 0.0
                    full_series = np.zeros(total_len)
                    for chunk_idx, (s, e) in enumerate(chunks):
                        full_series[s:e] = results_list[chunk_idx][col_name]
                    df[col_name] = full_series
                
                logger.info("âœ… ë³‘ë ¬ ê³„ì‚° ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")
                self._precalculate_strategies_sequential(df, start_idx, total_len)
        else:
            # ìˆœì°¨ ì²˜ë¦¬
            self._precalculate_strategies_sequential(df, start_idx, total_len)
        
        # ì €ì¥ (training_features.csvì™€ cached_strategies.csv ëª¨ë‘ ì €ì¥)
        df.to_csv('data/training_features.csv', index=True)
        # ì „ëµ ì»¬ëŸ¼ë§Œ ë³„ë„ë¡œ ì €ì¥ (ìºì‹œìš©)
        strategy_cols = [col for col in df.columns if col.startswith('strategy_')]
        if strategy_cols:
            cached_df = df[strategy_cols].copy()
            cached_df.to_csv('data/cached_strategies.csv', index=True)
            logger.info(f"ğŸ’¾ ì „ëµ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(strategy_cols)}ê°œ ì „ëµ")
        self.data_collector.eth_data = df

    def _precalculate_strategies_sequential(self, df, start_idx, total_len):
        """ìˆœì°¨ ì²˜ë¦¬ ì „ëµ ê³„ì‚°"""
        for i in range(len(self.strategies)):
            df[f'strategy_{i}'] = 0.0
        
        original_index = getattr(self.data_collector, 'current_index', 0)
        
        try:
            iterator = tqdm(range(start_idx, total_len), desc="Strategy Calc")
        except NameError:
            iterator = range(start_idx, total_len)
        
        for i in iterator:
            self.data_collector.current_index = i
            for s_idx, strategy in enumerate(self.strategies):
                try:
                    result = strategy.analyze(self.data_collector)
                    score = 0.0
                    if result:
                        conf = float(result.get('confidence', 0.0))
                        signal = result.get('signal', 'NEUTRAL')
                        if signal == 'LONG': score = conf
                        elif signal == 'SHORT': score = -conf
                    df.iat[i, df.columns.get_loc(f'strategy_{s_idx}')] = score
                except:
                    continue
        
        self.data_collector.current_index = original_index

    def _fit_global_scaler(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ"""
        try:
            logger.info("ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘ (Train Set 80%ë§Œ ì‚¬ìš©)...")
            df = self.data_collector.eth_data
            
            train_size = int(len(df) * config.TRAIN_SPLIT)
            self.train_end_idx = train_size
            train_df = df.iloc[:train_size].copy()
            
            target_cols = [
                'log_return', 'roll_return_6', 'atr_ratio', 'bb_width', 'bb_pos', 
                'rsi', 'macd_hist', 'hma_ratio', 'cci', 
                'rvol', 'taker_ratio', 'cvd_change', 'mfi', 'cmf', 'vwap_dist',
                'wick_upper', 'wick_lower', 'range_pos', 'swing_break', 'chop',
                'btc_return', 'btc_rsi', 'btc_corr', 'btc_vol', 'eth_btc_ratio',
                'rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'
            ]
            
            for col in target_cols:
                if col not in train_df.columns:
                    train_df[col] = 0.0
            
            sample_size = min(config.TRAIN_SAMPLE_SIZE, len(train_df))
            sampled_df = train_df.sample(n=sample_size)[target_cols]
            
            data_array = sampled_df.values.astype(np.float32)
            self.env.preprocessor.fit(data_array)
            self.env.scaler_fitted = True
            
            scaler_path = config.AI_MODEL_PATH.replace('.pth', '_scaler.pkl')
            if not scaler_path.endswith('.pkl'):
                scaler_path = config.AI_MODEL_PATH + '_scaler.pkl'
            self.env.preprocessor.save_scaler(scaler_path)
            
            logger.info(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)

    def train_episode(self, episode_num, max_steps=None):
        """ì—í”¼ì†Œë“œ í•™ìŠµ"""
        if max_steps is None:
            max_steps = config.TRAIN_MAX_STEPS_PER_EPISODE
        
        # ì‹œì‘ ì¸ë±ìŠ¤ ì„¤ì •
        start_min = config.LOOKBACK + 100
        start_max = self.train_end_idx - max_steps - 50
        
        if start_max <= start_min:
            logger.error("í•™ìŠµ ë°ì´í„° êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            return None
        
        start_idx = np.random.randint(start_min, start_max)
        self.data_collector.current_index = start_idx
        
        # [ì¤‘ìš”] ë³€ìˆ˜ ì´ˆê¸°í™” (NameError í•´ê²°)
        current_position = None
        entry_price = 0.0
        entry_index = 0
        episode_reward = 0.0
        trade_count = 0  # ê±°ë˜ íšŸìˆ˜ ì¶”ì 
        
        # [ê³¼ì‰ ê±°ë˜ ë°©ì§€] ìµœì†Œ ë³´ìœ  ì‹œê°„ ì„¤ì • (3~5 ìº”ë“¤)
        min_holding_steps = config.MIN_HOLDING_TIME if hasattr(config, 'MIN_HOLDING_TIME') else 3
        
        # LSTM ìƒíƒœ ì´ˆê¸°í™”
        self.agent.reset_episode_states()
        
        # ì§„í–‰ë°” ì„¤ì •
        pbar = tqdm(range(max_steps), desc=f"Ep {episode_num}", leave=False, unit="step")
        
        for step in pbar:
            current_idx = self.data_collector.current_index
            if current_idx >= self.train_end_idx:
                break
            
            # Position Info
            pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            holding_time = (current_idx - entry_index) if current_position is not None else 0
            curr_price = float(self.data_collector.eth_data.iloc[current_idx]['close'])
            
            unrealized_pnl = 0.0
            if current_position == 'LONG':
                unrealized_pnl = (curr_price - entry_price) / entry_price
            elif current_position == 'SHORT':
                unrealized_pnl = (entry_price - curr_price) / entry_price
            
            pos_info = [pos_val, unrealized_pnl * 10, holding_time / max_steps]
            
            # 1. ê´€ì¸¡
            state = self.env.get_observation(position_info=pos_info, current_index=current_idx)
            if state is None:
                break
            
            # 2. í–‰ë™
            action, prob = self.agent.select_action(state)
            
            # 3. íŠ¸ë ˆì´ë”© ë¡œì§
            reward = 0.0
            trade_done = False
            realized_pnl = 0.0
            
            # [ì ê¸ˆ ë¡œì§] ìµœì†Œ ë³´ìœ  ì‹œê°„ ë¯¸ë‹¬ ì‹œ ê°•ì œë¡œ í¬ì§€ì…˜ ìœ ì§€ (í–‰ë™ ë¬´ì‹œ)
            is_locked = (current_position is not None) and (holding_time < min_holding_steps)
            
            # A. ê°•ì œ ì²­ì‚° (Stop Loss - ì ê¸ˆ ë¬´ì‹œí•˜ê³  ì¦‰ì‹œ ì†ì ˆ)
            if current_position is not None and unrealized_pnl < config.STOP_LOSS_THRESHOLD:
                realized_pnl = unrealized_pnl
                trade_done = True
                current_position = None
                entry_price = 0.0
                entry_index = 0
                trade_count += 1
            
            # B. ëª¨ë¸ í–‰ë™ ì‹¤í–‰ (ì ê²¨ìˆì§€ ì•Šì„ ë•Œë§Œ)
            elif not is_locked and not trade_done:
                if action == 1:  # LONG ì‹ í˜¸
                    if current_position == 'SHORT':  # ìŠ¤ìœ„ì¹­ (Short -> Long)
                        realized_pnl = unrealized_pnl
                        trade_done = True
                        current_position = 'LONG'
                        entry_price = curr_price
                        entry_index = current_idx
                        trade_count += 1
                    elif current_position is None:  # ì‹ ê·œ ì§„ì… (Open Long)
                        current_position = 'LONG'
                        entry_price = curr_price
                        entry_index = current_idx
                        trade_count += 1
                        reward = 0  # ì§„ì… ì‹œì ì—” ë³´ìƒ 0
                    # ì´ë¯¸ LONGì´ë©´ ìœ ì§€ (Maintain) - ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
                        
                elif action == 2:  # SHORT ì‹ í˜¸
                    if current_position == 'LONG':  # ìŠ¤ìœ„ì¹­ (Long -> Short)
                        realized_pnl = unrealized_pnl
                        trade_done = True
                        current_position = 'SHORT'
                        entry_price = curr_price
                        entry_index = current_idx
                        trade_count += 1
                    elif current_position is None:  # ì‹ ê·œ ì§„ì… (Open Short)
                        current_position = 'SHORT'
                        entry_price = curr_price
                        entry_index = current_idx
                        trade_count += 1
                        reward = 0
                    # ì´ë¯¸ SHORTë©´ ìœ ì§€ (Maintain) - ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
                
                elif action == 0:  # EXIT / NEUTRAL ì‹ í˜¸
                    if current_position is not None:
                        # [í•µì‹¬ ë³€ê²½] Action 0ì´ ë‚˜ì˜¤ë©´ í¬ì§€ì…˜ ì²­ì‚°!
                        realized_pnl = unrealized_pnl
                        trade_done = True
                        current_position = None
                        entry_price = 0.0
                        entry_index = 0
                        trade_count += 1
                    else:
                        # í¬ì§€ì…˜ ì—†ìœ¼ë©´ ê³„ì† ê´€ë§ (Stay)
                        reward = 0  # ê´€ë§ì— ëŒ€í•œ ë³´ìƒ (í•„ìš”ì‹œ ì‘ì€ ì–‘ìˆ˜ ë¶€ì—¬ ê°€ëŠ¥)
            
            # ë³´ìƒ ê³„ì‚° (ê±°ë˜ ì™„ë£Œ ì‹œ)
            if trade_done:
                reward = self.env.calculate_reward(realized_pnl, True, holding_time)
            else:
                # í¬ì§€ì…˜ ìœ ì§€ ì¤‘ì—ë„ ì‹œê°„ í˜ë„í‹° ì ìš©
                if current_position is not None:
                    reward = self.env.calculate_reward(0.0, False, holding_time)
            
            # 4. ë‹¤ìŒ ìƒíƒœ (ì•ˆì „í•˜ê²Œ ìƒì„±)
            next_idx = current_idx + 1
            self.data_collector.current_index = next_idx
            
            # Next Info ê³„ì‚°
            next_pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            next_hold_time = (next_idx - entry_index) if current_position is not None else 0
            
            next_un_pnl = 0.0
            if next_idx < len(self.data_collector.eth_data) and current_position is not None:
                try:
                    next_price = float(self.data_collector.eth_data.iloc[next_idx]['close'])
                    if current_position == 'LONG':
                        next_un_pnl = (next_price - entry_price) / entry_price
                    elif current_position == 'SHORT':
                        next_un_pnl = (entry_price - next_price) / entry_price
                except:
                    pass
            
            next_pos_info = [next_pos_val, next_un_pnl * 10, next_hold_time / max_steps]
            next_state = self.env.get_observation(position_info=next_pos_info, current_index=next_idx)
            
            done = False if step < max_steps - 1 else True
            if next_state is None:
                done = True
                next_state = state  # Fallback
            
            # 5. ë°ì´í„° ì €ì¥
            self.agent.put_data((state, action, reward, next_state, prob, done))
            episode_reward += reward
            
            # ì§„í–‰ë°” ì—…ë°ì´íŠ¸ (í˜„ì¬ ìˆ˜ìµ, ê±°ë˜íšŸìˆ˜ í‘œì‹œ)
            pbar.set_postfix({'R': f'{episode_reward:.1f}', 'Tr': trade_count, 'P': f'{pos_val:.1f}'})
            
            if done:
                break
        
        pbar.close()
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ í•™ìŠµ
        loss = self.agent.train_net(episode=episode_num)
        return episode_reward

    def live_plot(self):
        """[NEW] ìœˆë„ìš°ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"""
        if not self.plotting_enabled:
            return
        
        try:
            x = range(len(self.episode_rewards))
            
            # ë°ì´í„° ì—…ë°ì´íŠ¸
            self.line1.set_data(x, self.episode_rewards)
            self.line2.set_data(x, self.avg_rewards)
            
            # ì¶• ë²”ìœ„ ìë™ ì¡°ì •
            self.ax.relim()
            self.ax.autoscale_view()
            
            # í™”ë©´ ê°±ì‹ 
            self.fig.canvas.draw()
            self.fig.canvas.flush_events()
            
            # ì•½ê°„ì˜ ë”œë ˆì´ (GUI ë°˜ì‘ìš©)
            plt.pause(0.01)
            
        except Exception:
            pass

    def train(self, num_episodes=1000):
        """í•™ìŠµ ë©”ì¸ ë£¨í”„"""
        logger.info("ğŸš€ PPO í•™ìŠµ ì‹œì‘ (Best Model Separation + Real-time Plotting)")
        
        best_reward = -float('inf')
        
        # ê²½ë¡œ ì„¤ì • (í™•ì¥ì ë¶„ë¦¬)
        base_path = config.AI_MODEL_PATH.replace('.pth', '')
        best_model_path = f"{base_path}_best.pth"
        best_scaler_path = f"{base_path}_best_scaler.pkl"
        last_model_path = f"{base_path}_last.pth"
        last_scaler_path = f"{base_path}_last_scaler.pkl"
        
        # ì´ˆê¸° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (Lastì— ì €ì¥)
        self.env.preprocessor.save(last_scaler_path)
        
        for ep in range(1, num_episodes + 1):
            try:
                reward = self.train_episode(ep)
                if reward is None:
                    continue
                
                self.episode_rewards.append(reward)
                avg_reward = np.mean(self.episode_rewards[-10:])
                self.avg_rewards.append(avg_reward)
                
                # ë¡œê·¸ëŠ” ë§¤ë²ˆ ì¶œë ¥ (ì§„í–‰ ìƒí™© í™•ì¸ìš©)
                logger.info(f"âœ… Ep {ep}: Reward {reward:.4f} | Avg {avg_reward:.4f}")
                
                # [NEW] ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
                self.live_plot()
                
                # [í•µì‹¬] ìµœê³  ê¸°ë¡ ê°±ì‹  ì‹œ -> '_best' íŒŒì¼ì— ì €ì¥
                if reward > best_reward:
                    best_reward = reward
                    logger.info(f"ğŸ† ì‹ ê¸°ë¡! ({best_reward:.4f}) -> Best ëª¨ë¸ ì €ì¥")
                    
                    self.agent.save_model(best_model_path)
                    self.env.preprocessor.save(best_scaler_path)
                
                # [í•µì‹¬] ì •ê¸° ì €ì¥ -> '_last' íŒŒì¼ì— ì €ì¥ (í˜¹ì€ ë§¤ë²ˆ ì €ì¥)
                # ì—ëŸ¬ ë“±ìœ¼ë¡œ ë©ˆì·„ì„ ë•Œ ì´ì–´ì„œ í•˜ê¸° ìœ„í•¨
                if ep % 10 == 0:
                    self.agent.save_model(last_model_path)
                    self.env.preprocessor.save(last_scaler_path)
                    
            except Exception as e:
                logger.error(f"Ep {ep} Fail: {e}")
                continue
        
        # í•™ìŠµ ì¢…ë£Œ ì‹œ ê·¸ë˜í”„ ì°½ ìœ ì§€
        if self.plotting_enabled:
            try:
                plt.ioff()  # Interactive Mode Off
                logger.info("ê·¸ë˜í”„ ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì°½ì„ ì§ì ‘ ë‹«ì•„ì£¼ì„¸ìš”.")
            except:
                pass

if __name__ == "__main__":
    trainer = PPOTrainer()
    trainer.train(num_episodes=config.TRAIN_NUM_EPISODES)
