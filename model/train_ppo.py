"""
PPO ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ë³„ë„ë¡œ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
"""
import logging
import os
import sys
import time
from datetime import datetime
from collections import deque
import numpy as np

# ì‹œê°í™” ëª¨ë“ˆ (ì„ íƒì )
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    plt = None

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€ (config, core, strategies ëª¨ë“ˆ ì ‘ê·¼ìš©)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core import DataCollector, BinanceClient
from core.indicators import Indicators
from strategies import (
    BTCEthCorrelationStrategy,
    VolatilitySqueezeStrategy,
    OrderblockFVGStrategy,
    HMAMomentumStrategy,
    MFIMomentumStrategy,
    BollingerMeanReversionStrategy,
    VWAPDeviationStrategy,
    RangeTopBottomStrategy,
    StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy
)

# AI ê°•í™”í•™ìŠµ ëª¨ë“ˆ
try:
    import torch
    from model.trading_env import TradingEnvironment
    from model.ppo_agent import PPOAgent
    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"âŒ AI ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("torchê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: pip install torch")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_ppo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# matplotlib ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë¡œê¹…
if not MATPLOTLIB_AVAILABLE:
    logger.warning("matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œê°í™” ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    logger.warning("ì„¤ì¹˜ ë°©ë²•: pip install matplotlib")


class LiveVisualizer:
    """í•™ìŠµ ë¦¬ì›Œë“œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„í™”í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, window_size=10):
        if not MATPLOTLIB_AVAILABLE:
            self.enabled = False
            return
        
        self.enabled = True
        plt.ion()  # ëŒ€í™”í˜• ëª¨ë“œ í™œì„±í™”
        self.fig, self.ax = plt.subplots(figsize=(10, 5))
        self.rewards = []
        self.moving_avg = []
        self.window_size = window_size
        
        self.ax.set_title("Real-time Training Performance")
        self.ax.set_xlabel("Episode")
        self.ax.set_ylabel("Total Reward")
        self.line1, = self.ax.plot([], [], label='Episode Reward', alpha=0.3, color='blue')
        self.line2, = self.ax.plot([], [], label=f'Moving Avg ({window_size})', color='red', linewidth=2)
        self.ax.legend()
        self.ax.grid(True)

    def update(self, reward):
        if not self.enabled:
            return
        
        self.rewards.append(reward)
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        if len(self.rewards) >= self.window_size:
            avg = np.mean(self.rewards[-self.window_size:])
        else:
            avg = np.mean(self.rewards)
        self.moving_avg.append(avg)
        
        # ë°ì´í„° ì—…ë°ì´íŠ¸
        x = np.arange(len(self.rewards))
        self.line1.set_data(x, self.rewards)
        self.line2.set_data(x, self.moving_avg)
        
        # í™”ë©´ ë²”ìœ„ ìë™ ì¡°ì ˆ
        self.ax.relim()
        self.ax.autoscale_view()
        
        plt.draw()
        plt.pause(0.01)  # ì§§ì€ íœ´ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ê°±ì‹  ë³´ì¥


class PPOTrainer:
    """PPO ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    def __init__(self, enable_visualization=False):
        """
        Args:
            enable_visualization: ì‹œê°í™” í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        # ì €ì¥ëœ ë°ì´í„° ì‚¬ìš© (í•™ìŠµìš©)
        self.data_collector = DataCollector(use_saved_data=True)
        self.client = BinanceClient()
        
        # ì „ëµ ì´ˆê¸°í™”
        self.breakout_strategies = []
        self.range_strategies = []
        
        # í­ë°œì¥ ì „ëµ
        if config.STRATEGIES.get('btc_eth_correlation', False):
            self.breakout_strategies.append(BTCEthCorrelationStrategy())
        if config.STRATEGIES.get('volatility_squeeze', False):
            self.breakout_strategies.append(VolatilitySqueezeStrategy())
        if config.STRATEGIES.get('orderblock_fvg', False):
            self.breakout_strategies.append(OrderblockFVGStrategy())
        if config.STRATEGIES.get('hma_momentum', False):
            self.breakout_strategies.append(HMAMomentumStrategy())
        if config.STRATEGIES.get('mfi_momentum', False):
            self.breakout_strategies.append(MFIMomentumStrategy())
        
        # íš¡ë³´ì¥ ì „ëµ
        if config.STRATEGIES.get('bollinger_mean_reversion', False):
            self.range_strategies.append(BollingerMeanReversionStrategy())
        if config.STRATEGIES.get('vwap_deviation', False):
            self.range_strategies.append(VWAPDeviationStrategy())
        if config.STRATEGIES.get('range_top_bottom', False):
            self.range_strategies.append(RangeTopBottomStrategy())
        if config.STRATEGIES.get('stoch_rsi_mean_reversion', False):
            self.range_strategies.append(StochRSIMeanReversionStrategy())
        if config.STRATEGIES.get('cmf_divergence', False):
            self.range_strategies.append(CMFDivergenceStrategy())
        
        self.strategies = self.breakout_strategies + self.range_strategies
        
        if len(self.strategies) == 0:
            raise ValueError("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. config.pyì—ì„œ ì „ëµì„ í™œì„±í™”í•˜ì„¸ìš”.")
        
        logger.info(f"ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.strategies)}ê°œ ì „ëµ")
        
        # íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„±
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì „ì—­ í•™ìŠµ (í•™ìŠµ ì‹œì‘ ì „ ì „ì²´ ë°ì´í„°ë¡œ í•œ ë²ˆë§Œ fit)
        self._fit_global_scaler()
        
        state_dim = self.env.get_state_dim()
        action_dim = 3  # 0: Hold, 1: Long, 2: Short
        
        # PPO ì—ì´ì „íŠ¸ ìƒì„± (Late Fusion êµ¬ì¡°: info_dim=13)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        self.agent = PPOAgent(state_dim, action_dim, hidden_dim=128, device=device, info_dim=13)
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if os.path.exists(config.AI_MODEL_PATH):
            try:
                self.agent.load_model(config.AI_MODEL_PATH)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {config.AI_MODEL_PATH}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘): {e}")
        else:
            logger.info("ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµ ì‹œì‘")
        
        # í•™ìŠµ ìƒíƒœ
        self.current_position = None
        self.entry_price = None
        self.entry_index = None  # [ìˆ˜ì •] entry_time ëŒ€ì‹  entry_index ì‚¬ìš© (ê³¼ê±° ë°ì´í„° í•™ìŠµìš©)
        self.prev_pnl = 0.0  # ì´ì „ ìŠ¤í…ì˜ ìˆ˜ìµë¥  (pnl_change ê³„ì‚°ìš©)
        self.episode_rewards = []
        self.total_steps = 0
        
        # ì‹¤ì‹œê°„ ì‹œê°í™” ì´ˆê¸°í™” (ì˜µì…˜)
        if enable_visualization:
            self.visualizer = LiveVisualizer(window_size=10)
        else:
            self.visualizer = None
    
    def _fit_global_scaler(self):
        """8ê°œ í•µì‹¬ ì‹œê³„ì—´ í”¼ì²˜ ê¸°ë°˜ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        try:
            logger.info("8ê°œ í•µì‹¬ í”¼ì²˜ ê¸°ë°˜ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            
            # ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
            if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
                logger.warning("ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            # ìƒ˜í”Œë§í•  ë°ì´í„° ìˆ˜ (ì „ì²´ ë°ì´í„°ê°€ ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§)
            total_candles = len(self.data_collector.eth_data)
            sample_size = min(50000, total_candles)  # ìµœëŒ€ 5ë§Œê°œ ìƒ˜í”Œ
            
            # ê· ë“± ê°„ê²© ìƒ˜í”Œë§ (ìµœì†Œ 20ê°œëŠ” í•„ìš”í•˜ë¯€ë¡œ 20ë¶€í„° ì‹œì‘)
            if total_candles > sample_size:
                indices = np.linspace(20, total_candles - 1, sample_size, dtype=int)
            else:
                indices = np.arange(20, total_candles)
            
            logger.info(f"ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ë°ì´í„°: {len(indices)}ê°œ ìƒ˜í”Œ (ì „ì²´: {total_candles}ê°œ)")
            
            # 8ê°œ í•µì‹¬ ì‹œê³„ì—´ í”¼ì²˜ ìˆ˜ì§‘
            window_size = 20
            all_seq_features = []
            
            for idx in indices:
                if idx < window_size:
                    continue
                
                try:
                    # ë§ˆì§€ë§‰ 20ë´‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    window = self.data_collector.eth_data.iloc[idx-window_size+1:idx+1]
                    if len(window) < window_size:
                        continue
                    
                    close = window['close'].values.astype(np.float32)
                    high = window['high'].values.astype(np.float32)
                    low = window['low'].values.astype(np.float32)
                    volume_raw = window['volume'].values.astype(np.float32)
                    trades_raw = window['trades'].values.astype(np.float32)
                    
                    # [ì¶”ê°€] VWAP ê³„ì‚° (Training í™˜ê²½ê³¼ ë™ì¼í•œ ë¡œì§)
                    tp = (high + low + close) / 3
                    vp = tp * volume_raw
                    cumulative_vp = np.cumsum(vp)
                    cumulative_vol = np.cumsum(volume_raw)
                    vwap = cumulative_vp / (cumulative_vol + 1e-8)
                    
                    # 8ê°œ ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±
                    # [ìµœì í™”] Volumeê³¼ Tradesì— ë¡œê·¸ ë³€í™˜ ì ìš© (ê±°ë˜ëŸ‰ í­ë°œ êµ¬ê°„ì˜ ê·¹ë‹¨ì  ì°¨ì´ ì™„í™”)
                    f1 = (window['open'].values - close) / (close + 1e-8)  # Open (close ëŒ€ë¹„)
                    f2 = (high - close) / (close + 1e-8)   # High (close ëŒ€ë¹„)
                    f3 = (low - close) / (close + 1e-8)    # Low (close ëŒ€ë¹„)
                    f4 = np.diff(np.log(close + 1e-8), prepend=np.log(close[0] + 1e-8))  # Log_Return
                    f5 = np.log1p(volume_raw)  # Volume (ë¡œê·¸ ë³€í™˜)
                    f6 = np.log1p(trades_raw)  # Trades (ë¡œê·¸ ë³€í™˜)
                    f7 = window['taker_buy_base'].values / (volume_raw + 1e-8)  # Taker_Ratio
                    f8 = (close - vwap) / (vwap + 1e-8)  # [NEW] VWAP ì´ê²©ë„
                    
                    # 8ê°œ í”¼ì²˜ ê²°í•©: (20, 8)
                    seq_features = np.column_stack([f1, f2, f3, f4, f5, f6, f7, f8])
                    all_seq_features.append(seq_features)
                    
                except Exception as e:
                    logger.debug(f"ì¸ë±ìŠ¤ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            if len(all_seq_features) == 0:
                logger.warning("í”¼ì²˜ ìˆ˜ì§‘ ì‹¤íŒ¨, ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ê±´ë„ˆëœ€")
                return
            
            # ì „ì²´ í”¼ì²˜ ê²°í•©: (N*20, 8)
            all_features_array = np.vstack(all_seq_features)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (8ê°œ ì°¨ì›)
            self.env.preprocessor.fit(all_features_array)
            self.env.scaler_fitted = True
            
            # [ì¶”ê°€] í•™ìŠµ ì™„ë£Œëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            self.env.preprocessor.save_scaler()
            
            logger.info(f"âœ… 8ê°œ í”¼ì²˜ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {len(all_features_array)}ê°œ ìƒ˜í”Œ (VWAP ì´ê²©ë„ í¬í•¨)")
            
        except Exception as e:
            logger.error(f"ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            logger.warning("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨, ì²« ê´€ì¸¡ ì‹œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
    def train_episode(self, episode_num, max_steps=1000, overfitting_test=False, fixed_start_index=1000):
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ
        
        Args:
            episode_num: í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸ (ì—”íŠ¸ë¡œí”¼ ìŠ¤ì¼€ì¤„ëŸ¬ìš©)
            max_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜
            overfitting_test: ê³¼ì í•© í…ŒìŠ¤íŠ¸ ëª¨ë“œ (Trueë©´ ê³ ì • ì¸ë±ìŠ¤ ì‚¬ìš©)
            fixed_start_index: ê³¼ì í•© í…ŒìŠ¤íŠ¸ ì‹œ ê³ ì • ì‹œì‘ ì¸ë±ìŠ¤
        """
        # [ì„¤ì •] ë³´ìœ  ì‹œê°„ ì •ê·œí™” ê¸°ì¤€ (8ì‹œê°„ = 480ë¶„/3ë¶„)
        # ì´ ì‹œê°„ì´ ì§€ë‚˜ë©´ 1.0ìœ¼ë¡œ ê³ ì •ë¨
        MAX_HOLDING_STEPS = 160.0
        
        episode_reward = 0.0
        steps = 0
        
        # ì €ì¥ëœ ë°ì´í„°ì—ì„œ ì¸ë±ìŠ¤ ë¦¬ì…‹
        if overfitting_test:
            # ê³¼ì í•© í…ŒìŠ¤íŠ¸: ê³ ì • ì‹œì‘ ì¸ë±ìŠ¤ ì‚¬ìš© (ê°™ì€ ë°ì´í„° ë°˜ë³µ í•™ìŠµ)
            self.data_collector.current_index = fixed_start_index
            logger.info(f"ğŸ§ª [ê³¼ì í•© í…ŒìŠ¤íŠ¸] ê³ ì • ì‹œì‘ ì¸ë±ìŠ¤: {fixed_start_index}, ìµœëŒ€ ìŠ¤í…: {max_steps}")
        else:
            # [ì‹¤ì „ í•™ìŠµ] ëœë¤ ìŠ¤íƒ€íŠ¸ í™œì„±í™”
            # ë§¤ë²ˆ ë‹¤ë¥¸ êµ¬ê°„ì„ í•™ìŠµí•˜ê²Œ í•˜ì—¬ ë‹¤ì–‘í•œ ì‹œì¥ ìƒí™©ì„ ê²½í—˜í•˜ë„ë¡ í•¨
            self.data_collector.reset_index(max_steps=max_steps, random_start=True)
            logger.debug(f"ğŸ² [ì‹¤ì „ í•™ìŠµ] ë¬´ì‘ìœ„ ì‹œì‘ ì¸ë±ìŠ¤: {self.data_collector.current_index}")
        
        # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì´ì „ ìˆ˜ìµë¥  ì´ˆê¸°í™”
        self.prev_pnl = 0.0
        self.current_position = None
        self.entry_price = None
        self.entry_index = None  # [ìˆ˜ì •] entry_time ëŒ€ì‹  entry_index ì‚¬ìš©
        
        # ì´ˆê¸° ë°ì´í„° í™•ì¸
        if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
            logger.error("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. model/collect_training_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ ìŠ¤í… ìˆ˜ ê³„ì‚°
        available_steps = len(self.data_collector.eth_data) - self.data_collector.current_index
        actual_steps = min(max_steps, available_steps)
        
        if actual_steps <= 0:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None
        
        logger.info(f"ì—í”¼ì†Œë“œ ì‹œì‘: ì´ {len(self.data_collector.eth_data)}ê°œ ìº”ë“¤ ì¤‘ {actual_steps}ê°œ ì‚¬ìš© (ì¸ë±ìŠ¤: {self.data_collector.current_index}ë¶€í„°)")
        
        for step in range(actual_steps):
            try:
                # 1. ì €ì¥ëœ ë°ì´í„°ì—ì„œ ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰ (ì¸ë±ìŠ¤ë§Œ ì¦ê°€)
                # get_candlesê°€ í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ
                # ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ì¦ê°€ì‹œì¼œì•¼ í•¨
                if self.data_collector.current_index >= len(self.data_collector.eth_data):
                    logger.info("ë°ì´í„° ëì— ë„ë‹¬, ì—í”¼ì†Œë“œ ì¢…ë£Œ")
                    break
                
                # ì¸ë±ìŠ¤ ì¦ê°€ (ë‹¤ìŒ ìº”ë“¤ë¡œ ì´ë™)
                self.data_collector.current_index += 1
                
                # 2. í¬ì§€ì…˜ ì •ë³´ ìˆ˜ì§‘ (Late Fusionìš©)
                pos_val = 1.0 if self.current_position == 'LONG' else (-1.0 if self.current_position == 'SHORT' else 0.0)
                
                # [ìˆ˜ì •] ì ˆëŒ€ ì‹œê°„ ê¸°ì¤€ ì •ê·œí™” (ì—í”¼ì†Œë“œ ê¸¸ì´ ì˜ì¡´ì„± ì œê±°)
                if self.entry_index is not None:
                    elapsed_steps = self.data_collector.current_index - self.entry_index
                    # 480ë¶„ì„ ë„˜ìœ¼ë©´ 1.0ìœ¼ë¡œ ê³ ì • (min ì‚¬ìš©)
                    hold_val = min(1.0, elapsed_steps / MAX_HOLDING_STEPS)
                else:
                    hold_val = 0.0
                    
                pnl_val = self.prev_pnl * 10  # PnL ìŠ¤ì¼€ì¼ ì¡°ì •
                pos_info = [pos_val, pnl_val, hold_val]
                
                # 3. ìƒíƒœ ê´€ì¸¡ (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í¬í•¨ + í¬ì§€ì…˜ ì •ë³´)
                # get_candlesê°€ í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ lookbackê°œë¥¼ ë°˜í™˜
                state = self.env.get_observation(position_info=pos_info)
                if state is None:
                    logger.warning("ìƒíƒœ ê´€ì¸¡ ì‹¤íŒ¨, ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰")
                    continue
                
                # [ë””ë²„ê¹…] ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œì˜ ì²« ìŠ¤í…ì—ì„œë§Œ ê°’ í™•ì¸
                if episode_num == 1 and steps < 5:
                    obs_seq, obs_info = state
                    logger.info(f"\nğŸ” [Step {steps}] ì…ë ¥ ë°ì´í„° ì ê²€:")
                    logger.info(f"   - ì‹œê³„ì—´ Shape: {obs_seq.shape}")
                    logger.info(f"   - ì‹œê³„ì—´(Min/Max/Mean): {obs_seq.min().item():.4f} ~ {obs_seq.max().item():.4f} / {obs_seq.mean().item():.4f}")
                    logger.info(f"   - ì •ë³´ Shape: {obs_info.shape}")
                    logger.info(f"   - ì •ë³´(Min/Max/Mean): {obs_info.min().item():.4f} ~ {obs_info.max().item():.4f} / {obs_info.mean().item():.4f}")
                    
                    # ë§Œì•½ ì—¬ê¸°ì„œ 10.0ì„ ë„˜ëŠ” ìˆ«ìê°€ ë³´ì´ë©´ ì •ê·œí™”ê°€ ê¹¨ì§„ ê²ƒì…ë‹ˆë‹¤.
                    if abs(obs_seq.max().item()) > 10.0 or abs(obs_seq.min().item()) > 10.0:
                        logger.warning("ğŸš¨ ê²½ê³ : ì‹œê³„ì—´ ì…ë ¥ê°’ì´ ë„ˆë¬´ í½ë‹ˆë‹¤! ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        logger.warning(f"   ê°’ ë²”ìœ„: {obs_seq.min().item():.4f} ~ {obs_seq.max().item():.4f}")
                    
                    if abs(obs_info.max().item()) > 10.0 or abs(obs_info.min().item()) > 10.0:
                        logger.warning("ğŸš¨ ê²½ê³ : ì •ë³´ ì…ë ¥ê°’ì´ ë„ˆë¬´ í½ë‹ˆë‹¤!")
                        logger.warning(f"   ê°’ ë²”ìœ„: {obs_info.min().item():.4f} ~ {obs_info.max().item():.4f}")
                    
                    # NaN/Inf ì²´í¬
                    if torch.isnan(obs_seq).any() or torch.isinf(obs_seq).any():
                        nan_count = torch.isnan(obs_seq).sum().item()
                        inf_count = torch.isinf(obs_seq).sum().item()
                        logger.error(f"ğŸš¨ ì‹œê³„ì—´ ë°ì´í„°ì— NaN({nan_count}) ë˜ëŠ” Inf({inf_count}) ë°œìƒ!")
                    
                    if torch.isnan(obs_info).any() or torch.isinf(obs_info).any():
                        nan_count = torch.isnan(obs_info).sum().item()
                        inf_count = torch.isinf(obs_info).sum().item()
                        logger.error(f"ğŸš¨ ì •ë³´ ë°ì´í„°ì— NaN({nan_count}) ë˜ëŠ” Inf({inf_count}) ë°œìƒ!")
                
                # 4. í–‰ë™ ì„ íƒ
                action, log_prob = self.agent.select_action(state)
                action_names = {0: 'HOLD', 1: 'LONG', 2: 'SHORT'}
                action_name = action_names[action]
                
                # 5. í˜„ì¬ ê°€ê²© í™•ì¸ (í˜„ì¬ ì¸ë±ìŠ¤ì˜ ìº”ë“¤)
                if self.data_collector.current_index > 0:
                    current_candle = self.data_collector.eth_data.iloc[self.data_collector.current_index - 1]
                    current_price = float(current_candle['close'])
                else:
                    continue
                
                # 6. ë³´ìƒ ê³„ì‚° ë° í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
                reward = 0.0
                trade_done = False
                current_pnl = 0.0
                pnl_change = 0.0
                
                if action == 1:  # LONG
                    if self.current_position != 'LONG':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'SHORT' and self.entry_price:
                            pnl = (self.entry_price - current_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl  # ì‹¤í˜„ ìˆ˜ìµì˜ ë³€í™”ëŸ‰
                            reward = self.env.calculate_reward(pnl, True, holding_time=0, pnl_change=pnl_change)
                            trade_done = True
                            logger.info(f"ğŸ’° ìˆ ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}")
                            self.prev_pnl = 0.0  # í¬ì§€ì…˜ ì²­ì‚° í›„ ì´ˆê¸°í™”
                        
                        # ë¡± ì§„ì…
                        self.current_position = 'LONG'
                        self.entry_price = current_price
                        self.entry_index = self.data_collector.current_index  # [ìˆ˜ì •] ì¸ë±ìŠ¤ ì €ì¥
                        self.prev_pnl = 0.0  # ìƒˆ í¬ì§€ì…˜ ì§„ì… ì‹œ ì´ˆê¸°í™”
                        logger.debug(f"ğŸ“ˆ ë¡± ì§„ì…: ${current_price:.2f} (ì¸ë±ìŠ¤: {self.entry_index})")
                
                elif action == 2:  # SHORT
                    if self.current_position != 'SHORT':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'LONG' and self.entry_price:
                            pnl = (current_price - self.entry_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl  # ì‹¤í˜„ ìˆ˜ìµì˜ ë³€í™”ëŸ‰
                            reward = self.env.calculate_reward(pnl, True, holding_time=0, pnl_change=pnl_change)
                            trade_done = True
                            logger.info(f"ğŸ’° ë¡± ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}")
                            self.prev_pnl = 0.0  # í¬ì§€ì…˜ ì²­ì‚° í›„ ì´ˆê¸°í™”
                        
                        # ìˆ ì§„ì…
                        self.current_position = 'SHORT'
                        self.entry_price = current_price
                        self.entry_index = self.data_collector.current_index  # [ìˆ˜ì •] ì¸ë±ìŠ¤ ì €ì¥
                        self.prev_pnl = 0.0  # ìƒˆ í¬ì§€ì…˜ ì§„ì… ì‹œ ì´ˆê¸°í™”
                        logger.debug(f"ğŸ“‰ ìˆ ì§„ì…: ${current_price:.2f} (ì¸ë±ìŠ¤: {self.entry_index})")
                
                else:  # HOLD
                    # ë³´ìœ  ì¤‘ì¸ í¬ì§€ì…˜ì˜ ìˆ˜ìµë¥  ê³„ì‚°
                    if self.current_position and self.entry_price:
                        if self.current_position == 'LONG':
                            current_pnl = (current_price - self.entry_price) / self.entry_price
                        else:  # SHORT
                            current_pnl = (self.entry_price - current_price) / self.entry_price
                        
                        # ì´ì „ ìŠ¤í… ëŒ€ë¹„ ìˆ˜ìµë¥ ì˜ ë³€í™”ëŸ‰ ê³„ì‚°
                        pnl_change = current_pnl - self.prev_pnl
                        
                        # [ìˆ˜ì •] ìº”ë“¤ ì¸ë±ìŠ¤ ì°¨ì´ë¡œ ë³´ìœ  ì‹œê°„ ê³„ì‚° (ê³¼ê±° ë°ì´í„° í•™ìŠµìš©)
                        holding_time = (self.data_collector.current_index - self.entry_index) if self.entry_index is not None else 0
                        reward = self.env.calculate_reward(current_pnl, False, holding_time, pnl_change)
                        
                        # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ í˜„ì¬ pnl ì €ì¥
                        self.prev_pnl = current_pnl
                
                # 7. íŠ¸ëœì§€ì…˜ ì €ì¥
                is_terminal = (step == actual_steps - 1)  # ì—í”¼ì†Œë“œ ë§ˆì§€ë§‰ ìŠ¤í… ì—¬ë¶€
                self.agent.store_transition(state, action, log_prob, reward, is_terminal)
                
                episode_reward += reward
                steps += 1
                self.total_steps += 1
                
                # 8. [ê³¼ì í•© í…ŒìŠ¤íŠ¸] ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ ì œê±°
                # ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì—í”¼ì†Œë“œ ì „ì²´ë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
                # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ í•œ ë²ˆì— ì—…ë°ì´íŠ¸í•˜ë„ë¡ ë³€ê²½ë¨
                
                # ì €ì¥ëœ ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰ë¨ (ëŒ€ê¸° ë¶ˆí•„ìš”)
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
                raise
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                time.sleep(5)
                continue
        
        # [ê³¼ì í•© í…ŒìŠ¤íŠ¸] ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ í•œ ë²ˆì— ì—…ë°ì´íŠ¸
        # ì´ë ‡ê²Œ í•˜ë©´ 1000ê°œì˜ ë°ì´í„°ë¥¼ í†µì§¸ë¡œ ë³´ê³  íŒë‹¨í•˜ë¯€ë¡œ í›¨ì”¬ ì•ˆì •ì ì…ë‹ˆë‹¤.
        if len(self.agent.memory) > 0:
            # next_stateëŠ” ì—í”¼ì†Œë“œ ëë‚¬ìœ¼ë¯€ë¡œ None
            self.agent.update(next_state=None, episode=episode_num)
            logger.info(f"ğŸš€ ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì „ì²´ ì—…ë°ì´íŠ¸ (ë°ì´í„°: {len(self.agent.memory)}ê°œ)")
        
        return episode_reward, steps
    
    def train(self, num_episodes=100, max_steps_per_episode=100, save_interval=10, overfitting_test=False, fixed_start_index=1000):
        """ëª¨ë¸ í•™ìŠµ
        
        Args:
            num_episodes: ì´ ì—í”¼ì†Œë“œ ìˆ˜
            max_steps_per_episode: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
            save_interval: ëª¨ë¸ ì €ì¥ ê°„ê²©
            overfitting_test: ê³¼ì í•© í…ŒìŠ¤íŠ¸ ëª¨ë“œ (Trueë©´ ê³ ì • ì¸ë±ìŠ¤ ì‚¬ìš©)
            fixed_start_index: ê³¼ì í•© í…ŒìŠ¤íŠ¸ ì‹œ ê³ ì • ì‹œì‘ ì¸ë±ìŠ¤
        """
        logger.info("=" * 60)
        if overfitting_test:
            logger.info("ğŸ§ª [ê³¼ì í•© í…ŒìŠ¤íŠ¸ ëª¨ë“œ] PPO ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            logger.info(f"âš ï¸  ê³ ì • ì‹œì‘ ì¸ë±ìŠ¤: {fixed_start_index}, ìµœëŒ€ ìŠ¤í…: {max_steps_per_episode}")
            logger.info("âš ï¸  ê°™ì€ ë°ì´í„°ë¥¼ ë°˜ë³µ í•™ìŠµí•˜ì—¬ ë³´ìƒì´ í­ë°œì ìœ¼ë¡œ ìƒìŠ¹í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
        else:
            logger.info("ğŸš€ PPO ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        logger.info(f"ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…: {max_steps_per_episode}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ê°„ê²©: {save_interval} ì—í”¼ì†Œë“œ")
        logger.info("=" * 60)
        
        best_reward = float('-inf')
        
        for episode in range(1, num_episodes + 1):
            try:
                logger.info(f"\n{'=' * 60}")
                logger.info(f"ğŸ“š ì—í”¼ì†Œë“œ {episode}/{num_episodes}")
                logger.info(f"{'=' * 60}")
                
                # ì—í”¼ì†Œë“œ ì‹¤í–‰ (ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì „ë‹¬)
                result = self.train_episode(
                    episode_num=episode, 
                    max_steps=max_steps_per_episode,
                    overfitting_test=overfitting_test,
                    fixed_start_index=fixed_start_index
                )
                if result is None:
                    logger.warning("ì—í”¼ì†Œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰")
                    continue
                
                episode_reward, steps = result
                self.episode_rewards.append(episode_reward)
                
                # ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸ (ì‹œê°í™” í™œì„±í™” ì‹œì—ë§Œ)
                if self.visualizer is not None:
                    self.visualizer.update(episode_reward)
                
                # í†µê³„ ì¶œë ¥
                avg_reward = sum(self.episode_rewards[-10:]) / len(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else episode_reward
                logger.info(f"âœ… ì—í”¼ì†Œë“œ {episode} ì™„ë£Œ")
                logger.info(f"   ì´ ë³´ìƒ: {episode_reward:.4f}")
                logger.info(f"   ìŠ¤í… ìˆ˜: {steps}")
                logger.info(f"   ìµœê·¼ 10ê°œ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if episode_reward > best_reward:
                    best_reward = episode_reward
                    os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
                    self.agent.save_model(config.AI_MODEL_PATH)
                    logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: ë³´ìƒ {best_reward:.4f}")
                
                # ì£¼ê¸°ì  ì €ì¥
                elif episode % save_interval == 0:
                    os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
                    self.agent.save_model(config.AI_MODEL_PATH)
                    logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ (ì—í”¼ì†Œë“œ {episode})")
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨")
                break
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ {episode} ì‹¤íŒ¨: {e}", exc_info=True)
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
        self.agent.save_model(config.AI_MODEL_PATH)
        logger.info("=" * 60)
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ")
        logger.info(f"ì´ ìŠ¤í…: {self.total_steps}")
        logger.info(f"í‰ê·  ë³´ìƒ: {sum(self.episode_rewards) / len(self.episode_rewards) if self.episode_rewards else 0:.4f}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {config.AI_MODEL_PATH}")
        logger.info("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='PPO ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--episodes', type=int, default=100, help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--steps', type=int, default=1000, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 1000, í° ì¶”ì„¸ í•™ìŠµìš©)')
    parser.add_argument('--save-interval', type=int, default=10, help='ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ)')
    parser.add_argument('--no-visualize', action='store_true', help='ì‹œê°í™” ë¹„í™œì„±í™”')
    parser.add_argument('--overfitting-test', action='store_true', help='ê³¼ì í•© í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê³ ì • ì¸ë±ìŠ¤ë¡œ ê°™ì€ ë°ì´í„° ë°˜ë³µ í•™ìŠµ')
    parser.add_argument('--fixed-start-index', type=int, default=1000, help='ê³¼ì í•© í…ŒìŠ¤íŠ¸ ì‹œ ê³ ì • ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 1000)')
    
    args = parser.parse_args()
    
    try:
        trainer = PPOTrainer(enable_visualization=not args.no_visualize)
        trainer.train(
            num_episodes=args.episodes,
            max_steps_per_episode=args.steps,
            save_interval=args.save_interval,
            overfitting_test=args.overfitting_test,
            fixed_start_index=args.fixed_start_index
        )
    except KeyboardInterrupt:
        logger.info("í•™ìŠµ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
