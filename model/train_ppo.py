"""
PPO ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ë³„ë„ë¡œ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
"""
import logging
import os
import sys
import time
from datetime import datetime
from collections import deque
import numpy as np

# ì‹œê°í™” ëª¨ë“ˆ (ì„ íƒì )
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    plt = None

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€ (config, core, strategies ëª¨ë“ˆ ì ‘ê·¼ìš©)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core import DataCollector, BinanceClient
from core.indicators import Indicators
from strategies import (
    BTCEthCorrelationStrategy,
    VolatilitySqueezeStrategy,
    OrderblockFVGStrategy,
    HMAMomentumStrategy,
    MFIMomentumStrategy,
    BollingerMeanReversionStrategy,
    VWAPDeviationStrategy,
    RangeTopBottomStrategy,
    StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy
)

# AI ê°•í™”í•™ìŠµ ëª¨ë“ˆ
try:
    import torch
    from model.trading_env import TradingEnvironment
    from model.ppo_agent import PPOAgent
    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"âŒ AI ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("torchê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: pip install torch")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_ppo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# matplotlib ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë¡œê¹…
if not MATPLOTLIB_AVAILABLE:
    logger.warning("matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œê°í™” ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    logger.warning("ì„¤ì¹˜ ë°©ë²•: pip install matplotlib")


class LiveVisualizer:
    """í•™ìŠµ ë¦¬ì›Œë“œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„í™”í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, window_size=10):
        if not MATPLOTLIB_AVAILABLE:
            self.enabled = False
            return
        
        self.enabled = True
        plt.ion()  # ëŒ€í™”í˜• ëª¨ë“œ í™œì„±í™”
        self.fig, self.ax = plt.subplots(figsize=(10, 5))
        self.rewards = []
        self.moving_avg = []
        self.window_size = window_size
        
        self.ax.set_title("Real-time Training Performance")
        self.ax.set_xlabel("Episode")
        self.ax.set_ylabel("Total Reward")
        self.line1, = self.ax.plot([], [], label='Episode Reward', alpha=0.3, color='blue')
        self.line2, = self.ax.plot([], [], label=f'Moving Avg ({window_size})', color='red', linewidth=2)
        self.ax.legend()
        self.ax.grid(True)

    def update(self, reward):
        if not self.enabled:
            return
        
        self.rewards.append(reward)
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        if len(self.rewards) >= self.window_size:
            avg = np.mean(self.rewards[-self.window_size:])
        else:
            avg = np.mean(self.rewards)
        self.moving_avg.append(avg)
        
        # ë°ì´í„° ì—…ë°ì´íŠ¸
        x = np.arange(len(self.rewards))
        self.line1.set_data(x, self.rewards)
        self.line2.set_data(x, self.moving_avg)
        
        # í™”ë©´ ë²”ìœ„ ìë™ ì¡°ì ˆ
        self.ax.relim()
        self.ax.autoscale_view()
        
        plt.draw()
        plt.pause(0.01)  # ì§§ì€ íœ´ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ê°±ì‹  ë³´ì¥


class PPOTrainer:
    """PPO ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    def __init__(self, enable_visualization=False):
        """
        Args:
            enable_visualization: ì‹œê°í™” í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        # ì €ì¥ëœ ë°ì´í„° ì‚¬ìš© (í•™ìŠµìš©)
        self.data_collector = DataCollector(use_saved_data=True)
        self.client = BinanceClient()
        
        # ì „ëµ ì´ˆê¸°í™”
        self.breakout_strategies = []
        self.range_strategies = []
        
        # í­ë°œì¥ ì „ëµ
        if config.STRATEGIES.get('btc_eth_correlation', False):
            self.breakout_strategies.append(BTCEthCorrelationStrategy())
        if config.STRATEGIES.get('volatility_squeeze', False):
            self.breakout_strategies.append(VolatilitySqueezeStrategy())
        if config.STRATEGIES.get('orderblock_fvg', False):
            self.breakout_strategies.append(OrderblockFVGStrategy())
        if config.STRATEGIES.get('hma_momentum', False):
            self.breakout_strategies.append(HMAMomentumStrategy())
        if config.STRATEGIES.get('mfi_momentum', False):
            self.breakout_strategies.append(MFIMomentumStrategy())
        
        # íš¡ë³´ì¥ ì „ëµ
        if config.STRATEGIES.get('bollinger_mean_reversion', False):
            self.range_strategies.append(BollingerMeanReversionStrategy())
        if config.STRATEGIES.get('vwap_deviation', False):
            self.range_strategies.append(VWAPDeviationStrategy())
        if config.STRATEGIES.get('range_top_bottom', False):
            self.range_strategies.append(RangeTopBottomStrategy())
        if config.STRATEGIES.get('stoch_rsi_mean_reversion', False):
            self.range_strategies.append(StochRSIMeanReversionStrategy())
        if config.STRATEGIES.get('cmf_divergence', False):
            self.range_strategies.append(CMFDivergenceStrategy())
        
        self.strategies = self.breakout_strategies + self.range_strategies
        
        if len(self.strategies) == 0:
            raise ValueError("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. config.pyì—ì„œ ì „ëµì„ í™œì„±í™”í•˜ì„¸ìš”.")
        
        logger.info(f"ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.strategies)}ê°œ ì „ëµ")
        
        # íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„±
        self.env = TradingEnvironment(self.data_collector, self.strategies)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì „ì—­ í•™ìŠµ (í•™ìŠµ ì‹œì‘ ì „ ì „ì²´ ë°ì´í„°ë¡œ í•œ ë²ˆë§Œ fit)
        self._fit_global_scaler()
        
        state_dim = self.env.get_state_dim()
        action_dim = 3  # 0: Hold, 1: Long, 2: Short
        
        # PPO ì—ì´ì „íŠ¸ ìƒì„±
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ë””ë°”ì´ìŠ¤: {device}")
        self.agent = PPOAgent(state_dim, action_dim, hidden_dim=128, device=device)
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if os.path.exists(config.AI_MODEL_PATH):
            try:
                self.agent.load_model(config.AI_MODEL_PATH)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {config.AI_MODEL_PATH}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘): {e}")
        else:
            logger.info("ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµ ì‹œì‘")
        
        # í•™ìŠµ ìƒíƒœ
        self.current_position = None
        self.entry_price = None
        self.entry_time = None
        self.prev_pnl = 0.0  # ì´ì „ ìŠ¤í…ì˜ ìˆ˜ìµë¥  (pnl_change ê³„ì‚°ìš©)
        self.episode_rewards = []
        self.total_steps = 0
        
        # ì‹¤ì‹œê°„ ì‹œê°í™” ì´ˆê¸°í™” (ì˜µì…˜)
        if enable_visualization:
            self.visualizer = LiveVisualizer(window_size=10)
        else:
            self.visualizer = None
    
    def _fit_global_scaler(self):
        """ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        try:
            logger.info("ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            
            # ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
            if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
                logger.warning("ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            # ìƒ˜í”Œë§í•  ë°ì´í„° ìˆ˜ (ì „ì²´ ë°ì´í„°ê°€ ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§)
            total_candles = len(self.data_collector.eth_data)
            sample_size = min(50000, total_candles)  # ìµœëŒ€ 5ë§Œê°œ ìƒ˜í”Œ
            
            # ëœë¤ ìƒ˜í”Œë§ ë˜ëŠ” ê· ë“± ê°„ê²© ìƒ˜í”Œë§
            if total_candles > sample_size:
                indices = np.linspace(0, total_candles - 1, sample_size, dtype=int)
            else:
                indices = np.arange(total_candles)
            
            logger.info(f"ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ë°ì´í„°: {len(indices)}ê°œ ìƒ˜í”Œ (ì „ì²´: {total_candles}ê°œ)")
            
            # í”¼ì²˜ ìˆ˜ì§‘
            window_size = 20
            lookback = self.env.lookback
            all_features = []
            
            for idx in indices:
                if idx < lookback:
                    continue
                
                try:
                    # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    candles = self.data_collector.eth_data.iloc[idx-lookback+1:idx+1]
                    if len(candles) < lookback:
                        continue
                    
                    # ì›ì‹œ ë°ì´í„° ì¶”ì¶œ
                    close_prices = candles['close'].values.astype(np.float32)
                    volumes = candles['volume'].values.astype(np.float32)
                    
                    # ìœˆë„ìš° ë°ì´í„°
                    prices_window = close_prices[-window_size:]
                    volumes_window = volumes[-window_size:]
                    
                    # ì „ëµ ì ìˆ˜ (0ìœ¼ë¡œ ì´ˆê¸°í™”, ì‹¤ì œ ê°’ì€ ë‚˜ì¤‘ì— ê³„ì‚°)
                    strategy_scores = np.zeros(self.env.num_strategies, dtype=np.float32)
                    scores_tiled = np.tile(strategy_scores, (window_size, 1))
                    
                    # í”¼ì²˜ ê²°í•©
                    features = np.column_stack([
                        prices_window,
                        volumes_window,
                        scores_tiled
                    ])
                    
                    all_features.append(features)
                    
                except Exception as e:
                    logger.debug(f"ì¸ë±ìŠ¤ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            if len(all_features) == 0:
                logger.warning("í”¼ì²˜ ìˆ˜ì§‘ ì‹¤íŒ¨, ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ê±´ë„ˆëœ€")
                return
            
            # ì „ì²´ í”¼ì²˜ ê²°í•©
            all_features_array = np.vstack(all_features)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            self.env.preprocessor.fit(all_features_array)
            self.env.scaler_fitted = True
            
            logger.info(f"âœ… ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ: {len(all_features_array)}ê°œ ìƒ˜í”Œ")
            
        except Exception as e:
            logger.error(f"ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            logger.warning("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨, ì²« ê´€ì¸¡ ì‹œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
    def train_episode(self, max_steps=100):
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ"""
        episode_reward = 0.0
        steps = 0
        
        # ì €ì¥ëœ ë°ì´í„°ì—ì„œ ì¸ë±ìŠ¤ ë¦¬ì…‹ (ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘ - ë¬´ì‘ìœ„ ì‹œì‘ ì¸ë±ìŠ¤)
        self.data_collector.reset_index(max_steps=max_steps, random_start=True)
        
        # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì´ì „ ìˆ˜ìµë¥  ì´ˆê¸°í™”
        self.prev_pnl = 0.0
        self.current_position = None
        self.entry_price = None
        self.entry_time = None
        
        # ì´ˆê¸° ë°ì´í„° í™•ì¸
        if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
            logger.error("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. model/collect_training_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ ìŠ¤í… ìˆ˜ ê³„ì‚°
        available_steps = len(self.data_collector.eth_data) - self.data_collector.current_index
        actual_steps = min(max_steps, available_steps)
        
        if actual_steps <= 0:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None
        
        logger.info(f"ì—í”¼ì†Œë“œ ì‹œì‘: ì´ {len(self.data_collector.eth_data)}ê°œ ìº”ë“¤ ì¤‘ {actual_steps}ê°œ ì‚¬ìš© (ì¸ë±ìŠ¤: {self.data_collector.current_index}ë¶€í„°)")
        
        for step in range(actual_steps):
            try:
                # 1. ì €ì¥ëœ ë°ì´í„°ì—ì„œ ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰ (ì¸ë±ìŠ¤ë§Œ ì¦ê°€)
                # get_candlesê°€ í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ
                # ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ì¦ê°€ì‹œì¼œì•¼ í•¨
                if self.data_collector.current_index >= len(self.data_collector.eth_data):
                    logger.info("ë°ì´í„° ëì— ë„ë‹¬, ì—í”¼ì†Œë“œ ì¢…ë£Œ")
                    break
                
                # ì¸ë±ìŠ¤ ì¦ê°€ (ë‹¤ìŒ ìº”ë“¤ë¡œ ì´ë™)
                self.data_collector.current_index += 1
                
                # 2. ìƒíƒœ ê´€ì¸¡ (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í¬í•¨)
                # get_candlesê°€ í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ lookbackê°œë¥¼ ë°˜í™˜
                state = self.env.get_observation()
                if state is None:
                    logger.warning("ìƒíƒœ ê´€ì¸¡ ì‹¤íŒ¨, ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰")
                    continue
                
                # 2. í–‰ë™ ì„ íƒ
                action, log_prob = self.agent.select_action(state)
                action_names = {0: 'HOLD', 1: 'LONG', 2: 'SHORT'}
                action_name = action_names[action]
                
                # 3. í˜„ì¬ ê°€ê²© í™•ì¸ (í˜„ì¬ ì¸ë±ìŠ¤ì˜ ìº”ë“¤)
                if self.data_collector.current_index > 0:
                    current_candle = self.data_collector.eth_data.iloc[self.data_collector.current_index - 1]
                    current_price = float(current_candle['close'])
                else:
                    continue
                
                # 4. ë³´ìƒ ê³„ì‚° ë° í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
                reward = 0.0
                trade_done = False
                current_pnl = 0.0
                pnl_change = 0.0
                
                if action == 1:  # LONG
                    if self.current_position != 'LONG':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'SHORT' and self.entry_price:
                            pnl = (self.entry_price - current_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl  # ì‹¤í˜„ ìˆ˜ìµì˜ ë³€í™”ëŸ‰
                            reward = self.env.calculate_reward(pnl, True, holding_time=0, pnl_change=pnl_change)
                            trade_done = True
                            logger.info(f"ğŸ’° ìˆ ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}")
                            self.prev_pnl = 0.0  # í¬ì§€ì…˜ ì²­ì‚° í›„ ì´ˆê¸°í™”
                        
                        # ë¡± ì§„ì…
                        self.current_position = 'LONG'
                        self.entry_price = current_price
                        self.entry_time = datetime.now()
                        self.prev_pnl = 0.0  # ìƒˆ í¬ì§€ì…˜ ì§„ì… ì‹œ ì´ˆê¸°í™”
                        logger.debug(f"ğŸ“ˆ ë¡± ì§„ì…: ${current_price:.2f}")
                
                elif action == 2:  # SHORT
                    if self.current_position != 'SHORT':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'LONG' and self.entry_price:
                            pnl = (current_price - self.entry_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl  # ì‹¤í˜„ ìˆ˜ìµì˜ ë³€í™”ëŸ‰
                            reward = self.env.calculate_reward(pnl, True, holding_time=0, pnl_change=pnl_change)
                            trade_done = True
                            logger.info(f"ğŸ’° ë¡± ì²­ì‚°: ìˆ˜ìµë¥  {pnl:.2%}, ë³´ìƒ: {reward:.4f}")
                            self.prev_pnl = 0.0  # í¬ì§€ì…˜ ì²­ì‚° í›„ ì´ˆê¸°í™”
                        
                        # ìˆ ì§„ì…
                        self.current_position = 'SHORT'
                        self.entry_price = current_price
                        self.entry_time = datetime.now()
                        self.prev_pnl = 0.0  # ìƒˆ í¬ì§€ì…˜ ì§„ì… ì‹œ ì´ˆê¸°í™”
                        logger.debug(f"ğŸ“‰ ìˆ ì§„ì…: ${current_price:.2f}")
                
                else:  # HOLD
                    # ë³´ìœ  ì¤‘ì¸ í¬ì§€ì…˜ì˜ ìˆ˜ìµë¥  ê³„ì‚°
                    if self.current_position and self.entry_price:
                        if self.current_position == 'LONG':
                            current_pnl = (current_price - self.entry_price) / self.entry_price
                        else:  # SHORT
                            current_pnl = (self.entry_price - current_price) / self.entry_price
                        
                        # ì´ì „ ìŠ¤í… ëŒ€ë¹„ ìˆ˜ìµë¥ ì˜ ë³€í™”ëŸ‰ ê³„ì‚°
                        pnl_change = current_pnl - self.prev_pnl
                        
                        holding_time = (datetime.now() - self.entry_time).total_seconds() / 60 if self.entry_time else 0
                        reward = self.env.calculate_reward(current_pnl, False, holding_time, pnl_change)
                        
                        # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ í˜„ì¬ pnl ì €ì¥
                        self.prev_pnl = current_pnl
                
                # 5. íŠ¸ëœì§€ì…˜ ì €ì¥
                is_terminal = (step == actual_steps - 1)  # ì—í”¼ì†Œë“œ ë§ˆì§€ë§‰ ìŠ¤í… ì—¬ë¶€
                self.agent.store_transition(state, action, log_prob, reward, is_terminal)
                
                episode_reward += reward
                steps += 1
                self.total_steps += 1
                
                # 6. ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ (256ê°œ íŠ¸ëœì§€ì…˜ë§ˆë‹¤ - ë²„í¼ ì—…ë°ì´íŠ¸ ë°©ì‹)
                if len(self.agent.memory) >= 256:
                    # ë‹¤ìŒ ìŠ¤í…ì—ì„œ ì‚¬ìš©í•  ìƒíƒœë¥¼ ë¯¸ë¦¬ ê´€ì¸¡í•˜ì—¬ Bootstrap ê°’ìœ¼ë¡œ ì‚¬ìš©
                    # (ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¦ê°€í–ˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ê´€ì¸¡ì´ ê³§ ë‹¤ìŒ ìƒíƒœ)
                    next_obs = None
                    if not is_terminal:
                        # ë‹¤ìŒ ì¸ë±ìŠ¤ë¡œ ì´ë™í•˜ì—¬ ë‹¤ìŒ ìƒíƒœ ê´€ì¸¡
                        if self.data_collector.current_index < len(self.data_collector.eth_data):
                            # ì„ì‹œë¡œ ì¸ë±ìŠ¤ ì¦ê°€ (ë‹¤ìŒ ìƒíƒœ ê´€ì¸¡ìš©)
                            temp_index = self.data_collector.current_index
                            self.data_collector.current_index += 1
                            next_obs = self.env.get_observation()
                            # ì¸ë±ìŠ¤ ë³µì› (ì‹¤ì œ ì¦ê°€ëŠ” ë‹¤ìŒ ë£¨í”„ì—ì„œ)
                            self.data_collector.current_index = temp_index
                    
                    # Bootstrap ì—…ë°ì´íŠ¸ ìˆ˜í–‰
                    self.agent.update(next_state=next_obs if next_obs is not None else None)
                    logger.info(f"ğŸš€ Bootstrap ì—…ë°ì´íŠ¸ ì™„ë£Œ (Step: {step}, Memory: {len(self.agent.memory)}, Next Value: {'Yes' if next_obs is not None else 'No'})")
                
                # ì €ì¥ëœ ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒ ìº”ë“¤ë¡œ ì§„í–‰ë¨ (ëŒ€ê¸° ë¶ˆí•„ìš”)
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
                raise
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                time.sleep(5)
                continue
        
        return episode_reward, steps
    
    def train(self, num_episodes=100, max_steps_per_episode=100, save_interval=10):
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info("=" * 60)
        logger.info("ğŸš€ PPO ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        logger.info(f"ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…: {max_steps_per_episode}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ê°„ê²©: {save_interval} ì—í”¼ì†Œë“œ")
        logger.info("=" * 60)
        
        best_reward = float('-inf')
        
        for episode in range(1, num_episodes + 1):
            try:
                logger.info(f"\n{'=' * 60}")
                logger.info(f"ğŸ“š ì—í”¼ì†Œë“œ {episode}/{num_episodes}")
                logger.info(f"{'=' * 60}")
                
                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                result = self.train_episode(max_steps=max_steps_per_episode)
                if result is None:
                    logger.warning("ì—í”¼ì†Œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰")
                    continue
                
                episode_reward, steps = result
                self.episode_rewards.append(episode_reward)
                
                # ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸ (ì‹œê°í™” í™œì„±í™” ì‹œì—ë§Œ)
                if self.visualizer is not None:
                    self.visualizer.update(episode_reward)
                
                # í†µê³„ ì¶œë ¥
                avg_reward = sum(self.episode_rewards[-10:]) / len(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else episode_reward
                logger.info(f"âœ… ì—í”¼ì†Œë“œ {episode} ì™„ë£Œ")
                logger.info(f"   ì´ ë³´ìƒ: {episode_reward:.4f}")
                logger.info(f"   ìŠ¤í… ìˆ˜: {steps}")
                logger.info(f"   ìµœê·¼ 10ê°œ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if episode_reward > best_reward:
                    best_reward = episode_reward
                    os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
                    self.agent.save_model(config.AI_MODEL_PATH)
                    logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: ë³´ìƒ {best_reward:.4f}")
                
                # ì£¼ê¸°ì  ì €ì¥
                elif episode % save_interval == 0:
                    os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
                    self.agent.save_model(config.AI_MODEL_PATH)
                    logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ (ì—í”¼ì†Œë“œ {episode})")
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨")
                break
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ {episode} ì‹¤íŒ¨: {e}", exc_info=True)
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        os.makedirs(os.path.dirname(config.AI_MODEL_PATH), exist_ok=True)
        self.agent.save_model(config.AI_MODEL_PATH)
        logger.info("=" * 60)
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ")
        logger.info(f"ì´ ìŠ¤í…: {self.total_steps}")
        logger.info(f"í‰ê·  ë³´ìƒ: {sum(self.episode_rewards) / len(self.episode_rewards) if self.episode_rewards else 0:.4f}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {config.AI_MODEL_PATH}")
        logger.info("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='PPO ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--episodes', type=int, default=100, help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--steps', type=int, default=1000, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 1000, í° ì¶”ì„¸ í•™ìŠµìš©)')
    parser.add_argument('--save-interval', type=int, default=10, help='ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ)')
    parser.add_argument('--no-visualize', action='store_true', help='ì‹œê°í™” ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    
    try:
        trainer = PPOTrainer(enable_visualization=not args.no_visualize)
        trainer.train(
            num_episodes=args.episodes,
            max_steps_per_episode=args.steps,
            save_interval=args.save_interval
        )
    except KeyboardInterrupt:
        logger.info("í•™ìŠµ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
