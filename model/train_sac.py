"""
SAC (Soft Actor-Critic) ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Final)
- Best/Last ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¶„ë¦¬ ì €ì¥
- ì‹¤ì‹œê°„ ë¦¬ì›Œë“œ ê·¸ë˜í”„ (Live Plotting)
- ì—°ì†í˜• í–‰ë™ ê³µê°„ (Action Dead-zone ì ìš©)
"""
import logging
import os
import sys
import time
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from datetime import datetime
from collections import deque

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core import DataCollector, BinanceClient
from strategies import (
    BTCEthCorrelationStrategy, VolatilitySqueezeStrategy, OrderblockFVGStrategy,
    HMAMomentumStrategy, MFIMomentumStrategy, BollingerMeanReversionStrategy,
    VWAPDeviationStrategy, RangeTopBottomStrategy, StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy,
    CCIReversalStrategy, WilliamsRStrategy
)

from model.trading_env import TradingEnvironment
from model.sac_agent import SACAgent
from model.feature_engineering import FeatureEngineer
from model.mtf_processor import MTFProcessor

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(message)s',
    handlers=[
        logging.FileHandler('logs/train_sac.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger('model.feature_engineering').setLevel(logging.WARNING)
logging.getLogger('model.mtf_processor').setLevel(logging.WARNING)


class SACTrainer:
    """SAC ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, enable_visualization=True):
        self.enable_visualization = enable_visualization
        # 1. ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.data_collector = DataCollector(use_saved_data=True)
        if not self.data_collector.load_saved_data():
            raise ValueError("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        
        # 2. ì „ëµ ì´ˆê¸°í™”
        self.breakout_strategies = []
        self.range_strategies = []
        
        # í­ë°œì¥ ì „ëµ
        if config.STRATEGIES.get('btc_eth_correlation', False):
            self.breakout_strategies.append(BTCEthCorrelationStrategy())
        if config.STRATEGIES.get('volatility_squeeze', False):
            self.breakout_strategies.append(VolatilitySqueezeStrategy())
        if config.STRATEGIES.get('orderblock_fvg', False):
            self.breakout_strategies.append(OrderblockFVGStrategy())
        if config.STRATEGIES.get('hma_momentum', False):
            self.breakout_strategies.append(HMAMomentumStrategy())
        if config.STRATEGIES.get('mfi_momentum', False):
            self.breakout_strategies.append(MFIMomentumStrategy())
        if config.STRATEGIES.get('cci_reversal', False):
            self.breakout_strategies.append(CCIReversalStrategy())
        
        # íš¡ë³´ì¥ ì „ëµ
        if config.STRATEGIES.get('bollinger_mean_reversion', False):
            self.range_strategies.append(BollingerMeanReversionStrategy())
        if config.STRATEGIES.get('vwap_deviation', False):
            self.range_strategies.append(VWAPDeviationStrategy())
        if config.STRATEGIES.get('range_top_bottom', False):
            self.range_strategies.append(RangeTopBottomStrategy())
        if config.STRATEGIES.get('stoch_rsi_mean_reversion', False):
            self.range_strategies.append(StochRSIMeanReversionStrategy())
        if config.STRATEGIES.get('cmf_divergence', False):
            self.range_strategies.append(CMFDivergenceStrategy())
        if config.STRATEGIES.get('williams_r', False):
            self.range_strategies.append(WilliamsRStrategy())
        
        self.strategies = self.breakout_strategies + self.range_strategies
        
        if len(self.strategies) == 0:
            raise ValueError("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. config.pyì—ì„œ ì „ëµì„ í™œì„±í™”í•˜ì„¸ìš”.")
        
        logger.info(f"âœ… ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {len(self.strategies)}ê°œ")
        
        # 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (CSV íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ê³„ì‚°)
        self._load_or_create_features()
        
        # [í•µì‹¬] ì „ëµ ì‹ í˜¸ ë¯¸ë¦¬ ê³„ì‚° (Pre-calculation)
        # CSVì— ì „ëµ ì»¬ëŸ¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        self.precalculate_strategies()
        
        # 4. í™˜ê²½ ìƒì„± (config.LOOKBACK ì‚¬ìš©)
        self.env = TradingEnvironment(self.data_collector, self.strategies, lookback=config.LOOKBACK)
        
        # 5. ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        self._fit_global_scaler()
        
        # 6. Agent ìƒì„±
        state_dim = self.env.get_state_dim()  # 29
        action_dim = 1  # ì—°ì†í˜•: ë§¤ìˆ˜/ë§¤ë„ ê°•ë„ (-1 ~ 1)
        info_dim = len(self.strategies) + 3  # ì „ëµ ì ìˆ˜ + í¬ì§€ì…˜ ì •ë³´
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
        
        # configì—ì„œ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        self.agent = SACAgent(
            state_dim, 
            action_dim, 
            info_dim=info_dim, 
            hidden_dim=config.NETWORK_HIDDEN_DIM, 
            device=device
        )
        
        # ëª¨ë¸ ë¡œë“œ (Last ëª¨ë¸ ìš°ì„  ë¡œë“œ)
        base_path = config.AI_MODEL_PATH.replace('ppo_model', 'sac_model').replace('.pth', '')
        last_model_path = f"{base_path}_last.pth"
        
        if os.path.exists(last_model_path):
            try:
                self.agent.load_model(last_model_path)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸(Last) ë¡œë“œ: {last_model_path}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘): {e}")
        else:
            logger.info("ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµ ì‹œì‘")
        
        # í•™ìŠµ ìƒíƒœ ë³€ìˆ˜
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        self.prev_pnl = 0.0
        self.episode_rewards = []
        self.avg_rewards = []
        self.total_steps = 0

        # ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì„¤ì •
        if self.enable_visualization:
            try:
                plt.ion()
                self.fig, self.ax = plt.subplots(figsize=(10, 5))
                self.ax.set_title('SAC Real-time Training')
                self.ax.set_xlabel('Episode')
                self.ax.set_ylabel('Reward')
                self.ax.grid(True, alpha=0.3)
                self.line1, = self.ax.plot([], [], label='Reward', alpha=0.3, color='gray')
                self.line2, = self.ax.plot([], [], label='Avg (10)', color='red', linewidth=2)
                self.ax.legend()
            except Exception as e:
                logger.warning(f"ê·¸ë˜í”„ ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                self.enable_visualization = False
    
    def _load_or_create_features(self):
        """
        í”¼ì²˜ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
        """
        feature_file_path = 'data/training_features.csv'
        
        if os.path.exists(feature_file_path):
            logger.info("ğŸ“‚ í”¼ì²˜ íŒŒì¼ ë¡œë“œ ì¤‘...")
            try:
                df = pd.read_csv(feature_file_path, index_col=0, parse_dates=True)
                self.data_collector.eth_data = df
                logger.info(f"âœ… í”¼ì²˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
                return
            except Exception as e:
                logger.warning(f"í”¼ì²˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ì¬ìƒì„±í•©ë‹ˆë‹¤: {e}")
        
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¬ìƒì„±
        logger.info("ğŸ“Š í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ ì¤‘...")
        try:
            # ETH ë°ì´í„° ì¤€ë¹„
            eth_data = self.data_collector.eth_data.copy()
            
            # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸ ë° ë³€í™˜
            if not isinstance(eth_data.index, pd.DatetimeIndex):
                if 'timestamp' in eth_data.columns:
                    eth_data.index = pd.to_datetime(eth_data['timestamp'], unit='ms')
                else:
                    eth_data.index = pd.date_range(end=pd.Timestamp.now(), periods=len(eth_data), freq='3min')
            
            # BTC ë°ì´í„° ì¤€ë¹„
            btc_data = None
            if hasattr(self.data_collector, 'btc_data') and self.data_collector.btc_data is not None:
                btc_data = self.data_collector.btc_data.copy()
                if not isinstance(btc_data.index, pd.DatetimeIndex):
                    if 'timestamp' in btc_data.columns:
                        btc_data.index = pd.to_datetime(btc_data['timestamp'], unit='ms')
                    else:
                        btc_data.index = pd.date_range(end=pd.Timestamp.now(), periods=len(btc_data), freq='3min')
                
                # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
                common_index = eth_data.index.intersection(btc_data.index)
                if len(common_index) > 0:
                    eth_data = eth_data.loc[common_index]
                    btc_data = btc_data.loc[common_index]
            
            # (1) ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            feature_engineer = FeatureEngineer(eth_data, btc_data)
            df = feature_engineer.generate_features()
            
            if df is None:
                raise ValueError("í”¼ì²˜ ìƒì„± ì‹¤íŒ¨")
            
            # (2) ë©€í‹° íƒ€ì„í”„ë ˆì„ ì§€í‘œ ì¶”ê°€
            mtf_processor = MTFProcessor(df)
            df = mtf_processor.add_mtf_features()
            
            # ë°ì´í„° êµì²´
            self.data_collector.eth_data = df
            if btc_data is not None:
                self.data_collector.btc_data = btc_data
            
            # CSV ì €ì¥
            os.makedirs('data', exist_ok=True)
            df.to_csv(feature_file_path)
            logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ ë° ì €ì¥: {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨: {e}", exc_info=True)
            raise
    
    def precalculate_strategies(self):
        """
        ì „ëµ ì‹ í˜¸ ì‚¬ì „ ê³„ì‚° (ìºì‹± ê¸°ëŠ¥ ì¶”ê°€)
        - íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ (ë¹ ë¦„) âš¡
        - ì—†ìœ¼ë©´ ê³„ì‚° í›„ ì €ì¥ (ëŠë¦¼) ğŸ¢ -> ğŸ’¾
        """
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        cache_path = 'data/cached_strategies.csv'
        
        # 1. ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(cache_path):
            logger.info(f"âš¡ ìºì‹œëœ ì „ëµ ë°ì´í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤! ë¡œë“œ ì¤‘... ({cache_path})")
            try:
                # ì €ì¥ëœ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
                cached_df = pd.read_csv(cache_path, index_col=0, parse_dates=True)
                self.data_collector.eth_data = cached_df
                logger.info("âœ… ì „ëµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ê³„ì‚° ìƒëµ)")
                return
            except Exception as e:
                logger.warning(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ ê³„ì‚°í•©ë‹ˆë‹¤): {e}")

        # 2. ìºì‹œê°€ ì—†ìœ¼ë©´ ê³„ì‚° ì‹œì‘ (ê¸°ì¡´ ë¡œì§)
        logger.info("ğŸ§  ì „ëµ ì‹ í˜¸ ì‚¬ì „ ê³„ì‚° ì¤‘ (Pre-calculation)...")
        df = self.data_collector.eth_data
        
        # ì „ëµë³„ ì»¬ëŸ¼ ì´ˆê¸°í™”
        for i in range(len(self.strategies)):
            df[f'strategy_{i}'] = 0.0
            
        total_len = len(df)
        start_idx = config.LOOKBACK + 50
        
        # ì§„í–‰ë¥  í‘œì‹œ (tqdm)
        try:
            from tqdm import tqdm
            iterator = tqdm(range(start_idx, total_len), desc="Strategy Calc")
        except ImportError:
            logger.warning("tqdmì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            iterator = range(start_idx, total_len)
        
        for i in iterator:
            self.data_collector.current_index = i
            
            for s_idx, strategy in enumerate(self.strategies):
                try:
                    result = strategy.analyze(self.data_collector)
                    score = 0.0
                    if result:
                        conf = float(result.get('confidence', 0.0))
                        signal = result.get('signal', 'NEUTRAL')
                        
                        if signal == 'LONG': 
                            score = conf
                        elif signal == 'SHORT': 
                            score = -conf
                    
                    df.iat[i, df.columns.get_loc(f'strategy_{s_idx}')] = score
                    
                except Exception:
                    continue
        
        # 3. ê³„ì‚° ëë‚œ í›„ íŒŒì¼ë¡œ ì €ì¥ (ì¤‘ìš”!)
        logger.info(f"ğŸ’¾ ê³„ì‚°ëœ ì „ëµ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤: {cache_path}")
        os.makedirs('data', exist_ok=True)
        df.to_csv(cache_path)
        logger.info("âœ… ì „ëµ ì‹ í˜¸ ê³„ì‚° ë° ì €ì¥ ì™„ë£Œ!")
    
    def _fit_global_scaler(self):
        """29ê°œ ê³ ê¸‰ í”¼ì²˜ ê¸°ë°˜ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (ìµœì í™” ë²„ì „)"""
        try:
            logger.info("ğŸš€ 29ê°œ ê³ ê¸‰ í”¼ì²˜ ê¸°ë°˜ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            
            if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
                logger.warning("ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            # ì‚¬ìš©í•  29ê°œ ì»¬ëŸ¼ ì •ì˜
            target_cols = [
                'log_return', 'roll_return_6', 'atr_ratio', 'bb_width', 'bb_pos', 
                'rsi', 'macd_hist', 'hma_ratio', 'cci', 
                'rvol', 'taker_ratio', 'cvd_change', 'mfi', 'cmf', 'vwap_dist',
                'wick_upper', 'wick_lower', 'range_pos', 'swing_break', 'chop',
                'btc_return', 'btc_rsi', 'btc_corr', 'btc_vol', 'eth_btc_ratio',
                'rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'
            ]
            
            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            missing_cols = [c for c in target_cols if c not in self.data_collector.eth_data.columns]
            if missing_cols:
                logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆì–´ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤: {missing_cols}")
                for c in missing_cols:
                    self.data_collector.eth_data[c] = 0.0
            
            # ìƒ˜í”Œë§
            total_candles = len(self.data_collector.eth_data)
            min_required = self.env.lookback + 100
            sample_size = min(50000, total_candles - min_required)
            
            if total_candles > min_required + sample_size:
                indices = np.linspace(min_required, total_candles - 1, sample_size, dtype=int)
            else:
                indices = np.arange(min_required, total_candles)
            
            logger.info(f"ë°ì´í„° ì¶”ì¶œ ì¤‘... ({len(indices)}ê°œ ìƒ˜í”Œ)")
            
            # ë°ì´í„° ìˆ˜ì§‘
            all_seq_features = []
            for idx in indices:
                if idx < self.env.lookback:
                    continue
                recent_df = self.data_collector.eth_data[target_cols].iloc[idx-self.env.lookback+1:idx+1]
                if len(recent_df) == self.env.lookback:
                    seq_features = recent_df.values.astype(np.float32)
                    all_seq_features.append(seq_features)
            
            if len(all_seq_features) == 0:
                logger.warning("í”¼ì²˜ ìˆ˜ì§‘ ì‹¤íŒ¨, ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ê±´ë„ˆëœ€")
                return
            
            all_features_array = np.vstack(all_seq_features)
            
            # NaN ì²˜ë¦¬
            if np.isnan(all_features_array).any():
                all_features_array = np.nan_to_num(all_features_array)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            self.env.preprocessor.fit(all_features_array)
            self.env.scaler_fitted = True
            
            logger.info(f"âœ… ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ: {len(all_features_array)}ê°œ ìƒ˜í”Œ, Feature Dim: {all_features_array.shape[1]}")
            
        except Exception as e:
            logger.error(f"ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            logger.warning("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨, í•™ìŠµ ë„ì¤‘ online-fittingìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    
    def live_plot(self):
        """ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸"""
        if not self.enable_visualization:
            return
        try:
            x = range(len(self.episode_rewards))
            self.line1.set_data(x, self.episode_rewards)
            self.line2.set_data(x, self.avg_rewards)
            self.ax.relim()
            self.ax.autoscale_view()
            self.fig.canvas.draw()
            self.fig.canvas.flush_events()
            plt.pause(0.01)
        except Exception:
            pass  # ê·¸ë˜í”„ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  í•™ìŠµ ê³„ì†
    
    def interpret_action(self, action_value):
        """
        [ë¬¸ì œ í•´ê²° 3 & 4] Continuous Action í•´ì„ ê°œì„ 
        -0.3 ~ 0.3 êµ¬ê°„: Neutral (Exit/Hold) -> ë¬´í•œ ì¡´ë²„ ë°©ì§€
        
        Args:
            action_value: float, -1 ~ 1 ì‚¬ì´ì˜ ì—°ì†ê°’
        Returns:
            int: 0=NEUTRAL(ì²­ì‚°/ê´€ë§), 1=LONG, 2=SHORT
        """
        threshold = 0.3
        
        if action_value > threshold:
            return 1  # LONG ì§„ì… (ê°•ë„: action_value)
        elif action_value < -threshold:
            return 2  # SHORT ì§„ì… (ê°•ë„: abs(action_value))
        else:
            return 0  # NEUTRAL (ì²­ì‚° ë˜ëŠ” ê´€ë§)
    
    def train_episode(self, episode_num, max_steps=None):
        """
        í•œ ì—í”¼ì†Œë“œ í•™ìŠµ (Fixed Architecture)
        - Action Dead-zone ì ìš© (Exit Logic ê°œì„ )
        - Next State Indexing ì˜¤ë¥˜ ìˆ˜ì •
        """
        if max_steps is None:
            max_steps = config.TRAIN_MAX_STEPS_PER_EPISODE
        
        # í•™ìŠµ ë°ì´í„° ë²”ìœ„ ì„¤ì •
        train_size = int(len(self.data_collector.eth_data) * 0.8)
        self.train_end_idx = train_size
        
        # ë¬´ì‘ìœ„ ì‹œì‘ ì¸ë±ìŠ¤
        start_min = config.LOOKBACK + 100
        start_max = self.train_end_idx - max_steps - 50
        if start_max <= start_min:
            logger.warning("í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None
        
        start_idx = np.random.randint(start_min, start_max)
        
        # ì´ˆê¸°í™”
        self.data_collector.current_index = start_idx
        self.agent.reset_episode_states()  # [ì¶”ê°€] ì—í”¼ì†Œë“œ ì‹œì‘ ì „ ë‡Œ ë¦¬ì…‹ (ì¤‘ìš”!)
        current_position = None  # 'LONG', 'SHORT', None
        entry_price = 0.0
        entry_index = 0
        episode_reward = 0.0
        
        batch_size = getattr(config, 'SAC_BATCH_SIZE', 256)
        
        for step in range(max_steps):
            current_idx = self.data_collector.current_index
            if current_idx >= self.train_end_idx:
                break
            
            # Position Info êµ¬ì„±
            pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            holding_time = (current_idx - entry_index) if current_position is not None else 0
            
            # Unrealized PnL (ê´€ì¸¡ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©, ë³´ìƒì—” ì•ˆ ì”€)
            curr_price = float(self.data_collector.eth_data.iloc[current_idx]['close'])
            unrealized_pnl = 0.0
            if current_position == 'LONG':
                unrealized_pnl = (curr_price - entry_price) / entry_price
            elif current_position == 'SHORT':
                unrealized_pnl = (entry_price - curr_price) / entry_price
                
            pos_info = [pos_val, unrealized_pnl * 10, holding_time / max_steps]
            
            # 1. ìƒíƒœ ê´€ì¸¡ (State) - [ë¬¸ì œ í•´ê²° 1] ì¸ë±ìŠ¤ ëª…ì‹œì  ì „ë‹¬
            state = self.env.get_observation(position_info=pos_info, current_index=current_idx)
            if state is None:
                break

            # 2. í–‰ë™ ì„ íƒ (Action)
            action_continuous = self.agent.select_action(state)  # [-1, 1]
            action_code = self.interpret_action(action_continuous[0])  # 0, 1, 2
            
            # 3. íŠ¸ë ˆì´ë”© ë¡œì§ ì‹¤í–‰
            reward = 0.0
            trade_done = False
            realized_pnl = 0.0
            
            # A. í¬ì§€ì…˜ ì²­ì‚° ì¡°ê±´ (ì‹ í˜¸ ë°˜ì „ or Neutral ì‹ í˜¸ or ì†ì ˆ)
            if current_position is not None:
                should_exit = False
                
                # Exit ì¡°ê±´ 1: ì‹ í˜¸ ë³€ê²½ (Longì¸ë° Short/Neutral ì‹ í˜¸ ëœ¸)
                if current_position == 'LONG' and action_code != 1:
                    should_exit = True
                if current_position == 'SHORT' and action_code != 2:
                    should_exit = True
                
                # Exit ì¡°ê±´ 2: ì†ì ˆ (Stop Loss) -2%
                if unrealized_pnl < -0.02:
                    should_exit = True
                
                if should_exit:
                    realized_pnl = unrealized_pnl  # í™•ì •
                    trade_done = True
                    current_position = None  # í¬ì§€ì…˜ í•´ì œ
                    entry_price = 0.0
                    entry_index = 0
            
            # B. ì‹ ê·œ ì§„ì… ì¡°ê±´ (í¬ì§€ì…˜ ì—†ì„ ë•Œë§Œ)
            if current_position is None and not trade_done:
                if action_code == 1:  # LONG Entry
                    current_position = 'LONG'
                    entry_price = curr_price
                    entry_index = current_idx
                elif action_code == 2:  # SHORT Entry
                    current_position = 'SHORT'
                    entry_price = curr_price
                    entry_index = current_idx
            
            # 4. ë³´ìƒ ê³„ì‚° (Realized PnL ìœ„ì£¼)
            reward = self.env.calculate_reward(realized_pnl, trade_done, holding_time)
            
            # 5. ë‹¤ìŒ ìƒíƒœ ê´€ì¸¡ (Next State) - [ë¬¸ì œ í•´ê²° 1] ì¸ë±ìŠ¤ ëª…ì‹œì  ì „ë‹¬
            next_idx = current_idx + 1
            self.data_collector.current_index = next_idx  # Loop ì§„í–‰ì„ ìœ„í•´ ì—…ë°ì´íŠ¸
            
            # Next Position Info ì¶”ì •
            next_pos_val = 1.0 if current_position == 'LONG' else (-1.0 if current_position == 'SHORT' else 0.0)
            next_hold_time = (next_idx - entry_index) if current_position is not None else 0
            
            # ë‹¤ìŒ ê°€ê²© (ìˆë‹¤ë©´)
            if next_idx < len(self.data_collector.eth_data):
                next_price = float(self.data_collector.eth_data.iloc[next_idx]['close'])
                next_un_pnl = 0.0
                if current_position == 'LONG':
                    next_un_pnl = (next_price - entry_price) / entry_price
                elif current_position == 'SHORT':
                    next_un_pnl = (entry_price - next_price) / entry_price
            else:
                next_un_pnl = 0.0
                
            next_pos_info = [next_pos_val, next_un_pnl * 10, next_hold_time / max_steps]
            
            next_state = self.env.get_observation(position_info=next_pos_info, current_index=next_idx)
            
            # ì¢…ë£Œ ì—¬ë¶€
            done = False if step < max_steps - 1 else True
            if next_state is None:
                done = True
            
            # Fallback for next_state (ëë¶€ë¶„ ì²˜ë¦¬)
            if next_state is None:
                next_state = state

            # 6. ì €ì¥ ë° í•™ìŠµ
            self.agent.memory.push(state, action_continuous, reward, next_state, done)
            episode_reward += reward
            
            if len(self.agent.memory) > batch_size:
                self.agent.update(batch_size=batch_size)
                self.agent.step_schedulers()

        return episode_reward
    
    def train(self, num_episodes=1000, max_steps_per_episode=None, save_interval=None):
        """ëª¨ë¸ í•™ìŠµ"""
        if max_steps_per_episode is None:
            max_steps_per_episode = config.TRAIN_MAX_STEPS_PER_EPISODE
        if save_interval is None:
            save_interval = config.TRAIN_SAVE_INTERVAL
        
        # í•™ìŠµ ì‹œì‘ ì „ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        # (ì´ ì˜ˆìƒ ì—…ë°ì´íŠ¸ íšŸìˆ˜ = ì—í”¼ì†Œë“œ * ìŠ¤í… ìˆ˜)
        total_steps = num_episodes * max_steps_per_episode
        warmup_ratio = getattr(config, 'SAC_WARMUP_RATIO', 0.05)
        
        logger.info("=" * 60)
        logger.info("ğŸš€ SAC ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Best/Last Save Enabled)")
        logger.info("=" * 60)
        logger.info(f"ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        logger.info(f"ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…: {max_steps_per_episode}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ê°„ê²©: {save_interval} ì—í”¼ì†Œë“œ")
        logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: ì´ {total_steps} ìŠ¤í…, Warmup {warmup_ratio*100:.1f}%")
        logger.info("=" * 60)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.agent.setup_schedulers(total_steps, warmup_ratio)
        
        # [NEW] ì €ì¥ ê²½ë¡œ ì„¤ì • (Best/Last ë¶„ë¦¬)
        base_path = config.AI_MODEL_PATH.replace('ppo_model', 'sac_model').replace('.pth', '')
        
        best_model_path = f"{base_path}_best.pth"
        best_scaler_path = f"{base_path}_best_scaler.pkl"
        
        last_model_path = f"{base_path}_last.pth"
        last_scaler_path = f"{base_path}_last_scaler.pkl"
        
        # ì´ˆê¸° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (Lastì— ë°±ì—…)
        os.makedirs(os.path.dirname(last_scaler_path), exist_ok=True)
        self.env.preprocessor.save(last_scaler_path)
        
        logger.info(f"ğŸš€ SAC í•™ìŠµ ì‹œì‘ (Best/Last Save Enabled)")
        best_reward = float('-inf')
        
        for episode in range(1, num_episodes + 1):
            try:
                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                episode_reward = self.train_episode(episode_num=episode, max_steps=max_steps_per_episode)
                if episode_reward is None:
                    logger.warning("ì—í”¼ì†Œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰")
                    continue
                
                self.episode_rewards.append(episode_reward)
                avg_reward = np.mean(self.episode_rewards[-10:])
                self.avg_rewards.append(avg_reward)
                
                # í†µê³„ ì¶œë ¥
                current_lr = self.agent.actor_scheduler.get_last_lr()[0] if self.agent.actor_scheduler else config.SAC_LEARNING_RATE
                logger.info(f"âœ… Ep {episode}: Reward {episode_reward:.4f} | Avg {avg_reward:.4f} | LR {current_lr:.6f}")
                
                # ê·¸ë˜í”„ ê°±ì‹ 
                self.live_plot()
                
                # [NEW] Best ëª¨ë¸ ì €ì¥ (ì‹ ê¸°ë¡ ê°±ì‹  ì‹œ)
                if episode_reward > best_reward:
                    best_reward = episode_reward
                    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
                    self.agent.save_model(best_model_path)
                    self.env.preprocessor.save(best_scaler_path)
                    logger.info(f"ğŸ† ì‹ ê¸°ë¡ ë‹¬ì„±! ({best_reward:.4f}) -> Best ëª¨ë¸ ì €ì¥")
                
                # [NEW] Last ëª¨ë¸ ì €ì¥ (ë§¤ë²ˆ or ì£¼ê¸°ì ìœ¼ë¡œ)
                if episode % save_interval == 0:
                    os.makedirs(os.path.dirname(last_model_path), exist_ok=True)
                    self.agent.save_model(last_model_path)
                    self.env.preprocessor.save(last_scaler_path)
                    # logger.info(f"ğŸ’¾ ì •ê¸° ì €ì¥ ì™„ë£Œ (Ep {episode})")
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨")
                break
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ {episode} ì‹¤íŒ¨: {e}", exc_info=True)
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥ (Last)
        os.makedirs(os.path.dirname(last_model_path), exist_ok=True)
        self.agent.save_model(last_model_path)
        self.env.preprocessor.save(last_scaler_path)
        
        # í•™ìŠµ ì¢…ë£Œ ì‹œ ê·¸ë˜í”„ ìœ ì§€
        if self.enable_visualization:
            plt.ioff()
            plt.show()
        
        logger.info("=" * 60)
        logger.info("âœ… í•™ìŠµ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        logger.info(f"ì´ ìŠ¤í…: {self.total_steps}")
        logger.info(f"í‰ê·  ë³´ìƒ: {sum(self.episode_rewards) / len(self.episode_rewards) if self.episode_rewards else 0:.4f}")
        logger.info(f"Best ëª¨ë¸: {best_model_path}")
        logger.info(f"Last ëª¨ë¸: {last_model_path}")
        logger.info("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='SAC ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--episodes', type=int, default=config.TRAIN_NUM_EPISODES, help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--steps', type=int, default=config.TRAIN_MAX_STEPS_PER_EPISODE, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--save-interval', type=int, default=config.TRAIN_SAVE_INTERVAL, help='ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ)')
    parser.add_argument('--no-plot', action='store_true', help='ê·¸ë˜í”„ ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    
    # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['axes.unicode_minus'] = False
    
    try:
        trainer = SACTrainer(enable_visualization=not args.no_plot)
        trainer.train(
            num_episodes=args.episodes,
            max_steps_per_episode=args.steps,
            save_interval=args.save_interval
        )
    except KeyboardInterrupt:
        logger.info("í•™ìŠµ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
