"""
SAC (Soft Actor-Critic) ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ì—°ì†í˜• í–‰ë™ ê³µê°„ì„ ì‚¬ìš©í•˜ëŠ” Off-policy ì•Œê³ ë¦¬ì¦˜
"""
import logging
import os
import sys
import time
import numpy as np
import pandas as pd
import torch
from datetime import datetime
from collections import deque

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core import DataCollector, BinanceClient
from strategies import (
    BTCEthCorrelationStrategy, VolatilitySqueezeStrategy, OrderblockFVGStrategy,
    HMAMomentumStrategy, MFIMomentumStrategy, BollingerMeanReversionStrategy,
    VWAPDeviationStrategy, RangeTopBottomStrategy, StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy,
    CCIReversalStrategy, WilliamsRStrategy
)

from model.trading_env import TradingEnvironment
from model.sac_agent import SACAgent
from model.feature_engineering import FeatureEngineer
from model.mtf_processor import MTFProcessor

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_sac.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger('model.feature_engineering').setLevel(logging.WARNING)
logging.getLogger('model.mtf_processor').setLevel(logging.WARNING)


class SACTrainer:
    """SAC ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # 1. ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.data_collector = DataCollector(use_saved_data=True)
        if not self.data_collector.load_saved_data():
            raise ValueError("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        
        # 2. ì „ëµ ì´ˆê¸°í™”
        self.breakout_strategies = []
        self.range_strategies = []
        
        # í­ë°œì¥ ì „ëµ
        if config.STRATEGIES.get('btc_eth_correlation', False):
            self.breakout_strategies.append(BTCEthCorrelationStrategy())
        if config.STRATEGIES.get('volatility_squeeze', False):
            self.breakout_strategies.append(VolatilitySqueezeStrategy())
        if config.STRATEGIES.get('orderblock_fvg', False):
            self.breakout_strategies.append(OrderblockFVGStrategy())
        if config.STRATEGIES.get('hma_momentum', False):
            self.breakout_strategies.append(HMAMomentumStrategy())
        if config.STRATEGIES.get('mfi_momentum', False):
            self.breakout_strategies.append(MFIMomentumStrategy())
        
        self.breakout_strategies.append(CCIReversalStrategy())
        
        # íš¡ë³´ì¥ ì „ëµ
        if config.STRATEGIES.get('bollinger_mean_reversion', False):
            self.range_strategies.append(BollingerMeanReversionStrategy())
        if config.STRATEGIES.get('vwap_deviation', False):
            self.range_strategies.append(VWAPDeviationStrategy())
        if config.STRATEGIES.get('range_top_bottom', False):
            self.range_strategies.append(RangeTopBottomStrategy())
        if config.STRATEGIES.get('stoch_rsi_mean_reversion', False):
            self.range_strategies.append(StochRSIMeanReversionStrategy())
        if config.STRATEGIES.get('cmf_divergence', False):
            self.range_strategies.append(CMFDivergenceStrategy())
        
        self.range_strategies.append(WilliamsRStrategy())
        
        self.strategies = self.breakout_strategies + self.range_strategies
        
        if len(self.strategies) == 0:
            raise ValueError("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. config.pyì—ì„œ ì „ëµì„ í™œì„±í™”í•˜ì„¸ìš”.")
        
        logger.info(f"âœ… ì „ëµ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {len(self.strategies)}ê°œ")
        
        # 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í•œ ë²ˆë§Œ ìˆ˜í–‰)
        self._precalculate_features()
        
        # 4. í™˜ê²½ ìƒì„± (config.LOOKBACK ì‚¬ìš©)
        self.env = TradingEnvironment(self.data_collector, self.strategies, lookback=config.LOOKBACK)
        
        # 5. ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        self._fit_global_scaler()
        
        # 6. Agent ìƒì„±
        state_dim = self.env.get_state_dim()  # 29
        action_dim = 1  # ì—°ì†í˜•: ë§¤ìˆ˜/ë§¤ë„ ê°•ë„ (-1 ~ 1)
        info_dim = len(self.strategies) + 3  # ì „ëµ ì ìˆ˜ + í¬ì§€ì…˜ ì •ë³´
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
        
        # configì—ì„œ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        self.agent = SACAgent(
            state_dim, 
            action_dim, 
            info_dim=info_dim, 
            hidden_dim=config.NETWORK_HIDDEN_DIM, 
            device=device
        )
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        model_path = config.AI_MODEL_PATH.replace('ppo_model', 'sac_model')
        if os.path.exists(model_path):
            try:
                self.agent.load_model(model_path)
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {model_path}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìƒˆ ëª¨ë¸ë¡œ ì‹œì‘): {e}")
        else:
            logger.info("ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµ ì‹œì‘")
        
        # í•™ìŠµ ìƒíƒœ
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        self.prev_pnl = 0.0
        self.episode_rewards = []
        self.total_steps = 0
    
    def _precalculate_features(self):
        """ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ (í•œ ë²ˆë§Œ)"""
        try:
            logger.info("ğŸ“Š ì „ì²´ ë°ì´í„° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ ì¤‘...")
            
            # ETH ë°ì´í„° ì¤€ë¹„
            eth_data = self.data_collector.eth_data.copy()
            
            # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸ ë° ë³€í™˜
            if not isinstance(eth_data.index, pd.DatetimeIndex):
                if 'timestamp' in eth_data.columns:
                    eth_data.index = pd.to_datetime(eth_data['timestamp'], unit='ms')
                else:
                    eth_data.index = pd.date_range(end=pd.Timestamp.now(), periods=len(eth_data), freq='3min')
            
            # BTC ë°ì´í„° ì¤€ë¹„
            btc_data = None
            if hasattr(self.data_collector, 'btc_data') and self.data_collector.btc_data is not None:
                btc_data = self.data_collector.btc_data.copy()
                if not isinstance(btc_data.index, pd.DatetimeIndex):
                    if 'timestamp' in btc_data.columns:
                        btc_data.index = pd.to_datetime(btc_data['timestamp'], unit='ms')
                    else:
                        btc_data.index = pd.date_range(end=pd.Timestamp.now(), periods=len(btc_data), freq='3min')
                
                # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
                common_index = eth_data.index.intersection(btc_data.index)
                if len(common_index) > 0:
                    eth_data = eth_data.loc[common_index]
                    btc_data = btc_data.loc[common_index]
            
            # (1) ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            feature_engineer = FeatureEngineer(eth_data, btc_data)
            df = feature_engineer.generate_features()
            
            if df is None:
                raise ValueError("í”¼ì²˜ ìƒì„± ì‹¤íŒ¨")
            
            # (2) ë©€í‹° íƒ€ì„í”„ë ˆì„ ì§€í‘œ ì¶”ê°€
            mtf_processor = MTFProcessor(df)
            df = mtf_processor.add_mtf_features()
            
            # ë°ì´í„° êµì²´
            self.data_collector.eth_data = df
            if btc_data is not None:
                self.data_collector.btc_data = btc_data
            
            logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨: {e}", exc_info=True)
            raise
    
    def _fit_global_scaler(self):
        """29ê°œ ê³ ê¸‰ í”¼ì²˜ ê¸°ë°˜ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (ìµœì í™” ë²„ì „)"""
        try:
            logger.info("ğŸš€ 29ê°œ ê³ ê¸‰ í”¼ì²˜ ê¸°ë°˜ ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            
            if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
                logger.warning("ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            # ì‚¬ìš©í•  29ê°œ ì»¬ëŸ¼ ì •ì˜
            target_cols = [
                'log_return', 'roll_return_6', 'atr_ratio', 'bb_width', 'bb_pos', 
                'rsi', 'macd_hist', 'hma_ratio', 'cci', 
                'rvol', 'taker_ratio', 'cvd_change', 'mfi', 'cmf', 'vwap_dist',
                'wick_upper', 'wick_lower', 'range_pos', 'swing_break', 'chop',
                'btc_return', 'btc_rsi', 'btc_corr', 'btc_vol', 'eth_btc_ratio',
                'rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'
            ]
            
            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            missing_cols = [c for c in target_cols if c not in self.data_collector.eth_data.columns]
            if missing_cols:
                logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆì–´ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤: {missing_cols}")
                for c in missing_cols:
                    self.data_collector.eth_data[c] = 0.0
            
            # ìƒ˜í”Œë§
            total_candles = len(self.data_collector.eth_data)
            min_required = self.env.lookback + 100
            sample_size = min(50000, total_candles - min_required)
            
            if total_candles > min_required + sample_size:
                indices = np.linspace(min_required, total_candles - 1, sample_size, dtype=int)
            else:
                indices = np.arange(min_required, total_candles)
            
            logger.info(f"ë°ì´í„° ì¶”ì¶œ ì¤‘... ({len(indices)}ê°œ ìƒ˜í”Œ)")
            
            # ë°ì´í„° ìˆ˜ì§‘
            all_seq_features = []
            for idx in indices:
                if idx < self.env.lookback:
                    continue
                recent_df = self.data_collector.eth_data[target_cols].iloc[idx-self.env.lookback+1:idx+1]
                if len(recent_df) == self.env.lookback:
                    seq_features = recent_df.values.astype(np.float32)
                    all_seq_features.append(seq_features)
            
            if len(all_seq_features) == 0:
                logger.warning("í”¼ì²˜ ìˆ˜ì§‘ ì‹¤íŒ¨, ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ê±´ë„ˆëœ€")
                return
            
            all_features_array = np.vstack(all_seq_features)
            
            # NaN ì²˜ë¦¬
            if np.isnan(all_features_array).any():
                all_features_array = np.nan_to_num(all_features_array)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            self.env.preprocessor.fit(all_features_array)
            self.env.scaler_fitted = True
            
            logger.info(f"âœ… ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ: {len(all_features_array)}ê°œ ìƒ˜í”Œ, Feature Dim: {all_features_array.shape[1]}")
            
        except Exception as e:
            logger.error(f"ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            logger.warning("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨, í•™ìŠµ ë„ì¤‘ online-fittingìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    
    def interpret_action(self, action_value):
        """
        ì—°ì†í˜• ì•¡ì…˜(-1 ~ 1)ì„ íŠ¸ë ˆì´ë”© ëª…ë ¹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            action_value: float, -1 ~ 1 ì‚¬ì´ì˜ ì—°ì†ê°’
        Returns:
            int: 0=HOLD, 1=LONG, 2=SHORT
        """
        threshold = 0.3
        
        if action_value > threshold:
            return 1  # LONG
        elif action_value < -threshold:
            return 2  # SHORT
        else:
            return 0  # HOLD
    
    def train_episode(self, episode_num, max_steps=None):
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ"""
        if max_steps is None:
            max_steps = config.TRAIN_MAX_STEPS_PER_EPISODE
        
        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ Configì—ì„œ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ 256)
        batch_size = config.SAC_BATCH_SIZE
        
        episode_reward = 0.0
        steps = 0
        
        # ì €ì¥ëœ ë°ì´í„°ì—ì„œ ì¸ë±ìŠ¤ ë¦¬ì…‹ (ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘ - ë¬´ì‘ìœ„ ì‹œì‘ ì¸ë±ìŠ¤)
        self.data_collector.reset_index(max_steps=max_steps, random_start=True)
        
        # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì´ì „ ìˆ˜ìµë¥  ì´ˆê¸°í™”
        self.prev_pnl = 0.0
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        
        # ì´ˆê¸° ë°ì´í„° í™•ì¸
        if self.data_collector.eth_data is None or len(self.data_collector.eth_data) == 0:
            logger.error("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ ìŠ¤í… ìˆ˜ ê³„ì‚°
        available_steps = len(self.data_collector.eth_data) - self.data_collector.current_index
        actual_steps = min(max_steps, available_steps)
        
        if actual_steps <= 0:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None
        
        logger.info(f"ì—í”¼ì†Œë“œ ì‹œì‘: ì´ {len(self.data_collector.eth_data)}ê°œ ìº”ë“¤ ì¤‘ {actual_steps}ê°œ ì‚¬ìš©")
        
        for step in range(actual_steps):
            try:
                # 1. ì¸ë±ìŠ¤ ì¦ê°€ (ë‹¤ìŒ ìº”ë“¤ë¡œ ì´ë™)
                if self.data_collector.current_index >= len(self.data_collector.eth_data):
                    break
                
                self.data_collector.current_index += 1
                
                # 2. í¬ì§€ì…˜ ì •ë³´ ìˆ˜ì§‘
                pos_val = 1.0 if self.current_position == 'LONG' else (-1.0 if self.current_position == 'SHORT' else 0.0)
                hold_val = (self.data_collector.current_index - self.entry_index) / max_steps if self.entry_index is not None else 0.0
                pnl_val = self.prev_pnl * 10
                pos_info = [pos_val, pnl_val, hold_val]
                
                # 3. ìƒíƒœ ê´€ì¸¡
                state = self.env.get_observation(position_info=pos_info)
                if state is None:
                    continue
                
                # 4. í–‰ë™ ì„ íƒ (SAC - ì—°ì†í˜•)
                action_continuous = self.agent.select_action(state)  # ì˜ˆ: [0.75]
                action_discrete = self.interpret_action(action_continuous[0])
                
                # 5. í˜„ì¬ ê°€ê²© í™•ì¸
                if self.data_collector.current_index > 0:
                    current_candle = self.data_collector.eth_data.iloc[self.data_collector.current_index - 1]
                    current_price = float(current_candle['close'])
                else:
                    continue
                
                # 6. ë³´ìƒ ê³„ì‚° ë° í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
                reward = 0.0
                trade_done = False
                current_pnl = 0.0
                pnl_change = 0.0
                
                if action_discrete == 1:  # LONG
                    if self.current_position != 'LONG':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'SHORT' and self.entry_price:
                            pnl = (self.entry_price - current_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl
                            reward = self.env.calculate_reward(pnl, True, holding_time=0, pnl_change=pnl_change)
                            trade_done = True
                            self.prev_pnl = 0.0
                        
                        # ë¡± ì§„ì…
                        self.current_position = 'LONG'
                        self.entry_price = current_price
                        self.entry_index = self.data_collector.current_index
                        self.prev_pnl = 0.0
                
                elif action_discrete == 2:  # SHORT
                    if self.current_position != 'SHORT':
                        # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                        if self.current_position == 'LONG' and self.entry_price:
                            pnl = (current_price - self.entry_price) / self.entry_price
                            pnl_change = pnl - self.prev_pnl
                            reward = self.env.calculate_reward(pnl, True, holding_time=0, pnl_change=pnl_change)
                            trade_done = True
                            self.prev_pnl = 0.0
                        
                        # ìˆ ì§„ì…
                        self.current_position = 'SHORT'
                        self.entry_price = current_price
                        self.entry_index = self.data_collector.current_index
                        self.prev_pnl = 0.0
                
                else:  # HOLD
                    # ë³´ìœ  ì¤‘ì¸ í¬ì§€ì…˜ì˜ ìˆ˜ìµë¥  ê³„ì‚°
                    if self.current_position and self.entry_price:
                        if self.current_position == 'LONG':
                            current_pnl = (current_price - self.entry_price) / self.entry_price
                        else:
                            current_pnl = (self.entry_price - current_price) / self.entry_price
                        
                        pnl_change = current_pnl - self.prev_pnl
                        holding_time = (self.data_collector.current_index - self.entry_index) if self.entry_index is not None else 0
                        reward = self.env.calculate_reward(current_pnl, False, holding_time, pnl_change)
                        self.prev_pnl = current_pnl
                
                # 7. ë‹¤ìŒ ìƒíƒœ ê´€ì¸¡
                next_pos_val = 1.0 if self.current_position == 'LONG' else (-1.0 if self.current_position == 'SHORT' else 0.0)
                next_hold_val = (self.data_collector.current_index + 1 - self.entry_index) / max_steps if self.entry_index is not None else 0.0
                next_pnl_val = self.prev_pnl * 10
                next_pos_info = [next_pos_val, next_pnl_val, next_hold_val]
                
                # ì„ì‹œë¡œ ì¸ë±ìŠ¤ ì¦ê°€í•˜ì—¬ ë‹¤ìŒ ìƒíƒœ ê´€ì¸¡
                temp_index = self.data_collector.current_index
                if temp_index < len(self.data_collector.eth_data):
                    self.data_collector.current_index += 1
                    next_state = self.env.get_observation(position_info=next_pos_info)
                    self.data_collector.current_index = temp_index
                else:
                    next_state = None
                
                # 8. Replay Buffer ì €ì¥ (ì—°ì†í˜• ì•¡ì…˜ ì €ì¥)
                is_terminal = (step == actual_steps - 1)
                self.agent.memory.push(state, action_continuous, reward, next_state, is_terminal)
                
                episode_reward += reward
                steps += 1
                self.total_steps += 1
                
                # 9. í•™ìŠµ (ë§¤ ìŠ¤í…ë§ˆë‹¤ ë°°ì¹˜ë¥¼ ë½‘ì•„ì„œ í•™ìŠµ)
                # ë©”ëª¨ë¦¬ê°€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë³´ë‹¤ í´ ë•Œë§Œ ì—…ë°ì´íŠ¸
                if len(self.agent.memory) > batch_size:
                    c_loss, a_loss, alpha = self.agent.update(batch_size=batch_size)
                    # [ì¤‘ìš”] LR ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    self.agent.step_schedulers()
                    if step % 100 == 0:
                        current_lr = self.agent.actor_scheduler.get_last_lr()[0] if self.agent.actor_scheduler else config.SAC_LEARNING_RATE
                        logger.debug(f"Step {step}: Critic Loss={c_loss:.4f}, Actor Loss={a_loss:.4f}, Alpha={alpha:.4f}, LR={current_lr:.6f}")
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
                raise
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                time.sleep(5)
                continue
        
        return episode_reward, steps
    
    def train(self, num_episodes=1000, max_steps_per_episode=None, save_interval=None):
        """ëª¨ë¸ í•™ìŠµ"""
        if max_steps_per_episode is None:
            max_steps_per_episode = config.TRAIN_MAX_STEPS_PER_EPISODE
        if save_interval is None:
            save_interval = config.TRAIN_SAVE_INTERVAL
        
        # í•™ìŠµ ì‹œì‘ ì „ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        # (ì´ ì˜ˆìƒ ì—…ë°ì´íŠ¸ íšŸìˆ˜ = ì—í”¼ì†Œë“œ * ìŠ¤í… ìˆ˜)
        total_steps = num_episodes * max_steps_per_episode
        warmup_ratio = getattr(config, 'SAC_WARMUP_RATIO', 0.05)
        
        logger.info("=" * 60)
        logger.info("ğŸš€ SAC ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        logger.info(f"ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…: {max_steps_per_episode}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ê°„ê²©: {save_interval} ì—í”¼ì†Œë“œ")
        logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: ì´ {total_steps} ìŠ¤í…, Warmup {warmup_ratio*100:.1f}%")
        logger.info("=" * 60)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.agent.setup_schedulers(total_steps, warmup_ratio)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ê²½ë¡œ ì„¤ì •
        scaler_path = config.AI_MODEL_PATH.replace('ppo_model', 'sac_model').replace('.pth', '_scaler.pkl')
        if not scaler_path.endswith('.pkl'):
            scaler_path = config.AI_MODEL_PATH.replace('ppo_model', 'sac_model') + '_scaler.pkl'
        
        model_path = config.AI_MODEL_PATH.replace('ppo_model', 'sac_model')
        
        best_reward = float('-inf')
        
        for episode in range(1, num_episodes + 1):
            try:
                logger.info(f"\n{'=' * 60}")
                logger.info(f"ğŸ“š ì—í”¼ì†Œë“œ {episode}/{num_episodes}")
                logger.info(f"{'=' * 60}")
                
                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                result = self.train_episode(episode_num=episode, max_steps=max_steps_per_episode)
                if result is None:
                    logger.warning("ì—í”¼ì†Œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì§„í–‰")
                    continue
                
                episode_reward, steps = result
                self.episode_rewards.append(episode_reward)
                
                # í†µê³„ ì¶œë ¥
                avg_reward = sum(self.episode_rewards[-10:]) / len(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else episode_reward
                # ë¡œê·¸ ì¶œë ¥ ì‹œ í˜„ì¬ LRë„ í•¨ê»˜ ì¶œë ¥
                current_lr = self.agent.actor_scheduler.get_last_lr()[0] if self.agent.actor_scheduler else config.SAC_LEARNING_RATE
                logger.info(f"âœ… ì—í”¼ì†Œë“œ {episode} ì™„ë£Œ")
                logger.info(f"   ì´ ë³´ìƒ: {episode_reward:.4f}")
                logger.info(f"   ìŠ¤í… ìˆ˜: {steps}")
                logger.info(f"   ìµœê·¼ 10ê°œ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
                logger.info(f"   í˜„ì¬ í•™ìŠµë¥ : {current_lr:.6f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if episode_reward > best_reward:
                    best_reward = episode_reward
                    os.makedirs(os.path.dirname(model_path), exist_ok=True)
                    self.agent.save_model(model_path)
                    self.env.preprocessor.save(scaler_path)
                    logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ & ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: ë³´ìƒ {best_reward:.4f}")
                
                # ì£¼ê¸°ì  ì €ì¥
                elif episode % save_interval == 0:
                    os.makedirs(os.path.dirname(model_path), exist_ok=True)
                    self.agent.save_model(model_path)
                    self.env.preprocessor.save(scaler_path)
                    logger.info(f"ğŸ’¾ ì •ê¸° ì €ì¥ ì™„ë£Œ (ì—í”¼ì†Œë“œ {episode})")
                
            except KeyboardInterrupt:
                logger.info("í•™ìŠµ ì¤‘ë‹¨")
                break
            except Exception as e:
                logger.error(f"ì—í”¼ì†Œë“œ {episode} ì‹¤íŒ¨: {e}", exc_info=True)
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        self.agent.save_model(model_path)
        self.env.preprocessor.save(scaler_path)
        logger.info("=" * 60)
        logger.info("âœ… í•™ìŠµ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
        logger.info(f"ì´ ìŠ¤í…: {self.total_steps}")
        logger.info(f"í‰ê·  ë³´ìƒ: {sum(self.episode_rewards) / len(self.episode_rewards) if self.episode_rewards else 0:.4f}")
        logger.info(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")
        logger.info(f"ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ìœ„ì¹˜: {scaler_path}")
        logger.info("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='SAC ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--episodes', type=int, default=1000, help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--steps', type=int, default=480, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--save-interval', type=int, default=50, help='ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ)')
    
    args = parser.parse_args()
    
    try:
        trainer = SACTrainer()
        trainer.train(
            num_episodes=args.episodes,
            max_steps_per_episode=args.steps,
            save_interval=args.save_interval
        )
    except KeyboardInterrupt:
        logger.info("í•™ìŠµ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)
