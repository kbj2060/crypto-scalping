"""
DDQN í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Final Optimized)
ì „ëµ ì§€í‘œ 10ê°œ + PPO ê¸°ë³¸ ë°ì´í„° 5ê°œ = ì´ 15ê°œ í”¼ì²˜ ì‚¬ìš©
ë©”ëª¨ë¦¬ ì•ˆì „ ì—°ì‚°(.values) ì ìš© ì™„ë£Œ
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
import pandas as pd
import logging
import time

from core.data_collector import DataCollector
from core.indicators import Indicators
from model.dqn_agent import DDQNAgent
from model.trading_env import TradingEnvironment
from model.feature_selection import FeatureSelector
from model.mtf_processor import MTFProcessor
import config

# ì „ëµ íŒŒì¼ë“¤ ì„í¬íŠ¸
from strategies import (
    BTCEthCorrelationStrategy, VolatilitySqueezeStrategy, OrderblockFVGStrategy,
    HMAMomentumStrategy, MFIMomentumStrategy, BollingerMeanReversionStrategy,
    VWAPDeviationStrategy, RangeTopBottomStrategy, StochRSIMeanReversionStrategy,
    CMFDivergenceStrategy
)

# ì§„í–‰ë¥  í‘œì‹œìš© (ì„ íƒì )
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    def tqdm(iterable, desc=""):
        return iterable

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_dqn.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ì‹œê°í™” (ì„ íƒì )
try:
    import matplotlib.pyplot as plt
    from collections import deque
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    logger.warning("matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™” ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


class LiveVisualizer:
    """í•™ìŠµ ë¦¬ì›Œë“œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„í™”í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, window_size=10, enable=True):
        if not enable or not VISUALIZATION_AVAILABLE:
            self.enabled = False
            return
        
        self.enabled = True
        plt.ion()  # ëŒ€í™”í˜• ëª¨ë“œ í™œì„±í™”
        self.fig, self.ax = plt.subplots(figsize=(12, 6))
        self.rewards = []
        self.moving_avg = []
        self.window_size = window_size
        self.ax.set_title("DDQN Training Performance", fontsize=14, fontweight='bold')
        self.ax.set_xlabel("Episode", fontsize=12)
        self.ax.set_ylabel("Total Reward", fontsize=12)
        self.line1, = self.ax.plot([], [], label='Episode Reward', alpha=0.3, color='blue', linewidth=1)
        self.line2, = self.ax.plot([], [], label=f'Moving Avg ({window_size})', color='red', linewidth=2)
        self.ax.legend(loc='upper left')
        self.ax.grid(True, alpha=0.3)
        plt.tight_layout()
    
    def update(self, reward):
        """ë³´ìƒ ì—…ë°ì´íŠ¸ ë° ê·¸ë˜í”„ ê°±ì‹ """
        if not self.enabled:
            return
        
        try:
            self.rewards.append(reward)
            
            # ì´ë™ í‰ê·  ê³„ì‚°
            if len(self.rewards) >= self.window_size:
                avg = np.mean(self.rewards[-self.window_size:])
            else:
                avg = np.mean(self.rewards) if self.rewards else 0
            self.moving_avg.append(avg)
            
            # ë°ì´í„° ì—…ë°ì´íŠ¸
            x = np.arange(len(self.rewards))
            self.line1.set_data(x, self.rewards)
            self.line2.set_data(x, self.moving_avg)
            
            # í™”ë©´ ë²”ìœ„ ìë™ ì¡°ì ˆ
            self.ax.relim()
            self.ax.autoscale_view()
            
            # Yì¶• ë²”ìœ„ë¥¼ ì ì ˆí•˜ê²Œ ì„¤ì • (ì´ìƒì¹˜ ì œì™¸)
            if len(self.rewards) > 0:
                y_min = min(min(self.rewards), min(self.moving_avg))
                y_max = max(max(self.rewards), max(self.moving_avg))
                margin = (y_max - y_min) * 0.1
                self.ax.set_ylim(y_min - margin, y_max + margin)
            
            plt.draw()
            plt.pause(0.01)  # ì§§ì€ íœ´ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ê°±ì‹  ë³´ì¥
            
        except Exception as e:
            logger.debug(f"ì‹œê°í™” ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def close(self):
        """ê·¸ë˜í”„ ì°½ ë‹«ê¸°"""
        if self.enabled:
            plt.close(self.fig)


def calculate_technical_features(data):
    """
    ê¸°ìˆ ì  ì§€í‘œ 15ê°œ ê³„ì‚° (ê¸°ì¡´ í•¨ìˆ˜)
    ì•ˆì „í•œ Numpy ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°©ì§€
    """
    try:
        # 1. ë°ì´í„° ì¶”ì¶œ (Numpy Array)
        close = data['close'].values
        high = data['high'].values
        low = data['low'].values
        open_val = data['open'].values
        volume = data['volume'].values
        
        # DataFrame ì¤€ë¹„
        df = pd.DataFrame(index=data.index)
        
        # --- Group A: PPO ëª¨ë¸ ê¸°ë³¸ ë°ì´í„° (5ê°œ) ---
        
        # 1. Log Return (ë¡œê·¸ ìˆ˜ìµë¥ )
        df['log_return'] = np.concatenate([[0], np.diff(np.log(close + 1e-8))])
        
        # 2. Log Volume (ê±°ë˜ëŸ‰ ë¡œê·¸)
        df['log_volume'] = np.log1p(volume)
        
        # 3. High-Close Ratio (ìœ—ê¼¬ë¦¬)
        df['high_ratio'] = (high - close) / (close + 1e-8)
        
        # 4. Low-Close Ratio (ì•„ë«ê¼¬ë¦¬)
        df['low_ratio'] = (low - close) / (close + 1e-8)
        
        # 5. Taker Buy Ratio (ì—†ìœ¼ë©´ Tradesë¡œ ëŒ€ì²´)
        if 'taker_buy_base' in data.columns:
            df['taker_ratio'] = data['taker_buy_base'].values / (volume + 1e-8)
        elif 'taker_buy_base_volume' in data.columns:
            df['taker_ratio'] = data['taker_buy_base_volume'].values / (volume + 1e-8)
        else:
            df['taker_ratio'] = np.log1p(data['trades'].values) if 'trades' in data.columns else np.zeros_like(close)

        # --- Group B: ì „ëµ íŒŒì¼ ê¸°ë°˜ í•µì‹¬ ì§€í‘œ (10ê°œ) ---
        
        # 6. RSI (14)
        rsi = Indicators.calculate_rsi(data, period=14)
        if rsi is not None:
            if isinstance(rsi, pd.DataFrame):
                df['rsi'] = rsi.iloc[:, 0].values
            else:
                df['rsi'] = rsi.values
        else:
            df['rsi'] = np.zeros_like(close)

        # 7. MACD Histogram
        macd = Indicators.calculate_macd(data)
        if macd is not None and 'histogram' in macd:
            df['macd_hist'] = macd['histogram'].values
        else:
            df['macd_hist'] = np.zeros_like(close)

        # 8, 9. Bollinger Bands (Width, Position)
        bb = Indicators.calculate_bollinger_bands(data, period=20)
        if bb is not None:
            u = bb['upper'].values if isinstance(bb['upper'], pd.Series) else bb['upper']
            l = bb['lower'].values if isinstance(bb['lower'], pd.Series) else bb['lower']
            m = bb['middle'].values if isinstance(bb['middle'], pd.Series) else bb['middle']
            df['bb_width'] = (u - l) / (m + 1e-8)
            df['bb_position'] = (close - l) / (u - l + 1e-8)
        else:
            df['bb_width'] = np.zeros_like(close)
            df['bb_position'] = np.zeros_like(close)
        
        # 10. Stoch RSI K
        stoch = Indicators.calculate_stoch_rsi(data)
        if stoch is not None and 'k' in stoch:
            df['stoch_rsi'] = stoch['k'].values
        else:
            df['stoch_rsi'] = np.zeros_like(close)
        
        # 11. MFI (ìê¸ˆ íë¦„) - Indicatorsì— ì—†ìœ¼ë©´ ê³„ì‚°
        try:
            # MFIëŠ” Typical Priceì™€ Money Flow ê¸°ë°˜
            tp = (high + low + close) / 3
            money_flow = tp * volume
            positive_mf = np.where(tp > np.roll(tp, 1), money_flow, 0)
            negative_mf = np.where(tp < np.roll(tp, 1), money_flow, 0)
            positive_mf[0] = 0
            negative_mf[0] = 0
            
            # 14ê¸°ê°„ ë¡¤ë§ í•©
            period = 14
            pos_sum = pd.Series(positive_mf).rolling(period).sum().values
            neg_sum = pd.Series(negative_mf).rolling(period).sum().values
            money_ratio = pos_sum / (neg_sum + 1e-8)
            df['mfi'] = 100 - (100 / (1 + money_ratio))
        except:
            df['mfi'] = np.zeros_like(close)
        
        # 12. CMF (ë§¤ì§‘/ë¶„ì‚°) - Indicatorsì— ì—†ìœ¼ë©´ ê³„ì‚°
        try:
            # CMF = ((Close - Low) - (High - Close)) / (High - Low) * Volume
            mf_mult = ((close - low) - (high - close)) / ((high - low) + 1e-8)
            mf_vol = mf_mult * volume
            period = 20
            cmf = pd.Series(mf_vol).rolling(period).sum().values / (pd.Series(volume).rolling(period).sum().values + 1e-8)
            df['cmf'] = cmf
        except:
            df['cmf'] = np.zeros_like(close)
        
        # 13. HMA Ratio (ê´´ë¦¬ìœ¨)
        hma = Indicators.calculate_hma(data, period=14)
        if hma is not None:
            hma_val = hma.iloc[:, 0].values if isinstance(hma, pd.DataFrame) else hma.values
            df['hma_ratio'] = (close - hma_val) / (hma_val + 1e-8)
        else:
            df['hma_ratio'] = np.zeros_like(close)
            
        # 14. VWAP Deviation (ì´ê²©ë„)
        vwap = Indicators.calculate_vwap(data)
        if vwap is not None:
            vwap_val = vwap.iloc[:, 0].values if isinstance(vwap, pd.DataFrame) else vwap.values
            df['vwap_dist'] = (close - vwap_val) / (vwap_val + 1e-8)
        else:
            df['vwap_dist'] = np.zeros_like(close)
            
        # 15. ATR Ratio (ë³€ë™ì„± ë¹„ìœ¨)
        atr = Indicators.calculate_atr(data, period=14)
        if atr is not None:
            atr_val = atr.iloc[:, 0].values if isinstance(atr, pd.DataFrame) else atr.values
            df['atr_ratio'] = atr_val / (close + 1e-8)
        else:
            df['atr_ratio'] = np.zeros_like(close)
        
        # 16. ADX (ì¶”ì„¸ ê°•ë„ ì§€í‘œ) - ì‹œì¥ì˜ ì„±ê²©ì„ ê·œì •í•˜ëŠ” í•µì‹¬ ì§€í‘œ
        try:
            # TR (True Range) ê³„ì‚°
            tr1 = np.abs(high - low)
            tr2 = np.abs(high - np.roll(close, 1))
            tr3 = np.abs(low - np.roll(close, 1))
            tr = np.maximum(tr1, np.maximum(tr2, tr3))
            
            # DM (Directional Movement) ê³„ì‚°
            up_move = high - np.roll(high, 1)
            down_move = np.roll(low, 1) - low
            
            plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)
            minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)
            
            # Smoothing (14 period)
            alpha = 1/14
            
            # Pandas Seriesë¡œ ë³€í™˜í•˜ì—¬ ewm ì‚¬ìš© (êµ¬í˜„ í¸ì˜ì„±)
            tr_s = pd.Series(tr).ewm(alpha=alpha, adjust=False).mean()
            plus_dm_s = pd.Series(plus_dm).ewm(alpha=alpha, adjust=False).mean()
            minus_dm_s = pd.Series(minus_dm).ewm(alpha=alpha, adjust=False).mean()
            
            plus_di = 100 * (plus_dm_s / (tr_s + 1e-8))
            minus_di = 100 * (minus_dm_s / (tr_s + 1e-8))
            
            dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)
            adx = dx.ewm(alpha=alpha, adjust=False).mean().values
            
            # ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
            df['adx'] = np.nan_to_num(adx, nan=0.0, posinf=0.0, neginf=0.0)
            
            # [ì¤‘ìš”] Choppiness Index (íš¡ë³´ì¥ íŒë³„ê¸°)ë„ ì¶”ê°€
            # 0ì— ê°€ê¹Œìš°ë©´ ì¶”ì„¸, 100ì— ê°€ê¹Œìš°ë©´ íš¡ë³´
            high_14 = pd.Series(high).rolling(14).max()
            low_14 = pd.Series(low).rolling(14).min()
            atr_14 = pd.Series(tr).rolling(14).sum()
            chop = 100 * np.log10(atr_14 / (high_14 - low_14 + 1e-8)) / np.log10(14)
            df['chop'] = np.nan_to_num(chop.values, nan=50.0, posinf=50.0, neginf=50.0)  # NaNì€ ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´
            
        except Exception as e:
            logger.error(f"ADX/Chop ê³„ì‚° ì‹¤íŒ¨: {e}")
            df['adx'] = np.zeros_like(close)
            df['chop'] = np.full_like(close, 50.0)

        # NaN/Inf ì²˜ë¦¬ (0ìœ¼ë¡œ ì±„ì›€)
        df = df.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        # configì— ì •ì˜ëœ ê¸°ìˆ ì  í”¼ì²˜ë§Œ ë°˜í™˜
        final_cols = [c for c in config.TECHNICAL_FEATURES if c in df.columns]
        if len(final_cols) != len(config.TECHNICAL_FEATURES):
            missing = set(config.TECHNICAL_FEATURES) - set(final_cols)
            logger.warning(f"ëˆ„ë½ëœ ê¸°ìˆ ì  í”¼ì²˜: {missing}")
        
        return df[final_cols] if final_cols else df

    except Exception as e:
        logger.error(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return None


def precalculate_strategy_scores(collector, force_recalculate=False):
    """
    ëª¨ë“  ì „ëµì„ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ë¯¸ë¦¬ ì‹¤í–‰í•˜ì—¬ ì ìˆ˜(Score)ë¥¼ ê³„ì‚°
    Long ì‹ í˜¸: +Confidence, Short ì‹ í˜¸: -Confidence, None: 0
    
    Args:
        collector: DataCollector ì¸ìŠ¤í„´ìŠ¤
        force_recalculate: Trueë©´ ì €ì¥ëœ íŒŒì¼ì„ ë¬´ì‹œí•˜ê³  ë‹¤ì‹œ ê³„ì‚°
    """
    strategy_scores_path = 'data/strategy_scores.csv'
    
    # ì €ì¥ëœ íŒŒì¼ì´ ìˆê³  ì¬ê³„ì‚°ì´ ì•„ë‹ˆë©´ ë¡œë“œ
    if not force_recalculate and os.path.exists(strategy_scores_path):
        try:
            logger.info(f"ğŸ“‚ ì €ì¥ëœ ì „ëµ ì ìˆ˜ ë¡œë“œ ì¤‘: {strategy_scores_path}")
            scores_df = pd.read_csv(strategy_scores_path, index_col=0, parse_dates=True)
            
            # ë°ì´í„° ê¸¸ì´ í™•ì¸
            if len(scores_df) == len(collector.eth_data):
                logger.info(f"âœ… ì €ì¥ëœ ì „ëµ ì ìˆ˜ ë¡œë“œ ì™„ë£Œ: {len(scores_df)}ê°œ ìº”ë“¤")
                return scores_df
            else:
                logger.warning(f"ì €ì¥ëœ íŒŒì¼ ê¸¸ì´ ë¶ˆì¼ì¹˜ ({len(scores_df)} vs {len(collector.eth_data)}), ì¬ê³„ì‚°í•©ë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"ì €ì¥ëœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}, ì¬ê³„ì‚°í•©ë‹ˆë‹¤.")
    
    # ì „ëµ ì ìˆ˜ ê³„ì‚°
    logger.info("ğŸ§  ë‚´ ì „ëµë“¤ì˜ ì‹ í˜¸ ë¯¸ë¦¬ ê³„ì‚° ì¤‘ (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    
    # ì „ëµ ì´ˆê¸°í™”
    strategies = [
        (BTCEthCorrelationStrategy(), 'strat_btc_eth_corr'),
        (VolatilitySqueezeStrategy(), 'strat_vol_squeeze'),
        (OrderblockFVGStrategy(), 'strat_ob_fvg'),
        (HMAMomentumStrategy(), 'strat_hma'),
        (MFIMomentumStrategy(), 'strat_mfi'),
        (BollingerMeanReversionStrategy(), 'strat_bb_reversion'),
        (VWAPDeviationStrategy(), 'strat_vwap'),
        (RangeTopBottomStrategy(), 'strat_range'),
        (StochRSIMeanReversionStrategy(), 'strat_stoch'),
        (CMFDivergenceStrategy(), 'strat_cmf')
    ]
    
    # ê²°ê³¼ë¥¼ ë‹´ì„ DataFrame ìƒì„± (0ìœ¼ë¡œ ì´ˆê¸°í™”)
    total_len = len(collector.eth_data)
    scores_df = pd.DataFrame(0.0, index=collector.eth_data.index, columns=[s[1] for s in strategies])
    
    # íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¸ë±ìŠ¤ ë£¨í”„ë¥¼ ëŒë©° ì‹œë®¬ë ˆì´ì…˜
    # ì „ëµì˜ analyzeëŠ” 'í˜„ì¬ ì‹œì 'ì„ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°ë¥¼ ë´„.
    # ë”°ë¼ì„œ ê³¼ê±°ë¶€í„° ë¯¸ë˜ë¡œ ìˆœíšŒí•˜ë©° collectorì˜ indexë¥¼ ë³€ê²½í•´ì¤˜ì•¼ í•¨.
    
    # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œ í•„ìš”í•œ ìµœì†Œ ë°ì´í„°:
    # - RSI(14): ìµœì†Œ 15ê°œ
    # - MACD(12,26,9): ìµœì†Œ 26+9=35ê°œ
    # - Bollinger(20): ìµœì†Œ 20ê°œ
    # - Stochastic RSI: ìµœì†Œ 14+14+3=31ê°œ
    # - HMA(14): ìµœì†Œ 14*2=28ê°œ
    # - VWAP: ì„¸ì…˜ ê¸°ì¤€ì´ë¯€ë¡œ 1ê°œë¶€í„° ê°€ëŠ¥í•˜ì§€ë§Œ ì•ˆì •ì„±ì„ ìœ„í•´ 20ê°œ
    # - ATR(14): ìµœì†Œ 15ê°œ
    # ê°€ì¥ í° ê°’ì¸ MACD ê¸°ì¤€ìœ¼ë¡œ ì—¬ìœ ë¶„ í¬í•¨: 100ê°œ
    start_idx = 100  # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ìš© ì—¬ìœ ë¶„ (MACD ë“± ìµœëŒ€ ê¸°ê°„ ê³ ë ¤)
    
    # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë£¨í”„ ì‹¤í–‰
    for i in tqdm(range(start_idx, total_len), desc="ì „ëµ ì‹ í˜¸ ê³„ì‚°"):
        collector.current_index = i
        
        for strategy, col_name in strategies:
            try:
                # ì „ëµ ì‹¤í–‰
                result = strategy.analyze(collector)
                
                if result:
                    # ì‹ í˜¸ íŒŒì‹±
                    signal = result.get('signal')
                    confidence = float(result.get('confidence', 0.5))
                    
                    # ì ìˆ˜ ë³€í™˜ (Long: +, Short: -)
                    if signal == 'LONG':
                        scores_df.iloc[i, scores_df.columns.get_loc(col_name)] = confidence
                    elif signal == 'SHORT':
                        scores_df.iloc[i, scores_df.columns.get_loc(col_name)] = -confidence
            except Exception as e:
                logger.debug(f"ì „ëµ {col_name} ì‹¤í–‰ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}): {e}")
                pass  # ì—ëŸ¬ ë‚˜ë„ ì§„í–‰
                
    logger.info(f"âœ… ì „ëµ ì‹ í˜¸ ê³„ì‚° ì™„ë£Œ: {len(scores_df)}ê°œ ìº”ë“¤")
    
    # íŒŒì¼ë¡œ ì €ì¥
    try:
        scores_df.to_csv(strategy_scores_path)
        logger.info(f"ğŸ’¾ ì „ëµ ì ìˆ˜ ì €ì¥ ì™„ë£Œ: {strategy_scores_path}")
    except Exception as e:
        logger.warning(f"ì „ëµ ì ìˆ˜ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return scores_df


class DDQNTrainer:
    def __init__(self, force_recalculate_strategies=False):
        """
        Args:
            force_recalculate_strategies: Trueë©´ ì €ì¥ëœ ì „ëµ ì ìˆ˜ë¥¼ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°
        """
        # 1. ë°ì´í„° ë¡œë“œ
        self.data_collector = DataCollector(use_saved_data=True)
        if not self.data_collector.load_saved_data():
            raise ValueError("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: collect_training_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # 1.5. MTF í”„ë¡œì„¸ì„œ ì ìš© (15ë¶„ë´‰, 1ì‹œê°„ë´‰ ì§€í‘œ ì¶”ê°€)
        # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë³€í™˜
        if not isinstance(self.data_collector.eth_data.index, pd.DatetimeIndex):
            # ì¸ë±ìŠ¤ê°€ ë¬¸ìì—´ì´ê±°ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¼ ê²½ìš° ë³€í™˜ ì‹œë„
            try:
                self.data_collector.eth_data.index = pd.to_datetime(self.data_collector.eth_data.index)
            except:
                logger.warning("ì¸ë±ìŠ¤ë¥¼ DatetimeIndexë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. MTF í”„ë¡œì„¸ì„œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            try:
                mtf_processor = MTFProcessor(self.data_collector.eth_data)
                self.data_collector.eth_data = mtf_processor.add_mtf_features()
            except Exception as e:
                logger.warning(f"MTF í”„ë¡œì„¸ì„œ ì ìš© ì‹¤íŒ¨: {e}. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
        # 2. ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ ê³„ì‚° (15ê°œ)
        logger.info("1. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...")
        tech_df = calculate_technical_features(self.data_collector.eth_data)
        
        if tech_df is None or len(tech_df) == 0:
            raise ValueError("ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
        
        # 3. ì „ëµ ì ìˆ˜ í”¼ì²˜ ê³„ì‚° (10ê°œ)
        logger.info("2. ì „ëµ ì‹ í˜¸ ê³„ì‚° ì¤‘...")
        strat_df = precalculate_strategy_scores(self.data_collector, force_recalculate=force_recalculate_strategies)
        
        # ì¸ë±ìŠ¤ ì¼ì¹˜ í™•ì¸
        if len(tech_df) != len(self.data_collector.eth_data):
            raise ValueError(f"ê¸°ìˆ ì  ì§€í‘œ ê¸¸ì´ ë¶ˆì¼ì¹˜: ì›ë³¸={len(self.data_collector.eth_data)}, ê¸°ìˆ ={len(tech_df)}")
        if len(strat_df) != len(self.data_collector.eth_data):
            raise ValueError(f"ì „ëµ ì ìˆ˜ ê¸¸ì´ ë¶ˆì¼ì¹˜: ì›ë³¸={len(self.data_collector.eth_data)}, ì „ëµ={len(strat_df)}")
        
        if not tech_df.index.equals(self.data_collector.eth_data.index):
            logger.warning("ê¸°ìˆ ì  ì§€í‘œ ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜, ì¬ì¸ë±ì‹±í•©ë‹ˆë‹¤.")
            tech_df.index = self.data_collector.eth_data.index
        if not strat_df.index.equals(self.data_collector.eth_data.index):
            logger.warning("ì „ëµ ì ìˆ˜ ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜, ì¬ì¸ë±ì‹±í•©ë‹ˆë‹¤.")
            strat_df.index = self.data_collector.eth_data.index
        
        # 4. ë°ì´í„° ë³‘í•©
        for col in tech_df.columns:
            self.data_collector.eth_data[col] = tech_df[col]
        for col in strat_df.columns:
            self.data_collector.eth_data[col] = strat_df[col]
        
        # í”¼ì²˜ ì»¬ëŸ¼ ì´ˆê¸°í™” (configì— ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ)
        initial_features = list(config.FEATURE_COLUMNS)
        
        # [ì¶”ê°€] MTF í”¼ì²˜ ìë™ ê°ì§€ ë° ì¶”ê°€ (rsi_15m, trend_15m, rsi_1h, trend_1h)
        mtf_features = ['rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h']
        for mtf_feat in mtf_features:
            if mtf_feat in self.data_collector.eth_data.columns and mtf_feat not in initial_features:
                initial_features.append(mtf_feat)
                logger.info(f"âœ… MTF í”¼ì²˜ ìë™ ì¶”ê°€: {mtf_feat}")
        
        # ëˆ„ë½ëœ ì»¬ëŸ¼ 0ìœ¼ë¡œ ì±„ìš°ê¸° (XGBoost ì—ëŸ¬ ë°©ì§€)
        for col in initial_features:
            if col not in self.data_collector.eth_data.columns:
                logger.warning(f"ëˆ„ë½ëœ í”¼ì²˜ {col}ë¥¼ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
                self.data_collector.eth_data[col] = 0.0
        
        # [ìˆ˜ì • í›„ ì½”ë“œ: XGBoost ì ìš©] -----------------------------------------
        # MTF í”¼ì²˜ í™•ì¸ ë¡œê¹…
        mtf_features_in_data = [f for f in ['rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'] 
                                if f in self.data_collector.eth_data.columns]
        if mtf_features_in_data:
            logger.info(f"ğŸ“Š MTF í”¼ì²˜ í™•ì¸: {mtf_features_in_data} (ì´ {len(mtf_features_in_data)}ê°œ)")
            logger.info(f"ğŸ“Š MTF í”¼ì²˜ ìƒ˜í”Œ ê°’: {self.data_collector.eth_data[mtf_features_in_data[0]].head(5).tolist()}")
        else:
            logger.warning("âš ï¸ MTF í”¼ì²˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if config.USE_XGBOOST_SELECTION:
            logger.info("ğŸ¤– XGBoost í”¼ì²˜ ì„ íƒ í”„ë¡œì„¸ìŠ¤ ê°€ë™...")
            logger.info(f"ğŸ“‹ í›„ë³´ í”¼ì²˜ ê°œìˆ˜: {len(initial_features)}ê°œ (MTF í¬í•¨ ì—¬ë¶€ í™•ì¸)")
            
            selector = FeatureSelector(top_k=config.TOP_K_FEATURES)
            
            # ë¯¸ë˜ 20ë´‰(1ì‹œê°„) ë’¤ì˜ ë³€ë™ì„±ì„ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” í”¼ì²˜ ì„ ì •
            selected_features = selector.select_features(
                self.data_collector.eth_data, 
                initial_features, 
                target_horizon=10 
            )
            
            # MTF í”¼ì²˜ ì„ íƒ ì—¬ë¶€ í™•ì¸
            selected_mtf = [f for f in selected_features if f in mtf_features_in_data]
            if selected_mtf:
                logger.info(f"âœ… XGBoostê°€ ì„ íƒí•œ MTF í”¼ì²˜: {selected_mtf}")
            else:
                logger.info(f"â„¹ï¸ XGBoostê°€ MTF í”¼ì²˜ë¥¼ ì„ íƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ íƒëœ í”¼ì²˜: {selected_features})")
            
            # [ì•ˆì „ì¥ì¹˜] ë§Œì•½ ì„ íƒëœ í”¼ì²˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if len(selected_features) < 3:
                logger.warning("XGBoostê°€ ì„ íƒí•œ í”¼ì²˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë³µê·€í•©ë‹ˆë‹¤.")
                self.feature_columns = initial_features
            else:
                self.feature_columns = selected_features
        else:
            self.feature_columns = initial_features
        
        # [í•µì‹¬] ë°©í–¥ì„± í•„ìˆ˜ ì§€í‘œ ê°•ì œ í¬í•¨ (Whitelist)
        # RSI(ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„), MACD(ì¶”ì„¸), BB Position(í˜„ì¬ ìœ„ì¹˜), ADX(ì¶”ì„¸ ê°•ë„), Choppiness(íš¡ë³´/ì¶”ì„¸ íŒë³„)
        must_include = ['rsi', 'macd_hist', 'bb_position', 'adx', 'chop']
        
        # MTF í”¼ì²˜ë„ ê°•ì œ í¬í•¨ (ìƒìœ„ í”„ë ˆì„ ì •ë³´ëŠ” ì¤‘ìš”)
        mtf_must_include = [f for f in ['rsi_15m', 'trend_15m', 'rsi_1h', 'trend_1h'] 
                           if f in self.data_collector.eth_data.columns]
        must_include.extend(mtf_must_include)
        
        # í•„ìˆ˜ ì§€í‘œê°€ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸ í›„ ì¶”ê°€
        for f in must_include:
            if f in self.data_collector.eth_data.columns and f not in self.feature_columns:
                self.feature_columns.append(f)
                logger.info(f"âœ… í•„ìˆ˜ ì§€í‘œ ê°•ì œ í¬í•¨: {f}")
        # ---------------------------------------------------------------------
                
        logger.info(f"âœ… ìµœì¢… ì…ë ¥ í”¼ì²˜ ({len(self.feature_columns)}ê°œ): {self.feature_columns}")
        
        # 3. í™˜ê²½ ì„¤ì • (feature_columns ì „ë‹¬)
        self.env = TradingEnvironment(
            self.data_collector, 
            strategies=[], 
            lookback=config.LOOKBACK_WINDOW,  # [ìˆ˜ì •] 20 -> 60 (3ì‹œê°„ì˜ íë¦„ì„ ë³´ê²Œ í•¨)
            selected_features=self.feature_columns
        )
        
        # 4. ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        self._fit_global_scaler()
        
        # 5. ì—ì´ì „íŠ¸ ì„¤ì •
        ddqn_config = config.DDQN_CONFIG.copy()
        ddqn_config['input_dim'] = len(self.feature_columns)  # ì°¨ì› ë™ê¸°í™”
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"í•™ìŠµ ì¥ì¹˜: {device}")
        
        self.agent = DDQNAgent(
            input_dim=ddqn_config['input_dim'],
            hidden_dim=ddqn_config['hidden_dim'],
            num_layers=ddqn_config['num_layers'],
            action_dim=ddqn_config['action_dim'],
            lr=ddqn_config['learning_rate'],
            gamma=ddqn_config['gamma'],
            epsilon_start=ddqn_config['epsilon_start'],
            epsilon_end=ddqn_config['epsilon_end'],
            epsilon_decay=ddqn_config['epsilon_decay'],
            buffer_size=ddqn_config['buffer_size'],
            batch_size=ddqn_config['batch_size'],
            target_update=ddqn_config['target_update'],
            device=device,
            use_per=config.USE_PER  # PER ì‚¬ìš© ì—¬ë¶€
        )
        
        self.episode_rewards = []
        self.total_steps = 0
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        self.prev_pnl = 0.0
        
        # ì‹œê°í™” ì´ˆê¸°í™” (ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”)
        self.visualizer = None
    
    def _fit_global_scaler(self):
        """ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (ê¸°ìˆ ì  ì§€í‘œë§Œ í•™ìŠµ!)"""
        try:
            logger.info("ì „ì—­ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘ (ì „ëµ ì ìˆ˜ ì œì™¸)...")
            
            # ê¸°ìˆ ì  ì§€í‘œ ì»¬ëŸ¼ë§Œ í•„í„°ë§ (MTF í”¼ì²˜ í¬í•¨)
            tech_cols = [f for f in self.feature_columns if not f.startswith('strat_')]
            
            # MTF í”¼ì²˜ í™•ì¸
            mtf_in_scaler = [f for f in tech_cols if '_15m' in f or '_1h' in f]
            if mtf_in_scaler:
                logger.info(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ì— í¬í•¨ëœ MTF í”¼ì²˜: {mtf_in_scaler}")
            else:
                logger.warning(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ì— MTF í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤. (ê¸°ìˆ ì  ì§€í‘œ: {tech_cols})")
            
            if not tech_cols:
                logger.warning("ê¸°ìˆ ì  ì§€í‘œê°€ ì—†ì–´ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            # [ìˆ˜ì • 1] ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ 20 -> 100ìœ¼ë¡œ ë³€ê²½ (ì´ˆê¸° NaN/0 ë°ì´í„° ë°°ì œ)
            start_idx = 100
            
            # ë°ì´í„° ê¸¸ì´ í™•ì¸
            data_len = len(self.data_collector.eth_data)
            if data_len <= start_idx:
                logger.warning("ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # [ì†ë„ ìµœì í™”] forë¬¸ ì—†ì´ pandas ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ í•œë°©ì— í•´ê²°
            # 1. ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° í†µì§¸ë¡œ ê°€ì ¸ì˜¤ê¸° (100ë²ˆ ì´í›„)
            tech_df = self.data_collector.eth_data.iloc[start_idx:][tech_cols]
            
            # 2. ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ 5ë§Œê°œë§Œ)
            if len(tech_df) > 50000:
                tech_df = tech_df.sample(n=50000, random_state=42)
            
            # 3. Numpy ë³€í™˜ ë° 0/Inf ì²˜ë¦¬
            tech_data = tech_df.values.astype(np.float32)
            tech_data = np.nan_to_num(tech_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # 4. í•™ìŠµ
            self.env.preprocessor.fit(tech_data)
            self.env.scaler_fitted = True
            
            # [ì¶”ê°€] í•™ìŠµ ì™„ë£Œëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            self.env.preprocessor.save_scaler('model/scaler.pkl')
            
            logger.info(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {len(tech_data)}ê°œ ìƒ˜í”Œ (Index {start_idx}ë¶€í„° ì‚¬ìš©), {len(tech_cols)}ê°œ ê¸°ìˆ ì  í”¼ì²˜ ì •ê·œí™” (ì „ëµ ì ìˆ˜ {len(self.feature_columns) - len(tech_cols)}ê°œ ì œì™¸)")
            
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)

    def train_episode(self, episode_num, max_steps=1000):
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ"""
        episode_reward = 0.0
        steps = 0
        
        # ëœë¤ ì‹œì‘ (ê³¼ì í•© ë°©ì§€)
        self.data_collector.reset_index(max_steps=max_steps, random_start=True)
        
        # ìƒíƒœ ì´ˆê¸°í™”
        self.current_position = None
        self.entry_price = None
        self.entry_index = None
        self.prev_pnl = 0.0
        
        available_steps = len(self.data_collector.eth_data) - self.data_collector.current_index
        actual_steps = min(max_steps, available_steps)
        
        if actual_steps <= 50:
            return None  # ë°ì´í„° ë¶€ì¡± ì‹œ ìŠ¤í‚µ
        
        logger.info(f"ì—í”¼ì†Œë“œ {episode_num} ì‹œì‘: {actual_steps}ê°œ ìŠ¤í… (ì¸ë±ìŠ¤: {self.data_collector.current_index}ë¶€í„°)")
        
        for step in range(actual_steps):
            try:
                # 1. ì¸ë±ìŠ¤ ì¦ê°€
                self.data_collector.current_index += 1
                if self.data_collector.current_index >= len(self.data_collector.eth_data):
                    break
                
                # 2. ê´€ì¸¡ (15ê°œ í”¼ì²˜)
                pos_val = 1.0 if self.current_position == 'LONG' else (-1.0 if self.current_position == 'SHORT' else 0.0)
                pnl_val = self.prev_pnl * 10
                hold_val = 0.0
                if self.entry_index:
                    hold_val = min(1.0, (self.data_collector.current_index - self.entry_index) / 160.0)
                
                pos_info = [pos_val, pnl_val, hold_val]
                state = self.env.get_observation(position_info=pos_info)
                if state is None:
                    continue
                
                # 3. í–‰ë™ ì„ íƒ
                action = self.agent.act(state, training=True)
                
                # 4. ê°€ê²© í™•ì¸
                current_price = float(self.data_collector.eth_data.iloc[self.data_collector.current_index - 1]['close'])
                
                # 5. ë³´ìƒ ê³„ì‚° ë¡œì§
                reward = 0.0
                trade_done = False
                current_pnl = 0.0
                pnl_change = 0.0
                
                # --- ë§¤ë§¤ ë¡œì§ ---
                if action == 1:  # LONG
                    if self.current_position == 'SHORT':  # ìŠ¤ìœ„ì¹­
                        pnl = (self.entry_price - current_price) / self.entry_price
                        pnl_change = pnl - self.prev_pnl
                        reward = self.env.calculate_reward(pnl, True, 0, pnl_change)
                        trade_done = True
                        self.prev_pnl = 0.0
                    
                    if self.current_position != 'LONG':
                        self.current_position = 'LONG'
                        self.entry_price = current_price
                        self.entry_index = self.data_collector.current_index
                        self.prev_pnl = 0.0
                
                elif action == 2:  # SHORT
                    if self.current_position == 'LONG':  # ìŠ¤ìœ„ì¹­
                        pnl = (current_price - self.entry_price) / self.entry_price
                        pnl_change = pnl - self.prev_pnl
                        reward = self.env.calculate_reward(pnl, True, 0, pnl_change)
                        trade_done = True
                        self.prev_pnl = 0.0
                    
                    if self.current_position != 'SHORT':
                        self.current_position = 'SHORT'
                        self.entry_price = current_price
                        self.entry_index = self.data_collector.current_index
                        self.prev_pnl = 0.0
                        
                else:  # HOLD
                    if self.current_position:
                        if self.current_position == 'LONG':
                            current_pnl = (current_price - self.entry_price) / self.entry_price
                        else:
                            current_pnl = (self.entry_price - current_price) / self.entry_price
                        pnl_change = current_pnl - self.prev_pnl
                        holding_time = self.data_collector.current_index - self.entry_index
                        reward = self.env.calculate_reward(current_pnl, False, holding_time, pnl_change)
                        self.prev_pnl = current_pnl

                # 6. ë‹¤ìŒ ìƒíƒœ
                next_state = None
                if not trade_done and step < actual_steps - 1:
                    # ì„ì‹œ ì¸ë±ìŠ¤ ì¦ê°€
                    self.data_collector.current_index += 1
                    next_state = self.env.get_observation(position_info=pos_info)
                    self.data_collector.current_index -= 1  # ë³µêµ¬
                
                done = (step == actual_steps - 1)
                
                # 7. ì €ì¥ ë° í•™ìŠµ
                self.agent.remember(state, action, reward, next_state, done)
                loss = self.agent.train_step()
                
                episode_reward += reward
                steps += 1
                self.total_steps += 1
                
                if done:
                    break
                
            except Exception as e:
                logger.error(f"Step Error: {e}", exc_info=True)
                continue
                
        return episode_reward, steps

    def train(self, num_episodes=1000, max_steps_per_episode=1000, save_interval=100, enable_visualization=False):
        """í•™ìŠµ ë©”ì¸ ë£¨í”„"""
        logger.info("=" * 60)
        logger.info(f"ğŸš€ DDQN í•™ìŠµ ì‹œì‘: {num_episodes} ì—í”¼ì†Œë“œ")
        logger.info(f"í”¼ì²˜: {len(self.feature_columns)}ê°œ ({', '.join(self.feature_columns)})")
        logger.info(f"ì‹œê°í™”: {'í™œì„±í™”' if enable_visualization and VISUALIZATION_AVAILABLE else 'ë¹„í™œì„±í™”'}")
        logger.info("=" * 60)
        
        # ì‹œê°í™” ì´ˆê¸°í™”
        if enable_visualization and VISUALIZATION_AVAILABLE:
            self.visualizer = LiveVisualizer(window_size=10, enable=True)
        else:
            self.visualizer = LiveVisualizer(window_size=10, enable=False)
        
        best_reward = float('-inf')
        
        try:
            for episode in range(1, num_episodes + 1):
                result = self.train_episode(episode, max_steps_per_episode)
                if result:
                    rw, st = result
                    self.episode_rewards.append(rw)
                    avg_rw = np.mean(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else rw
                    logger.info(f"Ep {episode}: Reward {rw:.2f} | Avg {avg_rw:.2f} | Steps {st} | Eps {self.agent.epsilon:.4f} | Buffer {len(self.agent.memory)}")
                    
                    # ì‹œê°í™” ì—…ë°ì´íŠ¸
                    if self.visualizer:
                        self.visualizer.update(rw)
                    
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                    if rw > best_reward:
                        best_reward = rw
                        self.agent.save_model(config.DDQN_MODEL_PATH)
                        logger.info(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ë³´ìƒ: {rw:.2f})")
                    
                    # ì£¼ê¸°ì  ì €ì¥
                    if episode % save_interval == 0:
                        self.agent.save_model(config.DDQN_MODEL_PATH)
                        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ (ì—í”¼ì†Œë“œ {episode})")
        except KeyboardInterrupt:
            logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
        finally:
            # ì‹œê°í™” ì°½ ë‹«ê¸°
            if self.visualizer:
                self.visualizer.close()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self.agent.save_model(config.DDQN_MODEL_PATH)
        logger.info("=" * 60)
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ")
        logger.info(f"í‰ê·  ë³´ìƒ: {np.mean(self.episode_rewards) if self.episode_rewards else 0:.4f}")
        logger.info("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='DDQN ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--episodes', type=int, default=1000, help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--steps', type=int, default=1000, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--save-interval', type=int, default=100, help='ëª¨ë¸ ì €ì¥ ê°„ê²© (ì—í”¼ì†Œë“œ)')
    parser.add_argument('--visualize', action='store_true', help='ë³´ìƒ ê·¸ë˜í”„ ì‹œê°í™” í™œì„±í™”')
    parser.add_argument('--no-visualize', action='store_true', help='ë³´ìƒ ê·¸ë˜í”„ ì‹œê°í™” ë¹„í™œì„±í™” (ê¸°ë³¸ê°’)')
    parser.add_argument('--recalculate-strategies', action='store_true', help='ì €ì¥ëœ ì „ëµ ì ìˆ˜ë¥¼ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°')
    
    args = parser.parse_args()
    
    # ì‹œê°í™” ì˜µì…˜ ê²°ì •
    enable_viz = args.visualize and not args.no_visualize
    
    try:
        trainer = DDQNTrainer(force_recalculate_strategies=args.recalculate_strategies)
        trainer.train(
            num_episodes=args.episodes,
            max_steps_per_episode=args.steps,
            save_interval=args.save_interval,
            enable_visualization=enable_viz
        )
    except KeyboardInterrupt:
        logger.info("í•™ìŠµ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        import traceback
        traceback.print_exc()
