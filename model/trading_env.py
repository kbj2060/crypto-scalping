"""
ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© í™˜ê²½
ê¸°ì¡´ ì „ëµ Scoreì™€ ì‹œì¥ ë°ì´í„°ë¥¼ ê²°í•©í•˜ëŠ” í™˜ê²½ ì¸í„°í˜ì´ìŠ¤
ì›ì‹œ ë°ì´í„° ë³´ì¡´ + Z-Score ì •ê·œí™”
ë³€ë™ì„± ê¸°ë°˜ ë³´ìƒ ì‹œìŠ¤í…œ (Risk-Adjusted Reward)
"""
import numpy as np
import torch
import logging
from collections import deque
from .preprocess import DataPreprocessor

logger = logging.getLogger(__name__)


class TradingEnvironment:
    """íŠ¸ë ˆì´ë”© í™˜ê²½: ìƒíƒœ ê´€ì¸¡ ë° ë³€ë™ì„± ê¸°ë°˜ ë³´ìƒ ê³„ì‚°"""
    def __init__(self, data_collector, strategies, lookback=40):
        """
        Args:
            data_collector: DataCollector ì¸ìŠ¤í„´ìŠ¤
            strategies: ì „ëµ ë¦¬ìŠ¤íŠ¸
            lookback: ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ 40)
        """
        self.collector = data_collector
        self.strategies = strategies
        self.num_strategies = len(strategies)
        self.lookback = lookback
        
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Z-Score ì •ê·œí™”)
        self.preprocessor = DataPreprocessor()
        self.scaler_fitted = False  # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì—¬ë¶€
        
        # [ì¶”ê°€] ìµœê·¼ pnl_change ë‚´ì—­ì„ ì €ì¥í•˜ì—¬ ë³€ë™ì„± ê³„ì‚° (ìµœê·¼ 100ìŠ¤í…)
        self.pnl_change_history = deque(maxlen=100)

    def get_observation(self, position_info=None):
        """
        í˜„ì¬ ìƒíƒœ ê´€ì¸¡ (8ê°œ í•µì‹¬ ì‹œê³„ì—´ í”¼ì²˜ + Z-Score ì •ê·œí™” + í¬ì§€ì…˜ ì •ë³´)
        
        Args:
            position_info: [í¬ì§€ì…˜(1/0/-1), ë¯¸ì‹¤í˜„PnL, ë³´ìœ ì‹œê°„(ì •ê·œí™”)] ë¦¬ìŠ¤íŠ¸
                          - ë³´ìœ ì‹œê°„: (current_index - entry_index) / max_steps (0~1 ì‚¬ì´)
                          Noneì´ë©´ [0.0, 0.0, 0.0]ìœ¼ë¡œ ì²˜ë¦¬
        
        Returns:
            (obs_seq, obs_info): íŠœí”Œ
                - obs_seq: (1, 20, 8) í…ì„œ - 8ê°œ ì‹œê³„ì—´ í”¼ì²˜ (VWAP ì´ê²©ë„ í¬í•¨)
                - obs_info: (1, 13) í…ì„œ - ì „ëµ ì ìˆ˜(10) + í¬ì§€ì…˜ ì •ë³´(3)
        """
        try:
            # 1. ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ (ë§ˆì§€ë§‰ 20ë´‰)
            candles = self.collector.get_candles('ETH', count=20)
            if candles is None or len(candles) < 20:
                logger.warning(f"ë°ì´í„° ë¶€ì¡±: {len(candles) if candles is not None else 0}ê°œ (í•„ìš”: 20ê°œ)")
                return None
            
            close = candles['close'].values.astype(np.float32)
            high = candles['high'].values.astype(np.float32)
            low = candles['low'].values.astype(np.float32)
            volume = candles['volume'].values.astype(np.float32)
            
            # [ì¶”ê°€] VWAP ê³„ì‚° (í˜„ì¬ ìœˆë„ìš° 20ê°œ ê¸°ì¤€ Rolling VWAP)
            # ê³µì‹: Sum(Price * Volume) / Sum(Volume)
            tp = (high + low + close) / 3  # Typical Price
            vp = tp * volume
            # np.cumsumì„ ì‚¬ìš©í•˜ì—¬ ìœˆë„ìš° ë‚´ì—ì„œì˜ ëˆ„ì  VWAP íë¦„ì„ ìƒì„±
            cumulative_vp = np.cumsum(vp)
            cumulative_vol = np.cumsum(volume)
            # ê±°ë˜ëŸ‰ì´ 0ì¸ êµ¬ê°„ì´ ê¸¸ì–´ì§€ë©´ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ì¥ì¹˜ ì¶”ê°€
            vwap = cumulative_vp / (cumulative_vol + 1e-8)
            
            # VWAP NaN ì²´í¬ (ê±°ë˜ëŸ‰ì´ ëª¨ë‘ 0ì¸ ê²½ìš° ëŒ€ë¹„)
            if np.isnan(vwap).any() or np.isinf(vwap).any():
                logger.warning("VWAP ê³„ì‚° ì¤‘ NaN/Inf ë°œìƒ, close ê°’ìœ¼ë¡œ ëŒ€ì²´")
                vwap = np.where(np.isnan(vwap) | np.isinf(vwap), close, vwap)
            
            # 2. 8ê°œ ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± (ì°¨ì›: 20x8)
            # [ìµœì í™”] Volumeê³¼ Tradesì— ë¡œê·¸ ë³€í™˜ ì ìš© (ê±°ë˜ëŸ‰ í­ë°œ êµ¬ê°„ì˜ ê·¹ë‹¨ì  ì°¨ì´ ì™„í™”)
            # log1pëŠ” ìŒìˆ˜ ì…ë ¥ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš© (log(1+x))
            volume_log = np.log1p(np.maximum(volume, 0))  # ìŒìˆ˜ ë°©ì§€
            trades_raw = candles['trades'].values.astype(np.float32) if 'trades' in candles.columns else np.zeros(20, dtype=np.float32)
            trades_log = np.log1p(np.maximum(trades_raw, 0))  # ìŒìˆ˜ ë°©ì§€
            
            seq_features = np.column_stack([
                (candles['open'].values - close) / (close + 1e-8),  # f1: Open (close ëŒ€ë¹„)
                (high - close) / (close + 1e-8),                    # f2: High (close ëŒ€ë¹„)
                (low - close) / (close + 1e-8),                     # f3: Low (close ëŒ€ë¹„)
                np.diff(np.log(close + 1e-8), prepend=np.log(close[0] + 1e-8)),  # f4: Log_Return
                volume_log,                                         # f5: Volume (ë¡œê·¸ ë³€í™˜ í›„ Z-Score)
                trades_log,                                         # f6: Trades (ë¡œê·¸ ë³€í™˜ í›„ Z-Score)
                candles['taker_buy_base'].values / (volume + 1e-8), # f7: Taker_Ratio
                (close - vwap) / (vwap + 1e-8)                      # [NEW] f8: VWAP Deviation (ì´ê²©ë„)
            ])
            
            # 3. ì „ì²˜ë¦¬ (8ê°œ ì°¨ì›ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨)
            if not self.scaler_fitted:
                logger.warning("ìŠ¤ì¼€ì¼ëŸ¬ê°€ fitë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. transformë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            
            normalized_seq = self.preprocessor.transform(seq_features)
            obs_seq = torch.FloatTensor(normalized_seq).unsqueeze(0)  # (1, 20, 8)
            
            # 4. ê¸°ìˆ ì  ì „ëµ Score ìˆ˜ì§‘ (LONG/SHORT ë¶€í˜¸ ì¸ì½”ë”© ë°˜ì˜)
            strategy_scores = []
            for strategy in self.strategies:
                try:
                    result = strategy.analyze(self.collector)
                    if result and 'confidence' in result:
                        score = float(result['confidence'])
                        # SHORT ì‹ í˜¸ëŠ” ìŒìˆ˜ë¡œ ì¸ì½”ë”©
                        if result.get('signal') == 'SHORT':
                            score = -score
                        strategy_scores.append(score)
                    else:
                        strategy_scores.append(0.0)
                except Exception as e:
                    logger.debug(f"ì „ëµ {strategy.name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    strategy_scores.append(0.0)
            
            # 5. [í•´ê²°] 10ê°œ ì „ëµ ì ìˆ˜ + 3ê°œ í¬ì§€ì…˜ ì •ë³´ = 13ì°¨ì›
            if position_info is None:
                position_info = [0.0, 0.0, 0.0]
            
            obs_info = np.concatenate([strategy_scores, position_info], dtype=np.float32)
            obs_info_tensor = torch.FloatTensor(obs_info).unsqueeze(0)  # (1, 13)
            
            # [ê¸´ê¸‰ ì ê²€] NaN/Inf ì²´í¬
            if torch.isnan(obs_seq).any() or torch.isinf(obs_seq).any():
                nan_count = torch.isnan(obs_seq).sum().item()
                inf_count = torch.isinf(obs_seq).sum().item()
                logger.error("ğŸš¨ ì‹œê³„ì—´ ë°ì´í„°ì— NaN ë˜ëŠ” Inf ë°œìƒ!")
                logger.error(f"   NaN ê°œìˆ˜: {nan_count}, Inf ê°œìˆ˜: {inf_count}")
                logger.error(f"   ì‹œê³„ì—´ ë°ì´í„° ìƒ˜í”Œ: {obs_seq[0, :5, :] if obs_seq.shape[0] > 0 else 'N/A'}")
                return None  # í•™ìŠµ ë°©ì§€
            
            if torch.isnan(obs_info_tensor).any() or torch.isinf(obs_info_tensor).any():
                nan_count = torch.isnan(obs_info_tensor).sum().item()
                inf_count = torch.isinf(obs_info_tensor).sum().item()
                logger.error("ğŸš¨ ì •ë³´ ë°ì´í„°ì— NaN ë˜ëŠ” Inf ë°œìƒ!")
                logger.error(f"   NaN ê°œìˆ˜: {nan_count}, Inf ê°œìˆ˜: {inf_count}")
                logger.error(f"   ì •ë³´ ë°ì´í„°: {obs_info_tensor}")
                return None  # í•™ìŠµ ë°©ì§€
            
            return (obs_seq, obs_info_tensor)
            
        except Exception as e:
            logger.error(f"ê´€ì¸¡ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return None

    def calculate_reward(self, pnl, trade_done, holding_time=0, pnl_change=0):
        """
        ë³´ìƒ í•¨ìˆ˜ ê°œì„ : ì´ˆê¸° í•™ìŠµ ìœ ë„ë¥¼ ìœ„í•œ ì„ í˜•ì ì´ê³  ëŒ€ì¹­ì ì¸ ë³´ìƒ êµ¬ì¡°
        
        Args:
            pnl: ì†ìµ (ìˆ˜ìµë¥ )
            trade_done: ê±°ë˜ ì™„ë£Œ ì—¬ë¶€
            holding_time: ë³´ìœ  ì‹œê°„ (ë¶„)
            pnl_change: ì´ì „ ìŠ¤í… ëŒ€ë¹„ ìˆ˜ìµë¥ ì˜ ë³€í™”
        Returns:
            reward: ë³´ìƒê°’ (í´ë¦¬í•‘: -10 ~ +10)
        """
        # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ íŒ©í„° (100~300 ì¶”ì²œ)
        # ì½”ì¸ ì‹œì¥ì˜ ë†’ì€ ë³€ë™ì„±ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¡°ì •
        scaling_factor = 300  # ê¸°ì¡´ 1000ì—ì„œ í•˜í–¥ ì¡°ì •
        
        # 1. ê¸°ë³¸ ë³´ìƒ (ìˆ˜ìµë¥  ë³€í™”ëŸ‰)
        # ë°°ìœ¨ì„ ë‚®ì¶°ì„œ ì‹ ê²½ë§ì´ 'ë¯¸ì„¸í•œ ì°¨ì´'ë¥¼ í•™ìŠµí•˜ê²Œ ìœ ë„
        reward = pnl_change * scaling_factor
        
        # 2. ê±°ë˜ ì™„ë£Œ ì‹œ ë³´ìƒ (Trade Done)
        if trade_done:
            # [í•µì‹¬ ë³€ê²½] ë¹„ì„ í˜•(ì œê³±) ì œê±° -> ì„ í˜•(Linear) ë³´ìƒìœ¼ë¡œ ë³€ê²½
            # ì†ì‹¤ í˜ë„í‹°ë¥¼ ìˆ˜ìµ ë³´ìƒê³¼ 1:1 ëŒ€ì¹­ìœ¼ë¡œ ë§ì¶¤ (Risk:Reward = 1:1)
            step_reward = pnl * scaling_factor
            
            # ìˆ˜ìˆ˜ë£Œ í˜ë„í‹° ì™„í™” (-0.05 -> -0.01)
            # ë„ˆë¬´ ë†’ìœ¼ë©´ ì§„ì… ìì²´ë¥¼ êº¼ë¦¬ê²Œ ë¨
            reward += step_reward - 0.01
            
            # [ì¶”ê°€] í° ìˆ˜ìµì— ëŒ€í•œ ì¶”ê°€ ì¸ì„¼í‹°ë¸Œ (ì­íŒŸ ë³´ìƒ)
            if pnl > 0.01:  # 1% ì´ìƒ ìˆ˜ìµ ì‹œ
                reward += 1.0
        
        # 3. ì‹œê°„ í˜ë„í‹° (ìœ ì§€)
        # ë„ˆë¬´ ì˜¤ë˜ ë“¤ê³  ìˆëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•„ì£¼ ì‘ê²Œ ìœ ì§€
        reward -= 0.001
        
        # 4. [ì¤‘ìš”] ë¬´í¬ì§€ì…˜ ê¸°íšŒë¹„ìš© í˜ë„í‹° ì œê±°
        # ì´ˆê¸°ì—ëŠ” "ê´€ë§"ë„ í›Œë¥­í•œ ì „ëµì„ì„ ì¸ì •í•´ì•¼ í•¨. ì–µì§€ë¡œ ì§„ì…ì‹œí‚¤ë©´ ì†ì‹¤ë§Œ ì»¤ì§.
        # if not trade_done and holding_time == 0:
        #    reward -= 0.005 

        # 5. ë³´ìƒ í´ë¦¬í•‘ (Reward Clipping)
        # [ìˆ˜ì •] ë³´ìƒ í´ë¦¬í•‘ ë²”ìœ„ í™•ì¥ (-10, 10 -> -100, 100)
        # ì´ì œ ëŒ€ë°• ìˆ˜ìµì„ ë‚´ë©´ í™•ì‹¤í•˜ê²Œ í° ë³´ìƒì„ ì¤ë‹ˆë‹¤.
        reward = np.clip(reward, -100, 100)

        return reward

    def get_state_dim(self):
        """ìƒíƒœ ì°¨ì› ë°˜í™˜ (8ê°œ ì‹œê³„ì—´ í”¼ì²˜)"""
        return 8  # 8ê°œ í•µì‹¬ ì‹œê³„ì—´ í”¼ì²˜ (VWAP ì´ê²©ë„ í¬í•¨)
