"""
1ë…„ì¹˜ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ë°”ì´ë‚¸ìŠ¤ì—ì„œ 1ë…„ì¹˜ ê³¼ê±° ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ data í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
"""
import os
import sys
import pandas as pd
import logging
from datetime import datetime, timedelta
import time

# ìƒìœ„ í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€ (config, core ëª¨ë“ˆ ì ‘ê·¼ìš©)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
from core.binance_client import BinanceClient

# ë¡œê¹… ì„¤ì •
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/collect_data.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def fetch_historical_klines(client, symbol, interval, start_time, end_time):
    """íŠ¹ì • ê¸°ê°„ì˜ ìº”ë“¤ ë°ì´í„° ì¡°íšŒ (ë°”ì´ë‚¸ìŠ¤ API ì œí•œ ê³ ë ¤)"""
    all_klines = []
    
    # ë°€ë¦¬ì´ˆ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜
    start_timestamp = int(start_time.timestamp() * 1000)
    end_timestamp = int(end_time.timestamp() * 1000)
    
    # 1ë…„ì¹˜ ë°ì´í„°ëŠ” ì•½ 175,200ë´‰ (3ë¶„ë´‰ ê¸°ì¤€)
    # í•œ ë²ˆì— 1000ë´‰ì”© ê°€ì ¸ì˜¤ë¯€ë¡œ ì•½ 176ë²ˆ í˜¸ì¶œ í•„ìš”
    # ì—­ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìµœì‹ ë¶€í„° ê³¼ê±°ë¡œ)
    current_end = end_timestamp
    batch_count = 0
    max_batches = 200  # ì•ˆì „ì¥ì¹˜
    
    logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_time.strftime('%Y-%m-%d')} ~ {end_time.strftime('%Y-%m-%d')}")
    
    while current_end > start_timestamp and batch_count < max_batches:
        try:
            batch_count += 1
            
            # í•œ ë²ˆì— ìµœëŒ€ 1000ë´‰ ì¡°íšŒ
            if client.use_futures:
                # ì„ ë¬¼ ê±°ë˜: endTimeì„ ì‚¬ìš©í•˜ì—¬ ì—­ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                klines = client.client.futures_klines(
                    symbol=symbol,
                    interval=interval,
                    endTime=current_end,
                    limit=1000
                )
            else:
                # ìŠ¤íŒŸ ê±°ë˜: get_historical_klines ì‚¬ìš©
                current_end_dt = datetime.fromtimestamp(current_end / 1000)
                start_dt = datetime.fromtimestamp(start_timestamp / 1000)
                klines = client.client.get_historical_klines(
                    symbol=symbol,
                    interval=interval,
                    start_str=start_dt.strftime('%d %b %Y %H:%M:%S'),
                    end_str=current_end_dt.strftime('%d %b %Y %H:%M:%S'),
                    limit=1000
                )
            
            if not klines or len(klines) == 0:
                logger.warning("ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ í•„í„°ë§ (í•„ìš”í•œ ê¸°ê°„ë§Œ)
            filtered_klines = []
            for k in klines:
                k_time = int(k[0])  # open time
                if start_timestamp <= k_time <= end_timestamp:
                    filtered_klines.append(k)
            
            if not filtered_klines:
                logger.warning("í•„í„°ë§ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            all_klines.extend(filtered_klines)
            
            # ê°€ì¥ ì˜¤ë˜ëœ ìº”ë“¤ì˜ ì‹œê°„ì„ ë‹¤ìŒ endTimeìœ¼ë¡œ ì„¤ì • (ì—­ìˆœ)
            oldest_time = min(int(k[0]) for k in filtered_klines)
            current_end = oldest_time - 1
            
            oldest_dt = datetime.fromtimestamp(oldest_time / 1000)
            logger.info(f"  ë°°ì¹˜ {batch_count}: {len(filtered_klines)}ê°œ ìˆ˜ì§‘, ì´ {len(all_klines)}ê°œ (ê°€ì¥ ì˜¤ë˜ëœ: {oldest_dt.strftime('%Y-%m-%d %H:%M:%S')})")
            
            # API ì œí•œ ë°©ì§€
            time.sleep(0.2)
                
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            time.sleep(1)
            continue
    
    # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    all_klines.sort(key=lambda x: int(x[0]))
    
    logger.info(f"ì´ {len(all_klines)}ê°œ ìº”ë“¤ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_klines


def collect_one_year_data():
    """1ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"""
    logger.info("=" * 60)
    logger.info("ğŸ“¥ 1ë…„ì¹˜ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 60)
    
    # data í´ë” ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = BinanceClient()
    
    # 1ë…„ ì „ë¶€í„° í˜„ì¬ê¹Œì§€
    end_time = datetime.now()
    start_time = end_time - timedelta(days=365)
    
    logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')} ~ {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"íƒ€ì„í”„ë ˆì„: {config.TIMEFRAME}")
    
    # ETH ë°ì´í„° ìˆ˜ì§‘
    logger.info("")
    logger.info("ETH ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    eth_klines = fetch_historical_klines(
        client,
        config.ETH_SYMBOL,
        config.TIMEFRAME,
        start_time,
        end_time
    )
    
    if eth_klines:
        eth_df = pd.DataFrame(eth_klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])
        
        # ë°ì´í„° íƒ€ì… ë³€í™˜
        eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'], unit='ms')
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                          'quote_volume', 'taker_buy_base', 'taker_buy_quote']
        for col in numeric_columns:
            eth_df[col] = pd.to_numeric(eth_df[col], errors='coerce')
        
        eth_df.set_index('timestamp', inplace=True)
        eth_df.sort_index(inplace=True)
        
        # ì¤‘ë³µ ì œê±°
        eth_df = eth_df[~eth_df.index.duplicated(keep='last')]
        
        # CSV ì €ì¥
        eth_file = f'data/eth_{config.TIMEFRAME}_1year.csv'
        eth_df.to_csv(eth_file)
        logger.info(f"âœ… ETH ë°ì´í„° ì €ì¥ ì™„ë£Œ: {eth_file} ({len(eth_df)}ê°œ ìº”ë“¤)")
    else:
        logger.error("âŒ ETH ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return False
    
    # BTC ë°ì´í„° ìˆ˜ì§‘
    logger.info("")
    logger.info("BTC ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    btc_klines = fetch_historical_klines(
        client,
        config.BTC_SYMBOL,
        config.TIMEFRAME,
        start_time,
        end_time
    )
    
    if btc_klines:
        btc_df = pd.DataFrame(btc_klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])
        
        # ë°ì´í„° íƒ€ì… ë³€í™˜
        btc_df['timestamp'] = pd.to_datetime(btc_df['timestamp'], unit='ms')
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                          'quote_volume', 'taker_buy_base', 'taker_buy_quote']
        for col in numeric_columns:
            btc_df[col] = pd.to_numeric(btc_df[col], errors='coerce')
        
        btc_df.set_index('timestamp', inplace=True)
        btc_df.sort_index(inplace=True)
        
        # ì¤‘ë³µ ì œê±°
        btc_df = btc_df[~btc_df.index.duplicated(keep='last')]
        
        # CSV ì €ì¥
        btc_file = f'data/btc_{config.TIMEFRAME}_1year.csv'
        btc_df.to_csv(btc_file)
        logger.info(f"âœ… BTC ë°ì´í„° ì €ì¥ ì™„ë£Œ: {btc_file} ({len(btc_df)}ê°œ ìº”ë“¤)")
    else:
        logger.error("âŒ BTC ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return False
    
    logger.info("")
    logger.info("=" * 60)
    logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    logger.info(f"   ETH: {len(eth_df)}ê°œ ìº”ë“¤")
    logger.info(f"   BTC: {len(btc_df)}ê°œ ìº”ë“¤")
    logger.info(f"   ì €ì¥ ìœ„ì¹˜: data/ í´ë”")
    logger.info("=" * 60)
    
    return True


if __name__ == '__main__':
    try:
        success = collect_one_year_data()
        if success:
            logger.info("ì´ì œ model/train_ppo.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
    except KeyboardInterrupt:
        logger.info("ë°ì´í„° ìˆ˜ì§‘ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
