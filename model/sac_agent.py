"""
Continuous SAC Agent
- Uses Gaussian Policy (Reparameterization Trick)
- Automatic Entropy Tuning with target_entropy = -action_dim
"""
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
import numpy as np
import random
from collections import deque
import sys
import os
import logging

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from .sac_network import SACActor, SACCritic

logger = logging.getLogger(__name__)


class ReplayBuffer:
    """Experience Replay Buffer for Continuous SAC"""
    def __init__(self, capacity, device):
        self.buffer = deque(maxlen=capacity)
        self.device = device

    def push(self, state, action, reward, next_state, done):
        """action: Continuous array"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """ë°°ì¹˜ ìƒ˜í”Œë§"""
        batch = random.sample(self.buffer, batch_size)
        
        # State Unpacking (seq, info)
        obs_seq, obs_info = zip(*[b[0] for b in batch])
        # Action is Continuous FloatTensor
        actions = torch.FloatTensor(np.array([b[1] for b in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([b[2] for b in batch])).unsqueeze(1).to(self.device)
        
        # next_stateê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
        next_states = []
        for b in batch:
            if b[3] is None:
                # next_stateê°€ Noneì´ë©´ í˜„ì¬ state ì‚¬ìš©
                next_states.append(b[0])
            else:
                next_states.append(b[3])
        
        next_obs_seq, next_obs_info = zip(*next_states)
        dones = torch.FloatTensor(np.array([b[4] for b in batch])).unsqueeze(1).to(self.device)

        # Tensor ë³€í™˜
        obs_seq = torch.cat(obs_seq, dim=0).to(self.device)
        obs_info = torch.cat(obs_info, dim=0).to(self.device)
        next_obs_seq = torch.cat(next_obs_seq, dim=0).to(self.device)
        next_obs_info = torch.cat(next_obs_info, dim=0).to(self.device)

        return (obs_seq, obs_info), actions, rewards, (next_obs_seq, next_obs_info), dones
        
    def __len__(self):
        return len(self.buffer)


class SACAgent:
    """Soft Actor-Critic Agent (Continuous)"""
    def __init__(self, state_dim, action_dim, info_dim=13, hidden_dim=None, device='cpu'):
        """
        Args:
            state_dim: ì‹œê³„ì—´ í”¼ì²˜ ì°¨ì› (29)
            action_dim: í–‰ë™ ì°¨ì› (1: ì—°ì†í˜• ë§¤ìˆ˜/ë§¤ë„ ê°•ë„)
            info_dim: í¬ì§€ì…˜ ì •ë³´ ì°¨ì› (ì „ëµ ì ìˆ˜ + í¬ì§€ì…˜ ì •ë³´)
            hidden_dim: Hidden dimension (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
            device: 'cuda' or 'cpu'
        """
        self.device = device
        # Continuous Action Dim (ë³´í†µ 1 ë˜ëŠ” 3)
        self.action_dim = action_dim 
        
        self.gamma = config.SAC_GAMMA
        self.tau = config.SAC_TAU
        self.alpha = config.SAC_ALPHA
        
        # Hidden Dimë„ Config ì‚¬ìš© (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        if hidden_dim is None:
            hidden_dim = config.NETWORK_HIDDEN_DIM
        
        # Networks (Continuous SACActor ì‚¬ìš©)
        self.actor = SACActor(state_dim, action_dim, info_dim, hidden_dim).to(device)
        self.critic = SACCritic(state_dim, action_dim, info_dim, hidden_dim).to(device)
        self.critic_target = SACCritic(state_dim, action_dim, info_dim, hidden_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # Optimizers (configì—ì„œ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.SAC_LEARNING_RATE)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.SAC_LEARNING_RATE)
        
        # Auto Entropy Tuning (Continuous: target = -action_dim)
        self.target_entropy = -float(action_dim)
        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.SAC_LEARNING_RATE)
        
        # Replay Buffer (configì—ì„œ í¬ê¸° ê°€ì ¸ì˜¤ê¸°)
        self.memory = ReplayBuffer(capacity=config.SAC_REPLAY_BUFFER_SIZE, device=device)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” (setup_schedulersì—ì„œ ì„¤ì •ë¨)
        self.actor_scheduler = None
        self.critic_scheduler = None
        self.alpha_scheduler = None
        
        # [NEW] ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
        self.actor_state = None

        logger.info(f"âœ… Continuous SAC Agent Initialized. Action Dim: {action_dim}")

    def setup_schedulers(self, total_steps, warmup_ratio=0.05):
        """
        Warmup + Linear Decay ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        
        Args:
            total_steps: ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜
            warmup_ratio: Warmup êµ¬ê°„ ë¹„ìœ¨ (ê¸°ë³¸ê°’ 0.05 = 5%)
        """
        warmup_steps = int(total_steps * warmup_ratio)
        
        def lr_lambda(step):
            # 1. Warmup êµ¬ê°„: 0 -> 1ë¡œ ì„ í˜• ì¦ê°€
            if step < warmup_steps:
                return float(step) / float(max(1, warmup_steps))
            
            # 2. Linear Decay êµ¬ê°„: 1 -> 0ìœ¼ë¡œ ì„ í˜• ê°ì†Œ
            progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))
            return max(0.0, 1.0 - progress)

        self.actor_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=lr_lambda)
        self.critic_scheduler = LambdaLR(self.critic_optimizer, lr_lambda=lr_lambda)
        self.alpha_scheduler = LambdaLR(self.alpha_optimizer, lr_lambda=lr_lambda)
        
        logger.info(f"ğŸ“ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ: ì´ {total_steps} ìŠ¤í…, Warmup {warmup_steps} ìŠ¤í… ({warmup_ratio*100:.1f}%)")
    
    def step_schedulers(self):
        """ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ í˜¸ì¶œí•˜ì—¬ LR ì¡°ì ˆ"""
        if self.actor_scheduler:
            self.actor_scheduler.step()
        if self.critic_scheduler:
            self.critic_scheduler.step()
        if self.alpha_scheduler:
            self.alpha_scheduler.step()

    def reset_episode_states(self):
        """ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ìƒíƒœ ì´ˆê¸°í™”"""
        self.actor_state = None

    def select_action(self, state, evaluate=False):
        """
        Stateful Action Selection
        LSTM ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° í–‰ë™ ê²°ì •
        
        Args:
            state: (obs_seq, obs_info) íŠœí”Œ
            evaluate: Trueë©´ í‰ê·  í–‰ë™ ë°˜í™˜, Falseë©´ ìƒ˜í”Œë§
        Returns:
            action: (numpy array) Continuous value [-1, 1]
        """
        # íŠœí”Œ ì…ë ¥ ì²˜ë¦¬
        if isinstance(state, (tuple, list)):
            if len(state) == 3:
                obs_seq, obs_info, _ = state
            else:
                obs_seq, obs_info = state
        else:
            obs_seq, obs_info = state

        obs_seq = obs_seq.to(self.device)
        obs_info = obs_info.to(self.device)
        
        with torch.no_grad():
            if evaluate:
                # í‰ê°€ ì‹œì—ëŠ” ìƒíƒœ ê°±ì‹  ì—†ì´ mean action ì‚¬ìš©
                mu, _, next_states = self.actor(obs_seq, obs_info, self.actor_state)
                action = torch.tanh(mu)
                # í‰ê°€ ì‹œì—ë„ ìƒíƒœëŠ” ê°±ì‹  (ì—°ì†ì„± ìœ ì§€)
                self.actor_state = next_states
            else:
                # [FIX] ìƒíƒœë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê³ , ë‹¤ìŒ ìƒíƒœë¥¼ ë°›ì•„ì™€ ì €ì¥
                action, _, _, next_states = self.actor.sample(obs_seq, obs_info, self.actor_state)
                self.actor_state = next_states  # ìƒíƒœ ê°±ì‹  (ê¸°ì–µ ìœ ì§€)
        
        return action.cpu().numpy()[0]  # 1D array

    def update(self, batch_size=None):
        """
        Continuous SAC ì—…ë°ì´íŠ¸ (Actor, Critic, Alpha)
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        Returns:
            critic_loss: Critic ì†ì‹¤
            actor_loss: Actor ì†ì‹¤
            alpha: í˜„ì¬ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
        """
        # Configì˜ Batch Size ì‚¬ìš© (ì¸ìê°€ ì—†ìœ¼ë©´)
        if batch_size is None:
            batch_size = config.SAC_BATCH_SIZE
        
        if len(self.memory) < batch_size:
            return 0, 0, 0

        # Sample Batch
        state, action, reward, next_state, done = self.memory.sample(batch_size)
        obs_seq, obs_info = state
        next_obs_seq, next_obs_info = next_state

        # ----------------------------
        # 1. Critic Update
        # ----------------------------
        with torch.no_grad():
            # í•™ìŠµ ì‹œì—ëŠ” ëœë¤ ë°°ì¹˜ì´ë¯€ë¡œ ìƒíƒœ(states)ë¥¼ Noneìœ¼ë¡œ í•˜ì—¬ ì´ˆê¸°í™”ëœ ìƒíƒœì—ì„œ ì‹œì‘
            # (Replay Bufferì— Hidden Stateë¥¼ ì €ì¥í•˜ì§€ ì•ŠëŠ” ë°©ì‹)
            next_action, next_log_prob, _, _ = self.actor.sample(next_obs_seq, next_obs_info, states=None)
            q1_next, q2_next, _ = self.critic_target(next_obs_seq, next_action, next_obs_info, states=None)
            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_prob
            q_target = reward + (1 - done) * self.gamma * min_q_next

        # Current Q
        q1, q2, _ = self.critic(obs_seq, action, obs_info, states=None)
        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_optimizer.step()

        # ----------------------------
        # 2. Actor Update
        # ----------------------------
        # Current Action sampling (Reparameterization)
        action_new, log_prob, _, _ = self.actor.sample(obs_seq, obs_info, states=None)
        q1_new, q2_new, _ = self.critic(obs_seq, action_new, obs_info, states=None)
        min_q_new = torch.min(q1_new, q2_new)
        
        # Maximize (min_q - alpha * log_prob)
        actor_loss = (self.alpha * log_prob - min_q_new).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optimizer.step()

        # ----------------------------
        # 3. Alpha Update
        # ----------------------------
        # Target Entropy = -Action Dim
        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()

        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        self.alpha = self.log_alpha.exp().item()

        # Soft Update Target Networks
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        return critic_loss.item(), actor_loss.item(), self.alpha

    def save_model(self, path):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'critic_target': self.critic_target.state_dict(),
            'log_alpha': self.log_alpha,
            'alpha': self.alpha
        }, path)
        logger.info(f"ğŸ’¾ SAC ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load_model(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device, weights_only=False)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.critic_target.load_state_dict(checkpoint['critic_target'])
        self.log_alpha = checkpoint['log_alpha']
        self.alpha = checkpoint.get('alpha', 0.2)
        logger.info(f"âœ… SAC ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
